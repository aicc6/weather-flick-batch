#!/usr/bin/env python3
"""
ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬
ì‘ì„±ì¼: 2025-07-06
ëª©ì : ê¸°ì¡´ ì»¨í…ì¸ ì— ëŒ€í•œ ìƒì„¸ ì •ë³´(detailCommon2, detailIntro2, detailInfo2, detailImage2) ìˆ˜ì§‘
"""

import os
import sys
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

from app.collectors.unified_kto_client import UnifiedKTOClient
from app.core.database_manager_extension import get_extended_database_manager


@dataclass
class DetailedCollectionConfig:
    """ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì„¤ì •"""
    
    # ìˆ˜ì§‘ ëŒ€ìƒ ì»¨í…ì¸  íƒ€ì…
    content_types: List[str]
    
    # ë°°ì¹˜ ì‚¬ì´ì¦ˆ (API í˜¸ì¶œ ê°„ê²© ì¡°ì •ì„ ìœ„í•´)
    batch_size: int = 50
    
    # API í˜¸ì¶œ ê°„ê²© (ì´ˆ)
    api_delay: float = 0.5
    
    # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    max_retries: int = 3
    
    # ìˆ˜ì§‘í•  ìƒì„¸ ì •ë³´ íƒ€ì…ë“¤
    detail_apis: List[str] = None
    
    # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ìˆ˜ì§‘í• ì§€ ì—¬ë¶€
    force_refresh: bool = False
    
    def __post_init__(self):
        if self.detail_apis is None:
            self.detail_apis = ['detailCommon2', 'detailIntro2', 'detailInfo2', 'detailImage2']


class DetailedDataCollectionScheduler:
    """ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, config: DetailedCollectionConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.kto_client = UnifiedKTOClient()
        self.db_manager = get_extended_database_manager()
        
        # ì»¨í…ì¸  íƒ€ì…ë³„ í…Œì´ë¸” ë§¤í•‘
        self.content_type_tables = {
            "12": "tourist_attractions",
            "14": "cultural_facilities", 
            "15": "festivals_events",
            "25": "travel_courses",
            "28": "leisure_sports",
            "32": "accommodations",
            "38": "shopping",
            "39": "restaurants"
        }
    
    async def collect_detailed_data_for_all_content_types(
        self, 
        limit_per_content_type: Optional[int] = None
    ) -> Dict:
        """ëª¨ë“  ì»¨í…ì¸  íƒ€ì…ì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        
        self.logger.info("=== ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ===")
        
        total_results = {
            'started_at': datetime.now().isoformat(),
            'content_types': {},
            'total_processed': 0,
            'total_errors': 0,
            'errors': []
        }
        
        for content_type in self.config.content_types:
            if content_type not in self.content_type_tables:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì»¨í…ì¸  íƒ€ì…: {content_type}")
                continue
                
            table_name = self.content_type_tables[content_type]
            self.logger.info(f"ğŸ“Š {table_name} (íƒ€ì… {content_type}) ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")
            
            try:
                result = await self.collect_detailed_data_for_content_type(
                    content_type, 
                    table_name, 
                    limit_per_content_type
                )
                
                total_results['content_types'][content_type] = result
                total_results['total_processed'] += result.get('processed_count', 0)
                total_results['total_errors'] += result.get('error_count', 0)
                
                self.logger.info(
                    f"âœ… {table_name} ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: "
                    f"ì²˜ë¦¬ {result.get('processed_count', 0)}ê±´, "
                    f"ì˜¤ë¥˜ {result.get('error_count', 0)}ê±´"
                )
                
            except Exception as e:
                error_msg = f"{table_name} ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
                self.logger.error(f"âŒ {error_msg}")
                total_results['errors'].append(error_msg)
                total_results['total_errors'] += 1
        
        total_results['completed_at'] = datetime.now().isoformat()
        
        self.logger.info("=== ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì™„ë£Œ ===")
        self.logger.info(
            f"ì „ì²´ ê²°ê³¼: ì²˜ë¦¬ {total_results['total_processed']}ê±´, "
            f"ì˜¤ë¥˜ {total_results['total_errors']}ê±´"
        )
        
        return total_results
    
    async def collect_detailed_data_for_content_type(
        self, 
        content_type: str, 
        table_name: str,
        limit: Optional[int] = None
    ) -> Dict:
        """íŠ¹ì • ì»¨í…ì¸  íƒ€ì…ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        
        # 1. ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•œ ì»¨í…ì¸  ëª©ë¡ ì¡°íšŒ
        content_items = await self.get_content_items_needing_details(
            table_name, content_type, limit
        )
        
        if not content_items:
            self.logger.info(f"ğŸ“­ {table_name}: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤")
            return {
                'content_type': content_type,
                'table_name': table_name,
                'candidates_count': 0,
                'processed_count': 0,
                'error_count': 0,
                'skipped_count': 0
            }
        
        self.logger.info(f"ğŸ“‹ {table_name}: {len(content_items)}ê°œ í•­ëª©ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")
        
        # 2. ë°°ì¹˜ë³„ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        processed_count = 0
        error_count = 0
        skipped_count = 0
        
        for i in range(0, len(content_items), self.config.batch_size):
            batch = content_items[i:i + self.config.batch_size]
            batch_num = (i // self.config.batch_size) + 1
            total_batches = (len(content_items) + self.config.batch_size - 1) // self.config.batch_size
            
            self.logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ í•­ëª©)")
            
            batch_results = await self.process_batch(batch, content_type)
            
            processed_count += batch_results['processed']
            error_count += batch_results['errors']
            skipped_count += batch_results['skipped']
            
            # ë°°ì¹˜ ê°„ ì§€ì—°
            if i + self.config.batch_size < len(content_items):
                await asyncio.sleep(self.config.api_delay * 2)  # ë°°ì¹˜ ê°„ ë” ê¸´ ì§€ì—°
        
        return {
            'content_type': content_type,
            'table_name': table_name,
            'candidates_count': len(content_items),
            'processed_count': processed_count,
            'error_count': error_count,
            'skipped_count': skipped_count
        }
    
    async def get_content_items_needing_details(
        self, 
        table_name: str, 
        content_type: str,
        limit: Optional[int] = None
    ) -> List[Dict]:
        """ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•œ ì»¨í…ì¸  í•­ëª©ë“¤ ì¡°íšŒ"""
        
        try:
            # ê¸°ë³¸ ì¡°ê±´: detail_intro_infoê°€ NULLì´ê±°ë‚˜ ë¹ˆ ê²½ìš°
            where_conditions = [
                "(detail_intro_info IS NULL OR detail_intro_info = '{}'::jsonb)",
                "content_id IS NOT NULL"
            ]
            
            # force_refreshê°€ Trueë©´ ëª¨ë“  í•­ëª©ì„ ëŒ€ìƒìœ¼ë¡œ í•¨
            if not self.config.force_refresh:
                where_conditions.append("(detail_intro_info IS NULL OR detail_intro_info = '{}'::jsonb)")
            
            where_clause = " AND ".join(where_conditions)
            limit_clause = f"LIMIT {limit}" if limit else ""
            
            query = f"""
            SELECT content_id 
            FROM {table_name} 
            WHERE {where_clause}
            ORDER BY created_at DESC
            {limit_clause}
            """
            
            results = self.db_manager.fetch_all(query)
            
            if results:
                return [{'content_id': row['content_id']} for row in results]
            else:
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ {table_name} ì»¨í…ì¸  ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def process_batch(self, batch: List[Dict], content_type: str) -> Dict:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬"""
        
        processed = 0
        errors = 0
        skipped = 0
        
        for item in batch:
            content_id = item['content_id']
            
            try:
                # ê° APIë³„ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                success_count = 0
                
                for api_name in self.config.detail_apis:
                    success = await self.collect_single_detail_api(
                        content_id, content_type, api_name
                    )
                    if success:
                        success_count += 1
                    
                    # API í˜¸ì¶œ ê°„ ì§€ì—°
                    await asyncio.sleep(self.config.api_delay)
                
                if success_count > 0:
                    processed += 1
                    self.logger.debug(f"âœ… {content_id}: {success_count}/{len(self.config.detail_apis)} API ì„±ê³µ")
                else:
                    skipped += 1
                    self.logger.debug(f"âš ï¸ {content_id}: ëª¨ë“  API í˜¸ì¶œ ì‹¤íŒ¨")
                
            except Exception as e:
                errors += 1
                self.logger.warning(f"âŒ {content_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return {
            'processed': processed,
            'errors': errors,
            'skipped': skipped
        }
    
    async def collect_single_detail_api(
        self, 
        content_id: str, 
        content_type: str, 
        api_name: str
    ) -> bool:
        """ë‹¨ì¼ ìƒì„¸ ì •ë³´ API í˜¸ì¶œ"""
        
        try:
            if api_name == 'detailCommon2':
                result = await self.kto_client.collect_detail_common(content_id, content_type)
            elif api_name == 'detailIntro2':
                result = await self.kto_client.collect_detail_intro(content_id, content_type)
            elif api_name == 'detailInfo2':
                result = await self.kto_client.collect_detail_info(content_id, content_type)
            elif api_name == 'detailImage2':
                result = await self.kto_client.collect_detail_images(content_id)
            else:
                self.logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” API: {api_name}")
                return False
            
            return result is not None
            
        except Exception as e:
            self.logger.debug(f"âŒ {content_id} {api_name} í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return False


# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("=== Weather Flick ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ===")
    print()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ì„¤ì • ìƒì„±
    config = DetailedCollectionConfig(
        content_types=["12", "14", "15", "25", "28", "32", "38", "39"],  # ëª¨ë“  ì»¨í…ì¸  íƒ€ì…
        batch_size=20,  # ë°°ì¹˜ í¬ê¸°ë¥¼ ì‘ê²Œ ì„¤ì •
        api_delay=1.0,  # API í˜¸ì¶œ ê°„ê²©ì„ ëŠ˜ë¦¼
        max_retries=3,
        force_refresh=False  # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
    )
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
    scheduler = DetailedDataCollectionScheduler(config)
    
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê° ì»¨í…ì¸  íƒ€ì…ë‹¹ ìµœëŒ€ 5ê°œë§Œ ì²˜ë¦¬
    results = await scheduler.collect_detailed_data_for_all_content_types(
        limit_per_content_type=5
    )
    
    print()
    print("=== ìˆ˜ì§‘ ì™„ë£Œ ===")
    print(f"ì „ì²´ ì²˜ë¦¬: {results['total_processed']}ê±´")
    print(f"ì „ì²´ ì˜¤ë¥˜: {results['total_errors']}ê±´")
    
    if results['errors']:
        print()
        print("=== ì˜¤ë¥˜ ëª©ë¡ ===")
        for error in results['errors']:
            print(f"- {error}")


if __name__ == "__main__":
    asyncio.run(main())