---
description: Weather Flick Batch System Development Rules
globs: ["**/*.py", "**/*.md", "**/*.yml", "**/*.yaml", "**/*.toml"]
alwaysApply: true
---

# Weather Flick Batch System Team Rules

ë‹¹ì‹ ì€ Weather Flick ë°°ì¹˜ ì‹œìŠ¤í…œ ê°œë°œì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. Python, APScheduler, ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬, ì™¸ë¶€ API ì—°ë™, ê·¸ë¦¬ê³  ë°°ì¹˜ ì‹œìŠ¤í…œ ìµœì í™”ì— ëŠ¥ìˆ™í•©ë‹ˆë‹¤.

## ğŸ¯ í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸

### ì—­í•  ë° ì±…ì„
- **ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ**: KTO (í•œêµ­ê´€ê´‘ê³µì‚¬), KMA (ê¸°ìƒì²­) API ì—°ë™
- **ë°°ì¹˜ ìŠ¤ì¼€ì¤„ë§**: APScheduler ê¸°ë°˜ ì •ê¸° ì‘ì—… ì‹¤í–‰
- **ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**: ìˆ˜ì§‘ â†’ ë³€í™˜ â†’ ê²€ì¦ â†’ ì €ì¥
- **ì¶”ì²œ ì—”ì§„**: ë‚ ì”¨ ê¸°ë°˜ ì—¬í–‰ì§€ ì¶”ì²œ ì ìˆ˜ ê³„ì‚°
- **ì‹œìŠ¤í…œ ìœ ì§€ë³´ìˆ˜**: ë°±ì—…, ë¡œê·¸ ê´€ë¦¬, ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬

### ê¸°ìˆ  ìŠ¤íƒ
- **Framework**: APScheduler + Python 3.11+
- **API í´ë¼ì´ì–¸íŠ¸**: ë‹¤ì¤‘ API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ
- **Data Processing**: Pandas, NumPy ê¸°ë°˜ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬
- **Database**: PostgreSQL + SQLAlchemy
- **Caching**: Redis ê¸°ë°˜ ìºì‹± ì‹œìŠ¤í…œ
- **Code Quality**: Ruff + Black + MyPy + pytest

## ğŸ“ í•„ìˆ˜ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
app/
â”œâ”€â”€ core/                    # í•µì‹¬ ì‹œìŠ¤í…œ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ base_job.py         # ë°°ì¹˜ ì‘ì—… ê¸°ë³¸ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ multi_api_key_manager.py # API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ smart_scheduler.py  # ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”œâ”€â”€ error_handling.py   # í†µí•© ì—ëŸ¬ ì²˜ë¦¬
â”‚   â””â”€â”€ logger.py          # ë¡œê¹… ì‹œìŠ¤í…œ
â”œâ”€â”€ collectors/             # ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ kto_collector.py    # KTO API ìˆ˜ì§‘ê¸°
â”‚   â”œâ”€â”€ weather_collector.py # ê¸°ìƒì²­ API ìˆ˜ì§‘ê¸°
â”‚   â””â”€â”€ unified_kto_client.py # í†µí•© KTO í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ processors/            # ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ data_transformation_pipeline.py
â”‚   â””â”€â”€ tourism_data_processor.py
â”œâ”€â”€ jobs/                 # ë°°ì¹˜ ì‘ì—… ì •ì˜
â”‚   â”œâ”€â”€ weather/          # ë‚ ì”¨ ë°ì´í„° ì‘ì—…
â”‚   â”œâ”€â”€ tourism/          # ê´€ê´‘ ë°ì´í„° ì‘ì—…
â”‚   â”œâ”€â”€ recommendation/   # ì¶”ì²œ ì‹œìŠ¤í…œ ì‘ì—…
â”‚   â”œâ”€â”€ quality/         # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
â”‚   â””â”€â”€ system_maintenance/ # ì‹œìŠ¤í…œ ìœ ì§€ë³´ìˆ˜
â””â”€â”€ schedulers/          # ìŠ¤ì¼€ì¤„ëŸ¬ ê´€ë¦¬
    â””â”€â”€ advanced_scheduler.py
```

## ğŸ› ï¸ í•µì‹¬ ê°œë°œ ì›ì¹™

### 1. ë°°ì¹˜ ì‘ì—… ê¸°ë³¸ íŒ¨í„´ (í•„ìˆ˜ ì¤€ìˆ˜)

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from datetime import datetime
import logging

from app.core.error_handling import handle_batch_error, BatchRetryStrategy
from app.core.logger import get_batch_logger

class BaseJob(ABC):
    """ë°°ì¹˜ ì‘ì—… ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, job_name: str):
        self.job_name = job_name
        self.logger = get_batch_logger(job_name)
        self.start_time: Optional[datetime] = None
        self.retry_strategy = BatchRetryStrategy()
    
    async def execute(self, **kwargs) -> Dict[str, Any]:
        """
        ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ ë©”ì¸ ë©”ì„œë“œ
        
        Returns:
            Dict[str, Any]: ì‘ì—… ì‹¤í–‰ ê²°ê³¼
        
        Raises:
            BatchJobError: ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨
        """
        self.start_time = datetime.now()
        job_context = {
            "job_name": self.job_name,
            "start_time": self.start_time,
            "parameters": kwargs
        }
        
        try:
            self.logger.info(f"ë°°ì¹˜ ì‘ì—… ì‹œì‘: {self.job_name}")
            
            # Pre-execution validation
            await self.validate_prerequisites()
            
            # Main job execution
            result = await self.run_job(**kwargs)
            
            # Post-execution cleanup
            await self.cleanup()
            
            execution_time = (datetime.now() - self.start_time).total_seconds()
            success_result = {
                "status": "SUCCESS",
                "execution_time": execution_time,
                "processed_records": result.get("processed_records", 0),
                "result": result
            }
            
            self.logger.info(
                f"ë°°ì¹˜ ì‘ì—… ì™„ë£Œ: {self.job_name}, "
                f"ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ, "
                f"ì²˜ë¦¬ ë ˆì½”ë“œ: {result.get('processed_records', 0)}ê°œ"
            )
            
            return success_result
            
        except Exception as e:
            execution_time = (datetime.now() - self.start_time).total_seconds()
            error_result = await handle_batch_error(
                e, job_context, self.retry_strategy
            )
            
            self.logger.error(
                f"ë°°ì¹˜ ì‘ì—… ì‹¤íŒ¨: {self.job_name}, "
                f"ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ, "
                f"ì˜¤ë¥˜: {str(e)}"
            )
            
            return {
                "status": "FAILED",
                "execution_time": execution_time,
                "error": str(e),
                "error_details": error_result
            }
    
    @abstractmethod
    async def run_job(self, **kwargs) -> Dict[str, Any]:
        """ì‹¤ì œ ë°°ì¹˜ ì‘ì—… ë¡œì§ êµ¬í˜„"""
        pass
    
    async def validate_prerequisites(self) -> None:
        """ì‘ì—… ì‹¤í–‰ ì „ í•„ìˆ˜ ì¡°ê±´ ê²€ì¦"""
        pass
    
    async def cleanup(self) -> None:
        """ì‘ì—… ì™„ë£Œ í›„ ì •ë¦¬ ì‘ì—…"""
        pass

# êµ¬ì²´ì ì¸ ë°°ì¹˜ ì‘ì—… êµ¬í˜„ ì˜ˆì‹œ
class WeatherDataJob(BaseJob):
    """ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ë°°ì¹˜ ì‘ì—…"""
    
    def __init__(self):
        super().__init__("weather_data_collection")
        self.weather_collector = WeatherCollector()
        self.data_validator = WeatherDataValidator()
    
    async def validate_prerequisites(self) -> None:
        """API í‚¤ ë° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸"""
        if not await self.weather_collector.check_api_availability():
            raise ValueError("ê¸°ìƒì²­ API ì—°ê²° ë¶ˆê°€ëŠ¥")
    
    async def run_job(self, regions: List[str] = None) -> Dict[str, Any]:
        """ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        # ì…ë ¥ ê²€ì¦ (Early Return)
        if not regions:
            regions = await self._get_default_regions()
        
        if not regions:
            raise ValueError("ìˆ˜ì§‘í•  ì§€ì—­ì´ ì—†ìŠµë‹ˆë‹¤")
        
        total_processed = 0
        failed_regions = []
        
        for region in regions:
            try:
                # ì§€ì—­ë³„ ë°ì´í„° ìˆ˜ì§‘
                weather_data = await self.weather_collector.collect_weather_data(region)
                
                # ë°ì´í„° ê²€ì¦
                validated_data = await self.data_validator.validate(weather_data)
                
                # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                saved_count = await self._save_weather_data(validated_data)
                total_processed += saved_count
                
                self.logger.info(f"ì§€ì—­ {region} ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {saved_count}ê°œ")
                
            except Exception as e:
                failed_regions.append({"region": region, "error": str(e)})
                self.logger.error(f"ì§€ì—­ {region} ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return {
            "processed_records": total_processed,
            "successful_regions": len(regions) - len(failed_regions),
            "failed_regions": failed_regions,
            "total_regions": len(regions)
        }
```

### 2. ë‹¤ì¤‘ API í‚¤ ê´€ë¦¬ íŒ¨í„´ (í•„ìˆ˜)

```python
from typing import List, Dict, Optional, Tuple
import asyncio
import json
from datetime import datetime, timedelta
import redis
import logging

logger = logging.getLogger(__name__)

class MultiApiKeyManager:
    """ë‹¤ì¤‘ API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 api_keys: List[str], 
                 daily_limit: int,
                 redis_client: redis.Redis = None):
        self.api_keys = api_keys
        self.daily_limit = daily_limit
        self.redis = redis_client or redis.Redis(host='localhost', port=6379, db=0)
        self.cache_prefix = "api_key_usage"
        self.current_key_index = 0
        
    async def get_available_api_key(self) -> Optional[Tuple[str, int]]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ ë°˜í™˜
        
        Returns:
            Tuple[str, int]: (API í‚¤, ë‚¨ì€ ì‚¬ìš©ëŸ‰) ë˜ëŠ” None
        
        Raises:
            APIKeyExhaustedException: ëª¨ë“  í‚¤ ì‚¬ìš©ëŸ‰ ì´ˆê³¼
        """
        for attempt in range(len(self.api_keys)):
            key_index = (self.current_key_index + attempt) % len(self.api_keys)
            api_key = self.api_keys[key_index]
            
            # í‚¤ë³„ ì‚¬ìš©ëŸ‰ í™•ì¸
            usage_count = await self._get_key_usage(api_key)
            remaining = self.daily_limit - usage_count
            
            if remaining > 0:
                # í‚¤ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (ë¼ìš´ë“œ ë¡œë¹ˆ)
                self.current_key_index = key_index
                return api_key, remaining
            
            logger.warning(f"API í‚¤ {key_index} ì¼ì¼ í•œë„ ì´ˆê³¼: {usage_count}/{self.daily_limit}")
        
        # ëª¨ë“  í‚¤ê°€ í•œë„ ì´ˆê³¼ì¸ ê²½ìš°
        raise APIKeyExhaustedException(
            f"ëª¨ë“  API í‚¤ì˜ ì¼ì¼ ì‚¬ìš©ëŸ‰ ì´ˆê³¼ (ì´ {len(self.api_keys)}ê°œ í‚¤)"
        )
    
    async def record_api_usage(self, api_key: str, request_count: int = 1) -> int:
        """
        API ì‚¬ìš©ëŸ‰ ê¸°ë¡
        
        Args:
            api_key: ì‚¬ìš©ëœ API í‚¤
            request_count: ìš”ì²­ ìˆ˜
        
        Returns:
            int: ì—…ë°ì´íŠ¸ëœ ì´ ì‚¬ìš©ëŸ‰
        """
        cache_key = f"{self.cache_prefix}:{api_key}:{datetime.now().strftime('%Y-%m-%d')}"
        
        try:
            # Redisì— ì‚¬ìš©ëŸ‰ ì¦ê°€
            current_usage = self.redis.incr(cache_key, request_count)
            
            # TTL ì„¤ì • (ë‹¤ìŒë‚  ìì •ê¹Œì§€)
            tomorrow = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
            ttl = int((tomorrow - datetime.now()).total_seconds())
            self.redis.expire(cache_key, ttl)
            
            # ì‚¬ìš©ëŸ‰ ë¡œê·¸
            remaining = self.daily_limit - current_usage
            logger.info(f"API í‚¤ ì‚¬ìš©ëŸ‰ ê¸°ë¡: {current_usage}/{self.daily_limit} (ë‚¨ì€ ì‚¬ìš©ëŸ‰: {remaining})")
            
            # 90% ë„ë‹¬ ì‹œ ê²½ê³ 
            if current_usage >= self.daily_limit * 0.9:
                logger.warning(f"API í‚¤ ì‚¬ìš©ëŸ‰ 90% ë„ë‹¬: {current_usage}/{self.daily_limit}")
            
            return current_usage
            
        except Exception as e:
            logger.error(f"API ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨: {e}")
            return 0
    
    async def _get_key_usage(self, api_key: str) -> int:
        """API í‚¤ì˜ ì¼ì¼ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        cache_key = f"{self.cache_prefix}:{api_key}:{datetime.now().strftime('%Y-%m-%d')}"
        
        try:
            usage = self.redis.get(cache_key)
            return int(usage) if usage else 0
        except Exception as e:
            logger.error(f"API ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0

class APIKeyExhaustedException(Exception):
    """ëª¨ë“  API í‚¤ ì‚¬ìš©ëŸ‰ ì´ˆê³¼ ì˜ˆì™¸"""
    pass
```

### 3. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ íŒ¨í„´

```python
import pandas as pd
import numpy as np
from typing import Iterator, Dict, List, Any
import logging
from concurrent.futures import ThreadPoolExecutor
import asyncio

logger = logging.getLogger(__name__)

class BatchDataProcessor:
    """ëŒ€ìš©ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, 
                 chunk_size: int = 10000,
                 max_workers: int = 4,
                 memory_limit_mb: int = 512):
        self.chunk_size = chunk_size
        self.max_workers = max_workers
        self.memory_limit_bytes = memory_limit_mb * 1024 * 1024
    
    async def process_large_dataset(self, 
                                   data_source: Any,
                                   processor_func: callable,
                                   **kwargs) -> Dict[str, Any]:
        """
        ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
        
        Args:
            data_source: ë°ì´í„° ì†ŒìŠ¤ (íŒŒì¼, DB ì¿¼ë¦¬ ë“±)
            processor_func: ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜
            **kwargs: ì²˜ë¦¬ í•¨ìˆ˜ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì
        
        Returns:
            Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        total_processed = 0
        total_chunks = 0
        failed_chunks = 0
        processing_errors = []
        
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            import psutil
            process = psutil.Process()
            initial_memory = process.memory_info().rss
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬
            async for chunk_data in self._get_data_chunks(data_source):
                total_chunks += 1
                
                try:
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                    current_memory = process.memory_info().rss
                    if current_memory - initial_memory > self.memory_limit_bytes:
                        logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í•œê³„ ì ‘ê·¼: {(current_memory - initial_memory) / 1024 / 1024:.1f}MB")
                        
                        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                        import gc
                        gc.collect()
                    
                    # ì²­í¬ ì²˜ë¦¬
                    processed_count = await self._process_chunk(
                        chunk_data, processor_func, **kwargs
                    )
                    total_processed += processed_count
                    
                    if total_chunks % 10 == 0:  # 10ê°œ ì²­í¬ë§ˆë‹¤ ë¡œê·¸
                        logger.info(f"ì²˜ë¦¬ ì§„í–‰ë¥ : {total_chunks}ê°œ ì²­í¬, {total_processed}ê°œ ë ˆì½”ë“œ ì™„ë£Œ")
                
                except Exception as e:
                    failed_chunks += 1
                    error_info = {
                        "chunk_number": total_chunks,
                        "error": str(e),
                        "data_sample": str(chunk_data.head(3) if hasattr(chunk_data, 'head') else chunk_data)[:200]
                    }
                    processing_errors.append(error_info)
                    logger.error(f"ì²­í¬ {total_chunks} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    # ì—°ì† ì‹¤íŒ¨ ì²´í¬
                    if failed_chunks >= 5:
                        logger.error("ì—°ì† 5ê°œ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨, ì‘ì—… ì¤‘ë‹¨")
                        break
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            final_memory = process.memory_info().rss
            memory_used = (final_memory - initial_memory) / 1024 / 1024
            
            return {
                "total_processed": total_processed,
                "total_chunks": total_chunks,
                "failed_chunks": failed_chunks,
                "success_rate": round((total_chunks - failed_chunks) / total_chunks * 100, 2) if total_chunks > 0 else 0,
                "memory_used_mb": round(memory_used, 2),
                "processing_errors": processing_errors[:10]  # ìµœëŒ€ 10ê°œ ì˜¤ë¥˜ë§Œ ë°˜í™˜
            }
            
        except Exception as e:
            logger.error(f"ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
```

## ğŸš¨ ì—ëŸ¬ ì²˜ë¦¬ ë° ëª¨ë‹ˆí„°ë§ ê·œì¹™

### 1. ë°°ì¹˜ ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´

```python
from enum import Enum
from typing import Dict, Any, Optional
import logging
import traceback
from datetime import datetime

class BatchErrorSeverity(Enum):
    """ë°°ì¹˜ ì˜¤ë¥˜ ì‹¬ê°ë„"""
    LOW = "LOW"
    MEDIUM = "MEDIUM"
    HIGH = "HIGH"
    CRITICAL = "CRITICAL"

class BatchJobError(Exception):
    """ë°°ì¹˜ ì‘ì—… ì˜¤ë¥˜"""
    def __init__(self, message: str, error_code: str = None, severity: BatchErrorSeverity = BatchErrorSeverity.MEDIUM):
        super().__init__(message)
        self.error_code = error_code
        self.severity = severity
        self.timestamp = datetime.now()

async def handle_batch_error(error: Exception, 
                           job_context: Dict[str, Any],
                           retry_strategy) -> Dict[str, Any]:
    """
    ë°°ì¹˜ ì˜¤ë¥˜ í†µí•© ì²˜ë¦¬
    
    Args:
        error: ë°œìƒí•œ ì˜¤ë¥˜
        job_context: ì‘ì—… ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        retry_strategy: ì¬ì‹œë„ ì „ëµ
    
    Returns:
        Dict[str, Any]: ì˜¤ë¥˜ ì²˜ë¦¬ ê²°ê³¼
    """
    error_info = {
        "job_name": job_context.get("job_name"),
        "error_type": type(error).__name__,
        "error_message": str(error),
        "traceback": traceback.format_exc(),
        "timestamp": datetime.now().isoformat(),
        "job_context": job_context
    }
    
    # ì˜¤ë¥˜ ì‹¬ê°ë„ íŒë‹¨
    severity = _determine_error_severity(error, job_context)
    error_info["severity"] = severity.value
    
    # ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    attempt = job_context.get("retry_attempt", 0)
    if retry_strategy.should_retry(error, attempt):
        retry_delay = retry_strategy.get_retry_delay(attempt)
        error_info.update({
            "will_retry": True,
            "retry_attempt": attempt + 1,
            "retry_delay_seconds": retry_delay
        })
    else:
        error_info["will_retry"] = False
        # ì˜¤ë¥˜ ì•Œë¦¼ ë°œì†¡
        await _send_error_notification(error_info)
    
    return error_info
```

## ğŸ”§ ê°œë°œ ì›Œí¬í”Œë¡œìš°

### 1. ë°°ì¹˜ ì‘ì—… ê°œë°œ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# 1. ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
ruff check --fix .
black .
mypy app/

# 2. ë°°ì¹˜ ì‘ì—… í…ŒìŠ¤íŠ¸
pytest tests/unit/ -v
pytest tests/integration/ -v

# 3. API í‚¤ ê´€ë¦¬ í…ŒìŠ¤íŠ¸
python -c "from app.core.multi_api_key_manager import MultiApiKeyManager; print('API í‚¤ ê´€ë¦¬ ì‹œìŠ¤í…œ ì •ìƒ')"

# 4. ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
python -m app.processors.data_transformation_pipeline

# 5. í™˜ê²½ë³€ìˆ˜ ê²€ì¦
python debug_env.py
```

### 2. ë°°ì¹˜ ì‘ì—… ê°œë°œ ìˆœì„œ

1. **ê¸°ë³¸ ì‘ì—… í´ë˜ìŠ¤ êµ¬í˜„** (BaseJob ìƒì†)
2. **ë°ì´í„° ìˆ˜ì§‘/ì²˜ë¦¬ ë¡œì§** êµ¬í˜„
3. **ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„** ì „ëµ ì„¤ì •
4. **ìŠ¤ì¼€ì¤„ ë“±ë¡** (APScheduler ì‚¬ìš©)
5. **í…ŒìŠ¤íŠ¸ ì‘ì„±** (ë‹¨ìœ„/í†µí•© í…ŒìŠ¤íŠ¸)
6. **ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­** ì¶”ê°€
7. **ë¬¸ì„œí™”** ì—…ë°ì´íŠ¸

ì´ ê·œì¹™ë“¤ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ Weather Flick ë°°ì¹˜ ì‹œìŠ¤í…œì„ ê°œë°œí•˜ì„¸ìš”.

