"""
ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

API ì €ì¥ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ì¸¡ì •í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
"""

import logging
import time
import threading
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import deque, defaultdict
from enum import Enum
import statistics

from app.core.selective_storage_manager import get_storage_manager
from app.core.ttl_policy_engine import get_ttl_engine
from app.monitoring.cleanup_monitor import get_cleanup_monitor

logger = logging.getLogger(__name__)


class PerformanceMetricType(Enum):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ íƒ€ì…"""
    RESPONSE_TIME = "response_time"
    THROUGHPUT = "throughput"
    ERROR_RATE = "error_rate"
    STORAGE_EFFICIENCY = "storage_efficiency"
    MEMORY_USAGE = "memory_usage"
    CPU_USAGE = "cpu_usage"


@dataclass
class PerformanceMetric:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„°"""
    metric_type: PerformanceMetricType
    value: float
    timestamp: datetime
    source: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PerformanceAlert:
    """ì„±ëŠ¥ ì•Œë¦¼"""
    metric_type: PerformanceMetricType
    severity: str  # critical, warning, info
    message: str
    value: float
    threshold: float
    timestamp: datetime
    source: str


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_history_size: int = 1000):
        """
        ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™”
        
        Args:
            max_history_size: ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ìµœëŒ€ í¬ê¸°
        """
        self.max_history_size = max_history_size
        
        # ì˜ì¡´ì„± ì£¼ì…
        self.storage_manager = get_storage_manager()
        self.ttl_engine = get_ttl_engine(dry_run=True)
        self.cleanup_monitor = get_cleanup_monitor()
        
        # ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
        self.metrics_history: Dict[PerformanceMetricType, deque] = {
            metric_type: deque(maxlen=max_history_size)
            for metric_type in PerformanceMetricType
        }
        
        # ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            PerformanceMetricType.RESPONSE_TIME: {
                "warning": 5000.0,    # 5ì´ˆ
                "critical": 10000.0   # 10ì´ˆ
            },
            PerformanceMetricType.THROUGHPUT: {
                "warning": 10.0,      # 10 requests/sec ì´í•˜
                "critical": 5.0       # 5 requests/sec ì´í•˜
            },
            PerformanceMetricType.ERROR_RATE: {
                "warning": 5.0,       # 5% ì´ìƒ
                "critical": 10.0      # 10% ì´ìƒ
            },
            PerformanceMetricType.STORAGE_EFFICIENCY: {
                "warning": 70.0,      # 70% ì´í•˜
                "critical": 50.0      # 50% ì´í•˜
            },
            PerformanceMetricType.MEMORY_USAGE: {
                "warning": 80.0,      # 80% ì´ìƒ
                "critical": 90.0      # 90% ì´ìƒ
            },
            PerformanceMetricType.CPU_USAGE: {
                "warning": 80.0,      # 80% ì´ìƒ
                "critical": 90.0      # 90% ì´ìƒ
            }
        }
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        self.monitoring_active = False
        self.monitoring_thread: Optional[threading.Thread] = None
        self.monitoring_interval = 30  # 30ì´ˆë§ˆë‹¤ ì²´í¬
        
        # ì•Œë¦¼ íˆìŠ¤í† ë¦¬
        self.alerts_history: deque = deque(maxlen=100)
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            "monitoring_start_time": None,
            "total_metrics_collected": 0,
            "total_alerts_generated": 0,
            "last_health_check": None
        }
        
        logger.info("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.monitoring_active:
            logger.warning("ëª¨ë‹ˆí„°ë§ì´ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            return
        
        self.monitoring_active = True
        self.performance_stats["monitoring_start_time"] = datetime.now()
        
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            daemon=True
        )
        self.monitoring_thread.start()
        
        logger.info("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=5)
        
        logger.info("ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            try:
                self._collect_performance_metrics()
                time.sleep(self.monitoring_interval)
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(self.monitoring_interval)
    
    def _collect_performance_metrics(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # 1. ì €ì¥ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self._collect_storage_metrics()
            
            # 2. TTL ì—”ì§„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            self._collect_ttl_metrics()
            
            # 3. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­
            self._collect_system_metrics()
            
            # 4. ì „ì²´ ê±´ê°• ìƒíƒœ ì²´í¬
            self._perform_health_check()
            
            self.performance_stats["last_health_check"] = datetime.now()
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def _collect_storage_metrics(self):
        """ì €ì¥ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            storage_stats = self.storage_manager.get_statistics()
            
            # í‰ê·  ì‘ë‹µ ì‹œê°„
            avg_decision_time = storage_stats.get("avg_decision_time_ms", 0)
            self._add_metric(
                PerformanceMetricType.RESPONSE_TIME,
                avg_decision_time,
                "storage_manager",
                {"metric": "decision_time"}
            )
            
            # ì²˜ë¦¬ëŸ‰ (requests per second)
            total_requests = storage_stats.get("total_requests", 0)
            if hasattr(self, '_last_storage_check'):
                time_diff = (datetime.now() - self._last_storage_check).total_seconds()
                if time_diff > 0:
                    requests_diff = total_requests - getattr(self, '_last_total_requests', 0)
                    throughput = requests_diff / time_diff
                    self._add_metric(
                        PerformanceMetricType.THROUGHPUT,
                        throughput,
                        "storage_manager",
                        {"metric": "requests_per_second"}
                    )
            
            # ì €ì¥ ì„±ê³µë¥ 
            success_rate = storage_stats.get("storage_success_rate", 100)
            error_rate = 100 - success_rate
            self._add_metric(
                PerformanceMetricType.ERROR_RATE,
                error_rate,
                "storage_manager",
                {"metric": "storage_error_rate"}
            )
            
            # ì €ì¥ íš¨ìœ¨ì„± (ì €ì¥ ê²°ì • ë¹„ìœ¨)
            decision_rate = storage_stats.get("decision_rate", 100)
            self._add_metric(
                PerformanceMetricType.STORAGE_EFFICIENCY,
                decision_rate,
                "storage_manager",
                {"metric": "decision_efficiency"}
            )
            
            # ë§ˆì§€ë§‰ ì²´í¬ ì‹œê°„ ë° ìš”ì²­ ìˆ˜ ì €ì¥
            self._last_storage_check = datetime.now()
            self._last_total_requests = total_requests
            
        except Exception as e:
            logger.error(f"ì €ì¥ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def _collect_ttl_metrics(self):
        """TTL ì—”ì§„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            ttl_stats = self.ttl_engine.get_statistics()
            
            # TTL ì—”ì§„ ì‘ë‹µ ì‹œê°„
            avg_execution_time = ttl_stats.get("avg_execution_time_sec", 0) * 1000  # msë¡œ ë³€í™˜
            self._add_metric(
                PerformanceMetricType.RESPONSE_TIME,
                avg_execution_time,
                "ttl_engine",
                {"metric": "cleanup_execution_time"}
            )
            
            # ì •ë¦¬ íš¨ìœ¨ì„± (ì‚­ì œëœ ë ˆì½”ë“œ ìˆ˜)
            total_deleted = ttl_stats.get("total_deleted", 0)
            cleanup_runs = ttl_stats.get("cleanup_runs", 1)
            cleanup_efficiency = total_deleted / max(cleanup_runs, 1)
            self._add_metric(
                PerformanceMetricType.STORAGE_EFFICIENCY,
                cleanup_efficiency,
                "ttl_engine",
                {"metric": "cleanup_efficiency", "unit": "records_per_run"}
            )
            
        except Exception as e:
            logger.error(f"TTL ì—”ì§„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def _collect_system_metrics(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            import psutil
            
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=1)
            self._add_metric(
                PerformanceMetricType.CPU_USAGE,
                cpu_percent,
                "system",
                {"metric": "cpu_percent"}
            )
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            self._add_metric(
                PerformanceMetricType.MEMORY_USAGE,
                memory_percent,
                "system",
                {"metric": "memory_percent", "available_gb": memory.available / (1024**3)}
            )
            
        except ImportError:
            logger.warning("psutilì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def _perform_health_check(self):
        """ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ì²´í¬"""
        try:
            health_result = self.cleanup_monitor.perform_health_check()
            
            # ì „ì²´ ê±´ê°• ì ìˆ˜ë¥¼ ì €ì¥ íš¨ìœ¨ì„± ë©”íŠ¸ë¦­ìœ¼ë¡œ ê¸°ë¡
            health_score = health_result.get("health_score", 100)
            self._add_metric(
                PerformanceMetricType.STORAGE_EFFICIENCY,
                health_score,
                "health_monitor",
                {"metric": "overall_health_score"}
            )
            
            # ì•Œë¦¼ ìƒì„± í™•ì¸
            alerts = health_result.get("alerts", [])
            for alert in alerts:
                self._generate_alert_from_health_check(alert)
                
        except Exception as e:
            logger.error(f"ê±´ê°• ìƒíƒœ ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def _add_metric(self, metric_type: PerformanceMetricType, value: float, 
                   source: str, metadata: Dict[str, Any]):
        """ë©”íŠ¸ë¦­ ì¶”ê°€ ë° ì„ê³„ê°’ ì²´í¬"""
        metric = PerformanceMetric(
            metric_type=metric_type,
            value=value,
            timestamp=datetime.now(),
            source=source,
            metadata=metadata
        )
        
        self.metrics_history[metric_type].append(metric)
        self.performance_stats["total_metrics_collected"] += 1
        
        # ì„ê³„ê°’ ì²´í¬ ë° ì•Œë¦¼ ìƒì„±
        self._check_thresholds(metric)
    
    def _check_thresholds(self, metric: PerformanceMetric):
        """ì„ê³„ê°’ ì²´í¬ ë° ì•Œë¦¼ ìƒì„±"""
        thresholds = self.thresholds.get(metric.metric_type, {})
        
        critical_threshold = thresholds.get("critical")
        warning_threshold = thresholds.get("warning")
        
        alert = None
        
        # ë©”íŠ¸ë¦­ íƒ€ì…ì— ë”°ë¥¸ ì„ê³„ê°’ ë¹„êµ ë¡œì§
        if metric.metric_type in [PerformanceMetricType.ERROR_RATE, 
                                PerformanceMetricType.MEMORY_USAGE, 
                                PerformanceMetricType.CPU_USAGE,
                                PerformanceMetricType.RESPONSE_TIME]:
            # ë†’ì„ìˆ˜ë¡ ë‚˜ì¨
            if critical_threshold and metric.value >= critical_threshold:
                alert = PerformanceAlert(
                    metric_type=metric.metric_type,
                    severity="critical",
                    message=f"{metric.metric_type.value} ìœ„í—˜ ìˆ˜ì¤€: {metric.value:.2f} (ì„ê³„ê°’: {critical_threshold})",
                    value=metric.value,
                    threshold=critical_threshold,
                    timestamp=metric.timestamp,
                    source=metric.source
                )
            elif warning_threshold and metric.value >= warning_threshold:
                alert = PerformanceAlert(
                    metric_type=metric.metric_type,
                    severity="warning",
                    message=f"{metric.metric_type.value} ê²½ê³  ìˆ˜ì¤€: {metric.value:.2f} (ì„ê³„ê°’: {warning_threshold})",
                    value=metric.value,
                    threshold=warning_threshold,
                    timestamp=metric.timestamp,
                    source=metric.source
                )
        
        elif metric.metric_type in [PerformanceMetricType.THROUGHPUT,
                                  PerformanceMetricType.STORAGE_EFFICIENCY]:
            # ë‚®ì„ìˆ˜ë¡ ë‚˜ì¨
            if critical_threshold and metric.value <= critical_threshold:
                alert = PerformanceAlert(
                    metric_type=metric.metric_type,
                    severity="critical",
                    message=f"{metric.metric_type.value} ìœ„í—˜ ìˆ˜ì¤€: {metric.value:.2f} (ì„ê³„ê°’: {critical_threshold} ì´í•˜)",
                    value=metric.value,
                    threshold=critical_threshold,
                    timestamp=metric.timestamp,
                    source=metric.source
                )
            elif warning_threshold and metric.value <= warning_threshold:
                alert = PerformanceAlert(
                    metric_type=metric.metric_type,
                    severity="warning",
                    message=f"{metric.metric_type.value} ê²½ê³  ìˆ˜ì¤€: {metric.value:.2f} (ì„ê³„ê°’: {warning_threshold} ì´í•˜)",
                    value=metric.value,
                    threshold=warning_threshold,
                    timestamp=metric.timestamp,
                    source=metric.source
                )
        
        if alert:
            self._handle_alert(alert)
    
    def _generate_alert_from_health_check(self, health_alert):
        """ê±´ê°• ìƒíƒœ ì²´í¬ì—ì„œ ì•Œë¦¼ ìƒì„±"""
        try:
            severity_map = {
                "INFO": "info",
                "WARNING": "warning", 
                "ERROR": "warning",
                "CRITICAL": "critical"
            }
            
            alert = PerformanceAlert(
                metric_type=PerformanceMetricType.STORAGE_EFFICIENCY,
                severity=severity_map.get(health_alert.level.value, "info"),
                message=health_alert.message,
                value=0.0,
                threshold=0.0,
                timestamp=health_alert.timestamp,
                source="health_monitor"
            )
            
            self._handle_alert(alert)
            
        except Exception as e:
            logger.error(f"ê±´ê°• ìƒíƒœ ì•Œë¦¼ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _handle_alert(self, alert: PerformanceAlert):
        """ì•Œë¦¼ ì²˜ë¦¬"""
        self.alerts_history.append(alert)
        self.performance_stats["total_alerts_generated"] += 1
        
        # ë¡œê·¸ ì¶œë ¥
        log_level = {
            "info": logging.INFO,
            "warning": logging.WARNING,
            "critical": logging.ERROR
        }.get(alert.severity, logging.INFO)
        
        logger.log(log_level, f"ğŸš¨ ì„±ëŠ¥ ì•Œë¦¼ ({alert.severity.upper()}): {alert.message}")
    
    def get_performance_summary(self, time_window_minutes: int = 60) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        cutoff_time = datetime.now() - timedelta(minutes=time_window_minutes)
        
        summary = {
            "time_window_minutes": time_window_minutes,
            "summary_generated_at": datetime.now(),
            "metrics_summary": {},
            "recent_alerts": [],
            "overall_health": "healthy"
        }
        
        # ê° ë©”íŠ¸ë¦­ íƒ€ì…ë³„ ìš”ì•½
        for metric_type in PerformanceMetricType:
            recent_metrics = [
                m for m in self.metrics_history[metric_type]
                if m.timestamp >= cutoff_time
            ]
            
            if recent_metrics:
                values = [m.value for m in recent_metrics]
                summary["metrics_summary"][metric_type.value] = {
                    "count": len(values),
                    "avg": statistics.mean(values),
                    "min": min(values),
                    "max": max(values),
                    "latest": values[-1],
                    "trend": self._calculate_trend(values)
                }
        
        # ìµœê·¼ ì•Œë¦¼
        recent_alerts = [
            {
                "severity": alert.severity,
                "message": alert.message,
                "timestamp": alert.timestamp.isoformat(),
                "source": alert.source
            }
            for alert in self.alerts_history
            if alert.timestamp >= cutoff_time
        ]
        summary["recent_alerts"] = recent_alerts
        
        # ì „ì²´ ê±´ê°• ìƒíƒœ í‰ê°€
        critical_alerts = [a for a in recent_alerts if a["severity"] == "critical"]
        warning_alerts = [a for a in recent_alerts if a["severity"] == "warning"]
        
        if critical_alerts:
            summary["overall_health"] = "critical"
        elif warning_alerts:
            summary["overall_health"] = "warning"
        elif recent_alerts:
            summary["overall_health"] = "attention_needed"
        
        return summary
    
    def _calculate_trend(self, values: List[float]) -> str:
        """ê°’ë“¤ì˜ íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(values) < 2:
            return "stable"
        
        # ë‹¨ìˆœ ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
        n = len(values)
        x = list(range(n))
        
        x_mean = statistics.mean(x)
        y_mean = statistics.mean(values)
        
        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))
        
        if denominator == 0:
            return "stable"
        
        slope = numerator / denominator
        
        if slope > 0.1:
            return "increasing"
        elif slope < -0.1:
            return "decreasing"
        else:
            return "stable"
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """í˜„ì¬ ë©”íŠ¸ë¦­ ê°’ë“¤ ë°˜í™˜"""
        current_metrics = {}
        
        for metric_type in PerformanceMetricType:
            if self.metrics_history[metric_type]:
                latest_metric = self.metrics_history[metric_type][-1]
                current_metrics[metric_type.value] = latest_metric.value
                
                # íŠ¹ë³„í•œ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
                if metric_type == PerformanceMetricType.MEMORY_USAGE:
                    current_metrics["memory_usage_mb"] = latest_metric.value
                elif metric_type == PerformanceMetricType.CPU_USAGE:
                    current_metrics["cpu_usage_percent"] = latest_metric.value
        
        # í™œì„± ì—°ê²° ìˆ˜ ì¶”ê°€ (ì˜ˆì‹œ)
        current_metrics["active_connections"] = len(current_metrics)
        
        return current_metrics
    
    def generate_report(self, output_dir: str = "logs") -> str:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        import os
        import json
        
        os.makedirs(output_dir, exist_ok=True)
        
        report_data = {
            "generated_at": datetime.now().isoformat(),
            "performance_summary": self.get_performance_summary(),
            "monitoring_statistics": self.get_monitoring_statistics(),
            "current_metrics": self.get_current_metrics()
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = os.path.join(output_dir, f"performance_report_{timestamp}.json")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, default=str, ensure_ascii=False)
        
        return report_file
    
    def get_monitoring_statistics(self) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜"""
        uptime = None
        if self.performance_stats["monitoring_start_time"]:
            uptime = (datetime.now() - self.performance_stats["monitoring_start_time"]).total_seconds()
        
        return {
            **self.performance_stats,
            "monitoring_active": self.monitoring_active,
            "monitoring_uptime_seconds": uptime,
            "monitoring_interval_seconds": self.monitoring_interval,
            "metrics_history_size": {
                metric_type.value: len(self.metrics_history[metric_type])
                for metric_type in PerformanceMetricType
            },
            "alerts_history_size": len(self.alerts_history)
        }
    
    def update_thresholds(self, new_thresholds: Dict[str, Dict[str, float]]):
        """ì„ê³„ê°’ ì—…ë°ì´íŠ¸"""
        for metric_name, thresholds in new_thresholds.items():
            try:
                metric_type = PerformanceMetricType(metric_name)
                self.thresholds[metric_type].update(thresholds)
                logger.info(f"ì„ê³„ê°’ ì—…ë°ì´íŠ¸: {metric_name} -> {thresholds}")
            except ValueError:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ë©”íŠ¸ë¦­ íƒ€ì…: {metric_name}")


# ì „ì—­ ì„±ëŠ¥ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
_performance_monitor: Optional[PerformanceMonitor] = None


def get_performance_monitor() -> PerformanceMonitor:
    """ì „ì—­ ì„±ëŠ¥ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _performance_monitor
    
    if _performance_monitor is None:
        _performance_monitor = PerformanceMonitor()
    
    return _performance_monitor


def reset_performance_monitor():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤ ì¬ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©)"""
    global _performance_monitor
    
    if _performance_monitor and _performance_monitor.monitoring_active:
        _performance_monitor.stop_monitoring()
    
    _performance_monitor = None