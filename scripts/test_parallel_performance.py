#!/usr/bin/env python3
"""
ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‘ì„±ì¼: 2025-07-07
ëª©ì : ìˆœì°¨ ì²˜ë¦¬ vs ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ
"""

import os
import sys
import asyncio
import time
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

from app.collectors.unified_kto_client import UnifiedKTOClient
from app.core.database_manager_extension import get_extended_database_manager
from app.core.concurrent_api_manager import ConcurrencyConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PerformanceTester:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.db_manager = get_extended_database_manager()
    
    async def get_sample_content_ids(self, content_type: str, table_name: str, limit: int = 20) -> list:
        """ìƒ˜í”Œ ì»¨í…ì¸  ID ì¡°íšŒ"""
        
        try:
            query = f"""
            SELECT content_id 
            FROM {table_name} 
            WHERE content_id IS NOT NULL
            ORDER BY created_at DESC
            LIMIT {limit}
            """
            
            results = self.db_manager.fetch_all(query)
            if results:
                return [row['content_id'] for row in results]
            else:
                logger.warning(f"âš ï¸ {table_name}ì—ì„œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
                
        except Exception as e:
            logger.error(f"âŒ {table_name} ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def test_sequential_processing(self, content_ids: list, content_type: str):
        """ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        
        logger.info(f"ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(content_ids)}ê°œ ì»¨í…ì¸ ")
        
        # ìˆœì°¨ ì²˜ë¦¬ìš© í´ë¼ì´ì–¸íŠ¸ (ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™”)
        kto_client = UnifiedKTOClient(enable_parallel=False)
        
        start_time = time.time()
        
        result = await kto_client._collect_detailed_info_sequential(
            content_ids=content_ids,
            content_type_id=content_type,
            store_raw=True
        )
        
        duration = time.time() - start_time
        
        return {
            'method': 'sequential',
            'duration': duration,
            'content_count': len(content_ids),
            'successful_apis': result['detail_common'] + result['detail_intro'] + result['detail_info'] + result['detail_images'],
            'total_possible_apis': len(content_ids) * 4,
            'success_rate': len(result['successful_content_ids']) / len(content_ids) * 100 if content_ids else 0,
            'errors': len(result['errors']),
            'api_per_second': (len(content_ids) * 4) / duration if duration > 0 else 0
        }
    
    async def test_parallel_processing(self, content_ids: list, content_type: str, batch_size: int = 10):
        """ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        
        logger.info(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(content_ids)}ê°œ ì»¨í…ì¸  (ë°°ì¹˜: {batch_size})")
        
        # ë³‘ë ¬ ì²˜ë¦¬ìš© í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        concurrency_config = ConcurrencyConfig(
            max_concurrent_kto=5,      # KTO API ë™ì‹œ í˜¸ì¶œ ìˆ˜
            max_concurrent_total=8,    # ì „ì²´ ë™ì‹œ í˜¸ì¶œ ìˆ˜
            min_delay_between_calls=0.1,
            adaptive_delay=True,
            batch_size=batch_size
        )
        
        kto_client = UnifiedKTOClient(enable_parallel=True, concurrency_config=concurrency_config)
        
        start_time = time.time()
        
        result = await kto_client.collect_detailed_info_parallel(
            content_ids=content_ids,
            content_type_id=content_type,
            store_raw=True,
            batch_size=batch_size
        )
        
        duration = time.time() - start_time
        
        # ì„±ëŠ¥ í†µê³„ ì¡°íšŒ
        performance_stats = {}
        if kto_client.concurrent_manager:
            performance_stats = kto_client.concurrent_manager.get_performance_stats()
        
        return {
            'method': 'parallel',
            'duration': duration,
            'content_count': len(content_ids),
            'batch_size': batch_size,
            'successful_apis': result['detail_common'] + result['detail_intro'] + result['detail_info'] + result['detail_images'],
            'total_possible_apis': len(content_ids) * 4,
            'success_rate': len(result['successful_content_ids']) / len(content_ids) * 100 if content_ids else 0,
            'errors': len(result['errors']),
            'api_per_second': (len(content_ids) * 4) / duration if duration > 0 else 0,
            'concurrent_peaks': performance_stats.get('concurrent_peaks', {}),
            'average_response_time': performance_stats.get('average_response_time', 0),
            'circuit_breaker_trips': performance_stats.get('circuit_breaker_trips', 0)
        }
    
    async def run_performance_comparison(self, content_type: str = "12", table_name: str = "tourist_attractions"):
        """ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        logger.info(f"=== ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘: {table_name} ===")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
        sample_content_ids = await self.get_sample_content_ids(content_type, table_name, limit=20)
        
        if len(sample_content_ids) < 5:
            logger.error(f"âŒ ì¶©ë¶„í•œ ìƒ˜í”Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {len(sample_content_ids)}ê°œ")
            return
        
        # ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸ (5ê°œ)
        small_sample = sample_content_ids[:5]
        
        # í° ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸ (20ê°œ)
        large_sample = sample_content_ids[:20]
        
        results = []
        
        logger.info("\n--- ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ (5ê°œ ì»¨í…ì¸ ) ---")
        
        # ìˆœì°¨ ì²˜ë¦¬ (ì†Œê·œëª¨)
        sequential_small = await self.test_sequential_processing(small_sample, content_type)
        results.append(sequential_small)
        
        # ë³‘ë ¬ ì²˜ë¦¬ (ì†Œê·œëª¨, ë°°ì¹˜ 5)
        parallel_small = await self.test_parallel_processing(small_sample, content_type, batch_size=5)
        results.append(parallel_small)
        
        logger.info("\n--- ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ (20ê°œ ì»¨í…ì¸ ) ---")
        
        # ìˆœì°¨ ì²˜ë¦¬ (ëŒ€ê·œëª¨)
        sequential_large = await self.test_sequential_processing(large_sample, content_type)
        results.append(sequential_large)
        
        # ë³‘ë ¬ ì²˜ë¦¬ (ëŒ€ê·œëª¨, ë°°ì¹˜ 10)
        parallel_large = await self.test_parallel_processing(large_sample, content_type, batch_size=10)
        results.append(parallel_large)
        
        # ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥
        self.print_performance_report(results)
        
        return results
    
    def print_performance_report(self, results):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ† ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        logger.info("="*80)
        
        for result in results:
            method = result['method']
            content_count = result['content_count']
            duration = result['duration']
            success_rate = result['success_rate']
            api_per_second = result['api_per_second']
            
            logger.info(f"\nğŸ“Š {method.upper()} ì²˜ë¦¬ ({content_count}ê°œ ì»¨í…ì¸ ):")
            logger.info(f"  â±ï¸  ì²˜ë¦¬ ì‹œê°„: {duration:.2f}ì´ˆ")
            logger.info(f"  âœ… ì„±ê³µë¥ : {success_rate:.1f}%")
            logger.info(f"  ğŸš€ ì²˜ë¦¬ ì†ë„: {api_per_second:.2f} API/ì´ˆ")
            logger.info(f"  ğŸ¯ ì„±ê³µí•œ API: {result['successful_apis']}/{result['total_possible_apis']}")
            logger.info(f"  âŒ ì˜¤ë¥˜ ìˆ˜: {result['errors']}")
            
            if method == 'parallel':
                logger.info(f"  ğŸ”§ ë°°ì¹˜ í¬ê¸°: {result['batch_size']}")
                logger.info(f"  ğŸ“ˆ ë™ì‹œ ì²˜ë¦¬ í”¼í¬: {result.get('concurrent_peaks', {})}")
                logger.info(f"  âš¡ í‰ê·  ì‘ë‹µì‹œê°„: {result.get('average_response_time', 0):.3f}ì´ˆ")
        
        # ì„±ëŠ¥ ê°œì„  ê³„ì‚°
        if len(results) >= 2:
            sequential_time = next(r['duration'] for r in results if r['method'] == 'sequential')
            parallel_time = next(r['duration'] for r in results if r['method'] == 'parallel')
            
            if parallel_time > 0:
                improvement = (sequential_time - parallel_time) / sequential_time * 100
                speedup = sequential_time / parallel_time
                
                logger.info(f"\nğŸ¯ ì„±ëŠ¥ ê°œì„  íš¨ê³¼:")
                logger.info(f"  âš¡ ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•: {improvement:.1f}%")
                logger.info(f"  ğŸš€ ì†ë„ í–¥ìƒ: {speedup:.1f}ë°°")
        
        logger.info("\n" + "="*80)


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("=== Weather Flick ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    print()
    
    tester = PerformanceTester()
    
    # ê´€ê´‘ì§€ ë°ì´í„°ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    await tester.run_performance_comparison(
        content_type="12",
        table_name="tourist_attractions"
    )
    
    print()
    print("ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    asyncio.run(main())