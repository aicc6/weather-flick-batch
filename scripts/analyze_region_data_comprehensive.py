#!/usr/bin/env python3
"""
ì§€ì—­ì •ë³´ ì¢…í•© ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ë°ì´í„°ë² ì´ìŠ¤ì— ìˆ˜ì§‘ëœ ëª¨ë“  ì§€ì—­ì •ë³´ë¥¼ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import sys
import os
import json
from datetime import datetime
from typing import Dict, List, Any
from collections import defaultdict, Counter

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.database_manager import DatabaseManager
from app.core.database_manager_extension import extend_database_manager


class RegionDataAnalyzer:
    """ì§€ì—­ì •ë³´ ì¢…í•© ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.db_manager = extend_database_manager(DatabaseManager().sync_manager)
    
    def analyze_all_region_data(self) -> Dict[str, Any]:
        """ëª¨ë“  ì§€ì—­ì •ë³´ ë°ì´í„° ì¢…í•© ë¶„ì„"""
        
        analysis = {
            "summary": self._get_data_summary(),
            "regions_analysis": self._analyze_regions_table(),
            "weather_regions_analysis": self._analyze_weather_regions_table(),
            "unified_regions_analysis": self._analyze_unified_regions_table(),
            "integration_quality": self._analyze_integration_quality(),
            "coverage_analysis": self._analyze_geographical_coverage(),
            "data_completeness": self._analyze_data_completeness(),
            "recommendations": self._generate_recommendations()
        }
        
        return analysis
    
    def _get_data_summary(self) -> Dict[str, Any]:
        """ë°ì´í„° ìš”ì•½"""
        
        summary = {}
        
        tables = ['regions', 'weather_regions', 'unified_regions', 'legal_dong_codes']
        
        for table in tables:
            try:
                count_query = f"SELECT COUNT(*) as count FROM {table}"
                result = self.db_manager.fetch_one(count_query)
                summary[table] = result.get('count', 0) if result else 0
            except:
                summary[table] = 0
        
        # ì´ í†µí•© ì§€ì—­ ìˆ˜
        summary['total_integrated_regions'] = summary.get('unified_regions', 0)
        
        # ì¤‘ë³µ ì œê±°ìœ¨ ê³„ì‚°
        total_source = summary.get('regions', 0) + summary.get('weather_regions', 0)
        if total_source > 0:
            summary['deduplication_rate'] = round(
                (1 - summary.get('unified_regions', 0) / total_source) * 100, 2
            )
        
        return summary
    
    def _analyze_regions_table(self) -> Dict[str, Any]:
        """regions í…Œì´ë¸” ìƒì„¸ ë¶„ì„"""
        
        analysis = {}
        
        # ê¸°ë³¸ í†µê³„
        stats_query = """
        SELECT 
            COUNT(*) as total_count,
            COUNT(CASE WHEN latitude IS NOT NULL AND longitude IS NOT NULL THEN 1 END) as with_coordinates,
            COUNT(CASE WHEN parent_region_code IS NOT NULL THEN 1 END) as with_parent,
            AVG(region_level) as avg_level,
            MIN(region_level) as min_level,
            MAX(region_level) as max_level
        FROM regions
        """
        
        stats = self.db_manager.fetch_one(stats_query)
        analysis['basic_stats'] = dict(stats) if stats else {}
        
        # ë ˆë²¨ë³„ ë¶„í¬
        level_query = """
        SELECT region_level, COUNT(*) as count
        FROM regions
        GROUP BY region_level
        ORDER BY region_level
        """
        
        levels = self.db_manager.fetch_all(level_query)
        analysis['level_distribution'] = {str(row['region_level']): row['count'] for row in levels}
        
        # ì¢Œí‘œ ì •ë³´ í˜„í™©
        coord_query = """
        SELECT 
            CASE 
                WHEN latitude IS NOT NULL AND longitude IS NOT NULL THEN 'complete'
                WHEN latitude IS NULL AND longitude IS NULL THEN 'missing'
                ELSE 'partial'
            END as coord_status,
            COUNT(*) as count
        FROM regions
        GROUP BY coord_status
        """
        
        coords = self.db_manager.fetch_all(coord_query)
        analysis['coordinate_status'] = {row['coord_status']: row['count'] for row in coords}
        
        # ìƒìœ„ ì§€ì—­ë³„ í•˜ìœ„ ì§€ì—­ ìˆ˜
        hierarchy_query = """
        SELECT 
            COALESCE(parent_region_code, 'ROOT') as parent,
            COUNT(*) as child_count
        FROM regions
        GROUP BY parent_region_code
        ORDER BY child_count DESC
        LIMIT 10
        """
        
        hierarchy = self.db_manager.fetch_all(hierarchy_query)
        analysis['hierarchy_stats'] = [dict(row) for row in hierarchy]
        
        return analysis
    
    def _analyze_weather_regions_table(self) -> Dict[str, Any]:
        """weather_regions í…Œì´ë¸” ìƒì„¸ ë¶„ì„"""
        
        analysis = {}
        
        # ê¸°ë³¸ í†µê³„
        stats_query = """
        SELECT 
            COUNT(*) as total_count,
            COUNT(CASE WHEN grid_x IS NOT NULL AND grid_y IS NOT NULL THEN 1 END) as with_grid,
            COUNT(CASE WHEN is_active = true THEN 1 END) as active_count,
            AVG(latitude) as avg_lat,
            AVG(longitude) as avg_lon,
            MIN(created_at) as earliest_data,
            MAX(created_at) as latest_data
        FROM weather_regions
        """
        
        stats = self.db_manager.fetch_one(stats_query)
        analysis['basic_stats'] = dict(stats) if stats else {}
        
        # ì§€ì—­ ì½”ë“œ íŒ¨í„´ ë¶„ì„
        pattern_query = """
        SELECT 
            CASE 
                WHEN region_code ~ '^[0-9]+$' THEN 'numeric'
                WHEN region_code ~ '^[0-9]+[A-Z][0-9]+$' THEN 'forecast_code'
                ELSE 'other'
            END as code_pattern,
            COUNT(*) as count
        FROM weather_regions
        WHERE region_code IS NOT NULL
        GROUP BY code_pattern
        """
        
        patterns = self.db_manager.fetch_all(pattern_query)
        analysis['code_patterns'] = {row['code_pattern']: row['count'] for row in patterns}
        
        # ê²©ì ì¢Œí‘œ ë¶„í¬
        grid_query = """
        SELECT 
            CASE 
                WHEN grid_x BETWEEN 1 AND 50 THEN '1-50'
                WHEN grid_x BETWEEN 51 AND 100 THEN '51-100'
                WHEN grid_x BETWEEN 101 AND 150 THEN '101-150'
                ELSE 'other'
            END as grid_x_range,
            COUNT(*) as count
        FROM weather_regions
        WHERE grid_x IS NOT NULL
        GROUP BY grid_x_range
        ORDER BY grid_x_range
        """
        
        grids = self.db_manager.fetch_all(grid_query)
        analysis['grid_distribution'] = {row['grid_x_range']: row['count'] for row in grids}
        
        # ìµœê·¼ ì¶”ê°€ëœ ì˜ˆë³´êµ¬ì—­ (ëª¨ì˜ ë°ì´í„°)
        recent_query = """
        SELECT region_code, region_name, created_at
        FROM weather_regions
        WHERE created_at >= CURRENT_DATE - INTERVAL '1 day'
        ORDER BY created_at DESC
        LIMIT 10
        """
        
        recent = self.db_manager.fetch_all(recent_query)
        analysis['recent_additions'] = [dict(row) for row in recent]
        
        return analysis
    
    def _analyze_unified_regions_table(self) -> Dict[str, Any]:
        """unified_regions í…Œì´ë¸” ìƒì„¸ ë¶„ì„"""
        
        analysis = {}
        
        # ê¸°ë³¸ í†µê³„
        stats_query = """
        SELECT 
            COUNT(*) as total_count,
            COUNT(CASE WHEN parent_region_id IS NOT NULL THEN 1 END) as with_parent,
            COUNT(CASE WHEN center_latitude IS NOT NULL AND center_longitude IS NOT NULL THEN 1 END) as with_coordinates,
            COUNT(CASE WHEN administrative_code IS NOT NULL THEN 1 END) as with_admin_code,
            COUNT(CASE WHEN region_name_en IS NOT NULL THEN 1 END) as with_english_name,
            AVG(region_level) as avg_level
        FROM unified_regions
        """
        
        stats = self.db_manager.fetch_one(stats_query)
        analysis['basic_stats'] = dict(stats) if stats else {}
        
        # ë ˆë²¨ë³„ ë¶„í¬
        level_query = """
        SELECT region_level, COUNT(*) as count
        FROM unified_regions
        GROUP BY region_level
        ORDER BY region_level
        """
        
        levels = self.db_manager.fetch_all(level_query)
        analysis['level_distribution'] = {str(row['region_level']): row['count'] for row in levels}
        
        # í–‰ì •êµ¬ì—­ ì½”ë“œ íŒ¨í„´
        admin_code_query = """
        SELECT 
            CASE 
                WHEN administrative_code IS NULL THEN 'missing'
                WHEN LENGTH(administrative_code) = 2 THEN 'sido_level'
                WHEN LENGTH(administrative_code) = 5 THEN 'sigungu_level'
                WHEN LENGTH(administrative_code) > 5 THEN 'dong_level'
                ELSE 'other'
            END as admin_code_type,
            COUNT(*) as count
        FROM unified_regions
        GROUP BY admin_code_type
        """
        
        admin_codes = self.db_manager.fetch_all(admin_code_query)
        analysis['admin_code_distribution'] = {row['admin_code_type']: row['count'] for row in admin_codes}
        
        # ìµœìƒìœ„ ì§€ì—­ (ë ˆë²¨ 1)
        top_level_query = """
        SELECT region_code, region_name, region_name_full, administrative_code
        FROM unified_regions
        WHERE region_level = 1
        ORDER BY region_name
        """
        
        top_regions = self.db_manager.fetch_all(top_level_query)
        analysis['top_level_regions'] = [dict(row) for row in top_regions]
        
        # ë¶€ëª¨-ìì‹ ê´€ê³„ í†µê³„
        hierarchy_query = """
        SELECT 
            COUNT(CASE WHEN parent_region_id IS NULL THEN 1 END) as root_regions,
            COUNT(CASE WHEN parent_region_id IS NOT NULL THEN 1 END) as child_regions
        FROM unified_regions
        """
        
        hierarchy = self.db_manager.fetch_one(hierarchy_query)
        analysis['hierarchy_stats'] = dict(hierarchy) if hierarchy else {}
        
        return analysis
    
    def _analyze_integration_quality(self) -> Dict[str, Any]:
        """í†µí•© í’ˆì§ˆ ë¶„ì„"""
        
        analysis = {}
        
        # ì§€ì—­ëª… ì¼ì¹˜ë„ ë¶„ì„
        name_match_query = """
        SELECT 
            COUNT(CASE WHEN r.region_name = ur.region_name THEN 1 END) as exact_matches,
            COUNT(CASE WHEN SIMILARITY(r.region_name, ur.region_name) > 0.8 THEN 1 END) as similar_matches,
            COUNT(*) as total_comparisons
        FROM regions r
        CROSS JOIN unified_regions ur
        WHERE r.region_name IS NOT NULL AND ur.region_name IS NOT NULL
        """
        
        try:
            name_matches = self.db_manager.fetch_one(name_match_query)
            analysis['name_matching'] = dict(name_matches) if name_matches else {}
        except:
            # SIMILARITY í•¨ìˆ˜ê°€ ì—†ëŠ” ê²½ìš° ë‹¨ìˆœ ë¹„êµ
            simple_match_query = """
            SELECT 
                COUNT(CASE WHEN r.region_name = ur.region_name THEN 1 END) as exact_matches,
                COUNT(*) as total_comparisons
            FROM regions r
            CROSS JOIN unified_regions ur
            WHERE r.region_name IS NOT NULL AND ur.region_name IS NOT NULL
            """
            name_matches = self.db_manager.fetch_one(simple_match_query)
            analysis['name_matching'] = dict(name_matches) if name_matches else {}
        
        # ì¢Œí‘œ ì •ë³´ í’ˆì§ˆ
        coord_quality_query = """
        SELECT 
            COUNT(CASE WHEN center_latitude BETWEEN 33 AND 39 AND center_longitude BETWEEN 124 AND 132 THEN 1 END) as valid_korea_coords,
            COUNT(CASE WHEN center_latitude IS NOT NULL AND center_longitude IS NOT NULL THEN 1 END) as total_coords,
            COUNT(*) as total_regions
        FROM unified_regions
        """
        
        coord_quality = self.db_manager.fetch_one(coord_quality_query)
        analysis['coordinate_quality'] = dict(coord_quality) if coord_quality else {}
        
        # ì¤‘ë³µ ì§€ì—­ ë¶„ì„
        duplicate_query = """
        SELECT region_name, COUNT(*) as count
        FROM unified_regions
        GROUP BY region_name
        HAVING COUNT(*) > 1
        ORDER BY count DESC
        LIMIT 10
        """
        
        duplicates = self.db_manager.fetch_all(duplicate_query)
        analysis['duplicate_names'] = [dict(row) for row in duplicates]
        
        return analysis
    
    def _analyze_geographical_coverage(self) -> Dict[str, Any]:
        """ì§€ë¦¬ì  ì»¤ë²„ë¦¬ì§€ ë¶„ì„"""
        
        analysis = {}
        
        # ì‹œë„ë³„ í†µí•© ì§€ì—­ ìˆ˜
        sido_coverage_query = """
        SELECT 
            CASE 
                WHEN administrative_code IN ('11', '26', '27', '28', '29', '30', '31', '36') THEN 
                    CASE administrative_code
                        WHEN '11' THEN 'ì„œìš¸íŠ¹ë³„ì‹œ'
                        WHEN '26' THEN 'ë¶€ì‚°ê´‘ì—­ì‹œ'
                        WHEN '27' THEN 'ëŒ€êµ¬ê´‘ì—­ì‹œ'
                        WHEN '28' THEN 'ì¸ì²œê´‘ì—­ì‹œ'
                        WHEN '29' THEN 'ê´‘ì£¼ê´‘ì—­ì‹œ'
                        WHEN '30' THEN 'ëŒ€ì „ê´‘ì—­ì‹œ'
                        WHEN '31' THEN 'ìš¸ì‚°ê´‘ì—­ì‹œ'
                        WHEN '36' THEN 'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ'
                    END
                WHEN administrative_code = '41' THEN 'ê²½ê¸°ë„'
                WHEN administrative_code = '42' THEN 'ê°•ì›íŠ¹ë³„ìì¹˜ë„'
                WHEN administrative_code = '43' THEN 'ì¶©ì²­ë¶ë„'
                WHEN administrative_code = '44' THEN 'ì¶©ì²­ë‚¨ë„'
                WHEN administrative_code = '45' THEN 'ì „ë¶íŠ¹ë³„ìì¹˜ë„'
                WHEN administrative_code = '46' THEN 'ì „ë¼ë‚¨ë„'
                WHEN administrative_code = '47' THEN 'ê²½ìƒë¶ë„'
                WHEN administrative_code = '48' THEN 'ê²½ìƒë‚¨ë„'
                WHEN administrative_code IN ('49', '50') THEN 'ì œì£¼íŠ¹ë³„ìì¹˜ë„'
                ELSE 'ê¸°íƒ€'
            END as sido_name,
            COUNT(*) as region_count
        FROM unified_regions
        WHERE administrative_code IS NOT NULL
        GROUP BY sido_name
        ORDER BY region_count DESC
        """
        
        sido_coverage = self.db_manager.fetch_all(sido_coverage_query)
        analysis['sido_coverage'] = [dict(row) for row in sido_coverage]
        
        # ì¢Œí‘œ ë²”ìœ„ ë¶„ì„
        coord_range_query = """
        SELECT 
            MIN(center_latitude) as min_lat,
            MAX(center_latitude) as max_lat,
            MIN(center_longitude) as min_lon,
            MAX(center_longitude) as max_lon,
            AVG(center_latitude) as avg_lat,
            AVG(center_longitude) as avg_lon
        FROM unified_regions
        WHERE center_latitude IS NOT NULL AND center_longitude IS NOT NULL
        """
        
        coord_range = self.db_manager.fetch_one(coord_range_query)
        analysis['coordinate_range'] = dict(coord_range) if coord_range else {}
        
        return analysis
    
    def _analyze_data_completeness(self) -> Dict[str, Any]:
        """ë°ì´í„° ì™„ì„±ë„ ë¶„ì„"""
        
        analysis = {}
        
        # í•„ìˆ˜ í•„ë“œ ì™„ì„±ë„ (unified_regions ê¸°ì¤€)
        completeness_query = """
        SELECT 
            COUNT(*) as total_regions,
            COUNT(region_code) * 100.0 / COUNT(*) as region_code_completeness,
            COUNT(region_name) * 100.0 / COUNT(*) as region_name_completeness,
            COUNT(center_latitude) * 100.0 / COUNT(*) as coordinate_completeness,
            COUNT(administrative_code) * 100.0 / COUNT(*) as admin_code_completeness,
            COUNT(region_name_full) * 100.0 / COUNT(*) as full_name_completeness,
            COUNT(region_name_en) * 100.0 / COUNT(*) as english_name_completeness
        FROM unified_regions
        """
        
        completeness = self.db_manager.fetch_one(completeness_query)
        analysis['field_completeness'] = dict(completeness) if completeness else {}
        
        # ë°ì´í„° ì†ŒìŠ¤ë³„ ê¸°ì—¬ë„
        source_query = """
        SELECT 
            CASE 
                WHEN region_code ~ '^[0-9]+[A-Z][0-9]+$' THEN 'forecast_regions'
                WHEN LENGTH(region_code) <= 4 THEN 'original_regions'
                WHEN region_code LIKE 'CITY-%' THEN 'city_data'
                ELSE 'other'
            END as data_source,
            COUNT(*) as count
        FROM unified_regions
        GROUP BY data_source
        """
        
        sources = self.db_manager.fetch_all(source_query)
        analysis['data_source_distribution'] = {row['data_source']: row['count'] for row in sources}
        
        return analysis
    
    def _generate_recommendations(self) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ê¸°ë³¸ ë¶„ì„ ë°ì´í„° ì¡°íšŒ
        unified_count_query = "SELECT COUNT(*) as count FROM unified_regions"
        unified_result = self.db_manager.fetch_one(unified_count_query)
        unified_count = unified_result.get('count', 0) if unified_result else 0
        
        coord_missing_query = """
        SELECT COUNT(*) as count FROM unified_regions 
        WHERE center_latitude IS NULL OR center_longitude IS NULL
        """
        coord_missing_result = self.db_manager.fetch_one(coord_missing_query)
        coord_missing = coord_missing_result.get('count', 0) if coord_missing_result else 0
        
        if coord_missing > 0:
            recommendations.append(
                f"ì¢Œí‘œ ì •ë³´ê°€ ëˆ„ë½ëœ {coord_missing}ê°œ ì§€ì—­ì— ëŒ€í•œ ì¢Œí‘œ ë³´ì™„ í•„ìš”"
            )
        
        if unified_count > 300:
            recommendations.append(
                "í†µí•© ì§€ì—­ ìˆ˜ê°€ ë§ìœ¼ë¯€ë¡œ ì¤‘ë³µ ë°ì´í„° ì¬ê²€í†  ë° ì •ë¦¬ ê¶Œì¥"
            )
        
        # ì˜ë¬¸ëª… ëˆ„ë½ í™•ì¸
        english_missing_query = """
        SELECT COUNT(*) as count FROM unified_regions 
        WHERE region_name_en IS NULL AND region_level = 1
        """
        english_missing_result = self.db_manager.fetch_one(english_missing_query)
        english_missing = english_missing_result.get('count', 0) if english_missing_result else 0
        
        if english_missing > 0:
            recommendations.append(
                f"ìµœìƒìœ„ ì§€ì—­ {english_missing}ê°œì˜ ì˜ë¬¸ëª… ì¶”ê°€ í•„ìš”"
            )
        
        recommendations.extend([
            "ê¸°ìƒì²­ API ì‹¤ì œ ì—°ë™ì„ í†µí•œ ì˜ˆë³´êµ¬ì—­ ë°ì´í„° ê°±ì‹  í•„ìš”",
            "ì§€ì—­ë³„ ë‚ ì”¨ ê²©ì ì¢Œí‘œ (nx, ny) ì •ë³´ ë³´ì™„ ê¶Œì¥",
            "ì •ê¸°ì ì¸ ì§€ì—­ì •ë³´ ë™ê¸°í™” ë°°ì¹˜ ì‘ì—… ì„¤ì • í•„ìš”",
            "ì§€ì—­ ê³„ì¸µ êµ¬ì¡° ê²€ì¦ ë° ë¶€ëª¨-ìì‹ ê´€ê³„ ì •ë¦¬"
        ])
        
        return recommendations


def print_analysis_report(analysis: Dict[str, Any]):
    """ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ ì¶œë ¥"""
    
    print("ğŸŒ " + "="*80)
    print("ğŸŒ ì§€ì—­ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ")
    print("ğŸŒ " + "="*80)
    print(f"ğŸ“… ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # 1. ë°ì´í„° ìš”ì•½
    summary = analysis.get('summary', {})
    print("ğŸ“Š **ë°ì´í„° ìš”ì•½**")
    print("-" * 50)
    print(f"â€¢ ê¸°ë³¸ ì§€ì—­ì •ë³´ (regions): {summary.get('regions', 0):,}ê°œ")
    print(f"â€¢ ê¸°ìƒ ì˜ˆë³´êµ¬ì—­ (weather_regions): {summary.get('weather_regions', 0):,}ê°œ")
    print(f"â€¢ í†µí•© ì§€ì—­ì •ë³´ (unified_regions): {summary.get('unified_regions', 0):,}ê°œ")
    print(f"â€¢ ë²•ì •ë™ ì½”ë“œ (legal_dong_codes): {summary.get('legal_dong_codes', 0):,}ê°œ")
    if 'deduplication_rate' in summary:
        print(f"â€¢ ì¤‘ë³µ ì œê±°ìœ¨: {summary['deduplication_rate']}%")
    print()
    
    # 2. Regions í…Œì´ë¸” ë¶„ì„
    regions_analysis = analysis.get('regions_analysis', {})
    if regions_analysis:
        print("ğŸ›ï¸ **ê¸°ë³¸ ì§€ì—­ì •ë³´ (regions) ë¶„ì„**")
        print("-" * 50)
        
        basic_stats = regions_analysis.get('basic_stats', {})
        print(f"â€¢ ì´ ì§€ì—­ ìˆ˜: {basic_stats.get('total_count', 0):,}ê°œ")
        print(f"â€¢ ì¢Œí‘œ ì •ë³´ ë³´ìœ : {basic_stats.get('with_coordinates', 0):,}ê°œ")
        print(f"â€¢ ìƒìœ„ ì§€ì—­ ì—°ê²°: {basic_stats.get('with_parent', 0):,}ê°œ")
        print(f"â€¢ í‰ê·  ë ˆë²¨: {basic_stats.get('avg_level', 0):.1f}")
        
        level_dist = regions_analysis.get('level_distribution', {})
        if level_dist:
            print("â€¢ ë ˆë²¨ë³„ ë¶„í¬:")
            for level, count in level_dist.items():
                level_name = {
                    '1': 'ì‹œë„ê¸‰', '2': 'ì‹œêµ°êµ¬ê¸‰', '3': 'ìë©´ë™ê¸‰'
                }.get(level, f'ë ˆë²¨{level}')
                print(f"  - {level_name}: {count:,}ê°œ")
        
        coord_status = regions_analysis.get('coordinate_status', {})
        if coord_status:
            print("â€¢ ì¢Œí‘œ ì •ë³´ ìƒíƒœ:")
            for status, count in coord_status.items():
                status_name = {
                    'complete': 'ì™„ì „', 'missing': 'ëˆ„ë½', 'partial': 'ë¶€ë¶„'
                }.get(status, status)
                print(f"  - {status_name}: {count:,}ê°œ")
        print()
    
    # 3. Weather Regions í…Œì´ë¸” ë¶„ì„
    weather_analysis = analysis.get('weather_regions_analysis', {})
    if weather_analysis:
        print("ğŸŒ¤ï¸ **ê¸°ìƒ ì˜ˆë³´êµ¬ì—­ (weather_regions) ë¶„ì„**")
        print("-" * 50)
        
        basic_stats = weather_analysis.get('basic_stats', {})
        print(f"â€¢ ì´ ì˜ˆë³´êµ¬ì—­ ìˆ˜: {basic_stats.get('total_count', 0):,}ê°œ")
        print(f"â€¢ ê²©ì ì¢Œí‘œ ë³´ìœ : {basic_stats.get('with_grid', 0):,}ê°œ")
        print(f"â€¢ í™œì„± ìƒíƒœ: {basic_stats.get('active_count', 0):,}ê°œ")
        
        if basic_stats.get('avg_lat') and basic_stats.get('avg_lon'):
            print(f"â€¢ í‰ê·  ì¢Œí‘œ: ({float(basic_stats['avg_lat']):.4f}, {float(basic_stats['avg_lon']):.4f})")
        
        code_patterns = weather_analysis.get('code_patterns', {})
        if code_patterns:
            print("â€¢ ì§€ì—­ ì½”ë“œ íŒ¨í„´:")
            for pattern, count in code_patterns.items():
                pattern_name = {
                    'numeric': 'ìˆ«ìí˜•', 'forecast_code': 'ì˜ˆë³´êµ¬ì—­ì½”ë“œ', 'other': 'ê¸°íƒ€'
                }.get(pattern, pattern)
                print(f"  - {pattern_name}: {count:,}ê°œ")
        
        recent_additions = weather_analysis.get('recent_additions', [])
        if recent_additions:
            print("â€¢ ìµœê·¼ ì¶”ê°€ëœ ì˜ˆë³´êµ¬ì—­ (ìƒìœ„ 5ê°œ):")
            for region in recent_additions[:5]:
                print(f"  - {region['region_code']}: {region['region_name']}")
        print()
    
    # 4. Unified Regions í…Œì´ë¸” ë¶„ì„
    unified_analysis = analysis.get('unified_regions_analysis', {})
    if unified_analysis:
        print("ğŸ—ºï¸ **í†µí•© ì§€ì—­ì •ë³´ (unified_regions) ë¶„ì„**")
        print("-" * 50)
        
        basic_stats = unified_analysis.get('basic_stats', {})
        print(f"â€¢ ì´ í†µí•©ì§€ì—­ ìˆ˜: {basic_stats.get('total_count', 0):,}ê°œ")
        print(f"â€¢ ë¶€ëª¨ ì§€ì—­ ì—°ê²°: {basic_stats.get('with_parent', 0):,}ê°œ")
        print(f"â€¢ ì¢Œí‘œ ì •ë³´ ë³´ìœ : {basic_stats.get('with_coordinates', 0):,}ê°œ")
        print(f"â€¢ í–‰ì •êµ¬ì—­ì½”ë“œ ë³´ìœ : {basic_stats.get('with_admin_code', 0):,}ê°œ")
        print(f"â€¢ ì˜ë¬¸ëª… ë³´ìœ : {basic_stats.get('with_english_name', 0):,}ê°œ")
        
        level_dist = unified_analysis.get('level_distribution', {})
        if level_dist:
            print("â€¢ ë ˆë²¨ë³„ ë¶„í¬:")
            for level, count in level_dist.items():
                level_name = {
                    '1': 'ìµœìƒìœ„ (ì‹œë„)', '2': 'ì¤‘ê°„ (ì‹œêµ°êµ¬)', '3': 'í•˜ìœ„ (ìë©´ë™)'
                }.get(level, f'ë ˆë²¨{level}')
                print(f"  - {level_name}: {count:,}ê°œ")
        
        top_regions = unified_analysis.get('top_level_regions', [])
        if top_regions:
            print("â€¢ ìµœìƒìœ„ ì§€ì—­ (ì‹œë„ê¸‰):")
            for region in top_regions[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                admin_code = region.get('administrative_code', 'N/A')
                print(f"  - {region['region_name']} ({admin_code})")
        print()
    
    # 5. í†µí•© í’ˆì§ˆ ë¶„ì„
    integration_quality = analysis.get('integration_quality', {})
    if integration_quality:
        print("ğŸ” **ë°ì´í„° í†µí•© í’ˆì§ˆ ë¶„ì„**")
        print("-" * 50)
        
        coord_quality = integration_quality.get('coordinate_quality', {})
        if coord_quality:
            total_coords = coord_quality.get('total_coords', 0)
            valid_coords = coord_quality.get('valid_korea_coords', 0)
            if total_coords > 0:
                validity_rate = (valid_coords / total_coords) * 100
                print(f"â€¢ ì¢Œí‘œ ìœ íš¨ì„±: {valid_coords:,}/{total_coords:,} ({validity_rate:.1f}%)")
        
        duplicates = integration_quality.get('duplicate_names', [])
        if duplicates:
            print("â€¢ ì¤‘ë³µ ì§€ì—­ëª… (ìƒìœ„ 5ê°œ):")
            for dup in duplicates[:5]:
                print(f"  - '{dup['region_name']}': {dup['count']}ê°œ")
        print()
    
    # 6. ì§€ë¦¬ì  ì»¤ë²„ë¦¬ì§€
    coverage = analysis.get('coverage_analysis', {})
    if coverage:
        print("ğŸ—¾ **ì§€ë¦¬ì  ì»¤ë²„ë¦¬ì§€ ë¶„ì„**")
        print("-" * 50)
        
        sido_coverage = coverage.get('sido_coverage', [])
        if sido_coverage:
            print("â€¢ ì‹œë„ë³„ í†µí•©ì§€ì—­ ìˆ˜:")
            for sido in sido_coverage[:10]:  # ìƒìœ„ 10ê°œ
                print(f"  - {sido['sido_name']}: {sido['region_count']:,}ê°œ")
        
        coord_range = coverage.get('coordinate_range', {})
        if coord_range and coord_range.get('min_lat'):
            print("â€¢ ì¢Œí‘œ ë²”ìœ„:")
            print(f"  - ìœ„ë„: {float(coord_range['min_lat']):.4f} ~ {float(coord_range['max_lat']):.4f}")
            print(f"  - ê²½ë„: {float(coord_range['min_lon']):.4f} ~ {float(coord_range['max_lon']):.4f}")
        print()
    
    # 7. ë°ì´í„° ì™„ì„±ë„
    completeness = analysis.get('data_completeness', {})
    if completeness:
        print("ğŸ“ˆ **ë°ì´í„° ì™„ì„±ë„ ë¶„ì„**")
        print("-" * 50)
        
        field_completeness = completeness.get('field_completeness', {})
        if field_completeness:
            print("â€¢ í•„ë“œë³„ ì™„ì„±ë„:")
            fields = {
                'region_code_completeness': 'ì§€ì—­ì½”ë“œ',
                'region_name_completeness': 'ì§€ì—­ëª…',
                'coordinate_completeness': 'ì¢Œí‘œì •ë³´',
                'admin_code_completeness': 'í–‰ì •êµ¬ì—­ì½”ë“œ',
                'full_name_completeness': 'ì „ì²´ëª…',
                'english_name_completeness': 'ì˜ë¬¸ëª…'
            }
            
            for field, name in fields.items():
                if field in field_completeness:
                    rate = float(field_completeness[field])
                    print(f"  - {name}: {rate:.1f}%")
        
        source_dist = completeness.get('data_source_distribution', {})
        if source_dist:
            print("â€¢ ë°ì´í„° ì†ŒìŠ¤ë³„ ë¶„í¬:")
            source_names = {
                'forecast_regions': 'ê¸°ìƒ ì˜ˆë³´êµ¬ì—­',
                'original_regions': 'ê¸°ì¡´ ì§€ì—­ì •ë³´',
                'city_data': 'ë„ì‹œ ë°ì´í„°',
                'other': 'ê¸°íƒ€'
            }
            for source, count in source_dist.items():
                name = source_names.get(source, source)
                print(f"  - {name}: {count:,}ê°œ")
        print()
    
    # 8. ê¶Œì¥ì‚¬í•­
    recommendations = analysis.get('recommendations', [])
    if recommendations:
        print("ğŸ’¡ **ê°œì„  ê¶Œì¥ì‚¬í•­**")
        print("-" * 50)
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")
        print()
    
    print("ğŸŒ " + "="*80)
    print("ğŸŒ ë¶„ì„ ì™„ë£Œ")
    print("ğŸŒ " + "="*80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    try:
        print("ğŸ” ì§€ì—­ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ ì¢…í•© ë¶„ì„ ì‹œì‘...\n")
        
        analyzer = RegionDataAnalyzer()
        analysis_result = analyzer.analyze_all_region_data()
        
        # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        print_analysis_report(analysis_result)
        
        # JSON íŒŒì¼ë¡œ ìƒì„¸ ê²°ê³¼ ì €ì¥
        output_file = f"region_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
        def json_serial(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            raise TypeError(f"Type {type(obj)} not serializable")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2, default=json_serial)
        
        print(f"\nğŸ“„ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()