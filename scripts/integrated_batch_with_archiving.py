#!/usr/bin/env python3
"""
í†µí•© ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ (ì•„ì¹´ì´ë¹™ í¬í•¨)

ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘, ì €ì¥, ëª¨ë‹ˆí„°ë§, ì•„ì¹´ì´ë¹™ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ë©”ì¸ ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

import sys
import os
import asyncio
import logging
import argparse
from datetime import datetime, timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.unified_api_client import get_unified_api_client
from app.collectors.unified_kto_client import UnifiedKTOClient
from scripts.collect_forecast_regions import ForecastRegionCollector
from app.monitoring.monitoring_manager import get_monitoring_manager, MonitoringConfig
from app.monitoring.alert_system import get_alert_system, setup_default_alerts
from app.archiving.archival_engine import get_archival_engine
from app.archiving.backup_manager import get_backup_manager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IntegratedBatchProcessor:
    """í†µí•© ë°°ì¹˜ ì²˜ë¦¬ê¸° (ì•„ì¹´ì´ë¹™ í¬í•¨)"""
    
    def __init__(self, enable_monitoring: bool = True, enable_archiving: bool = True):
        """
        í†µí•© ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            enable_monitoring: ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í™œì„±í™”
            enable_archiving: ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ í™œì„±í™”
        """
        self.enable_monitoring = enable_monitoring
        self.enable_archiving = enable_archiving
        
        # í•µì‹¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.api_client = get_unified_api_client()
        self.kto_client = UnifiedKTOClient(enable_parallel=True)
        
        # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        if enable_monitoring:
            setup_default_alerts()
            self.monitoring_manager = get_monitoring_manager()
            self.alert_system = get_alert_system()
        else:
            self.monitoring_manager = None
            self.alert_system = None
        
        # ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ
        if enable_archiving:
            self.archival_engine = get_archival_engine()
            self.backup_manager = get_backup_manager()
        else:
            self.archival_engine = None
            self.backup_manager = None
        
        # ë°°ì¹˜ í†µê³„
        self.batch_stats = {
            "start_time": None,
            "end_time": None,
            "duration_seconds": 0,
            "tasks_completed": 0,
            "tasks_failed": 0,
            "data_collected": 0,
            "data_archived": 0,
            "archival_compression_ratio": 0.0
        }
    
    async def run_full_batch(self, config: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ì „ì²´ ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            config: ë°°ì¹˜ ì„¤ì •
        
        Returns:
            ë°°ì¹˜ ì‹¤í–‰ ê²°ê³¼
        """
        self.batch_stats["start_time"] = datetime.now()
        
        try:
            logger.info("ğŸš€ í†µí•© ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (ì•„ì¹´ì´ë¹™ í¬í•¨)")
            
            # ëª¨ë‹ˆí„°ë§ ì‹œì‘
            if self.monitoring_manager:
                await self.monitoring_manager.start_monitoring()
                logger.info("ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘ë¨")
            
            # 1. ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„
            collection_results = await self._run_data_collection(config)
            
            # 2. ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜ ë‹¨ê³„
            processing_results = await self._run_data_processing(config)
            
            # 3. ì•„ì¹´ì´ë¹™ ë‹¨ê³„
            archival_results = {}
            if self.enable_archiving:
                archival_results = await self._run_archival_process(config)
            
            # 4. ì •ë¦¬ ë° ìµœì í™” ë‹¨ê³„
            cleanup_results = await self._run_cleanup_process(config)
            
            # ë°°ì¹˜ ì™„ë£Œ
            self.batch_stats["end_time"] = datetime.now()
            self.batch_stats["duration_seconds"] = (
                self.batch_stats["end_time"] - self.batch_stats["start_time"]
            ).total_seconds()
            
            # ìµœì¢… ê²°ê³¼ ì»´íŒŒì¼
            final_results = {
                "success": True,
                "batch_stats": self.batch_stats,
                "collection_results": collection_results,
                "processing_results": processing_results,
                "archival_results": archival_results,
                "cleanup_results": cleanup_results,
                "monitoring_summary": self._get_monitoring_summary() if self.monitoring_manager else None
            }
            
            # ì„±ê³µ ì•Œë¦¼
            if self.alert_system:
                await self._send_batch_completion_alert(final_results)
            
            logger.info(f"âœ… í†µí•© ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ (ì´ {self.batch_stats['duration_seconds']:.1f}ì´ˆ)")
            
            return final_results
            
        except Exception as e:
            # ì˜¤ë¥˜ ì²˜ë¦¬
            self.batch_stats["end_time"] = datetime.now()
            self.batch_stats["duration_seconds"] = (
                self.batch_stats["end_time"] - self.batch_stats["start_time"]
            ).total_seconds()
            
            error_result = {
                "success": False,
                "error": str(e),
                "batch_stats": self.batch_stats
            }
            
            # ì˜¤ë¥˜ ì•Œë¦¼
            if self.alert_system:
                await self._send_batch_error_alert(error_result)
            
            logger.error(f"âŒ í†µí•© ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return error_result
        
        finally:
            # ëª¨ë‹ˆí„°ë§ ì •ë¦¬
            if self.monitoring_manager:
                await self.monitoring_manager.stop_monitoring()
    
    async def _run_data_collection(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„"""
        logger.info("ğŸ“¡ ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ ì‹œì‘")
        
        collection_results = {
            "kto_data": {},
            "forecast_regions": {},
            "total_items_collected": 0
        }
        
        try:
            # KTO ë°ì´í„° ìˆ˜ì§‘
            if config.get("collect_kto_data", True):
                logger.info("KTO ê´€ê´‘ì§€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                
                async with self.api_client:
                    kto_result = await self.kto_client.collect_all_data(
                        content_types=config.get("kto_content_types"),
                        area_codes=config.get("kto_area_codes"),
                        store_raw=True,
                        auto_transform=True
                    )
                
                collection_results["kto_data"] = kto_result
                self.batch_stats["data_collected"] += kto_result.get("total_collected", 0)
                self.batch_stats["tasks_completed"] += 1
                
                logger.info(f"KTO ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {kto_result.get('total_collected', 0)}ê°œ")
            
            # ê¸°ìƒì²­ ì˜ˆë³´êµ¬ì—­ ìˆ˜ì§‘
            if config.get("collect_forecast_regions", True):
                logger.info("ê¸°ìƒì²­ ì˜ˆë³´êµ¬ì—­ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                
                async with ForecastRegionCollector() as collector:
                    regions = await collector.collect_forecast_regions()
                    saved_count = await collector.save_forecast_regions(regions)
                    
                    collection_results["forecast_regions"] = {
                        "collected": len(regions),
                        "saved": saved_count,
                        "storage_stats": collector.get_storage_statistics()
                    }
                
                self.batch_stats["data_collected"] += len(regions)
                self.batch_stats["tasks_completed"] += 1
                
                logger.info(f"ì˜ˆë³´êµ¬ì—­ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(regions)}ê°œ")
            
            collection_results["total_items_collected"] = self.batch_stats["data_collected"]
            
        except Exception as e:
            self.batch_stats["tasks_failed"] += 1
            logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            collection_results["error"] = str(e)
        
        return collection_results
    
    async def _run_data_processing(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜ ë‹¨ê³„"""
        logger.info("âš™ï¸ ë°ì´í„° ì²˜ë¦¬ ë‹¨ê³„ ì‹œì‘")
        
        processing_results = {
            "transformation_applied": 0,
            "quality_checks_passed": 0,
            "errors": []
        }
        
        try:
            # ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            if config.get("enable_data_transformation", True):
                # ì‹¤ì œ ë³€í™˜ ë¡œì§ì€ ë°ì´í„° ìˆ˜ì§‘ ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë¨
                processing_results["transformation_applied"] = self.batch_stats["data_collected"]
                self.batch_stats["tasks_completed"] += 1
            
            # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
            if config.get("enable_quality_checks", True):
                # ê¸°ë³¸ì ì¸ í’ˆì§ˆ ê²€ì‚¬ëŠ” ì €ì¥ ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë¨
                processing_results["quality_checks_passed"] = self.batch_stats["data_collected"]
                self.batch_stats["tasks_completed"] += 1
            
        except Exception as e:
            self.batch_stats["tasks_failed"] += 1
            processing_results["errors"].append(str(e))
            logger.error(f"ë°ì´í„° ì²˜ë¦¬ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
        
        return processing_results
    
    async def _run_archival_process(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ì•„ì¹´ì´ë¹™ í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„"""
        logger.info("ğŸ—„ï¸ ì•„ì¹´ì´ë¹™ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        archival_results = {}
        
        try:
            if not self.archival_engine:
                logger.warning("ì•„ì¹´ì´ë¹™ ì—”ì§„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                return {"skipped": "archiving_disabled"}
            
            # ì•„ì¹´ì´ë¹™ ì‹¤í–‰
            dry_run = config.get("archival_dry_run", False)
            api_provider = config.get("archival_api_provider")
            
            summary = await self.archival_engine.run_archival_process(
                api_provider=api_provider,
                dry_run=dry_run
            )
            
            archival_results = {
                "total_candidates": summary.total_candidates,
                "processed_items": summary.processed_items,
                "successful_backups": summary.successful_backups,
                "failed_backups": summary.failed_backups,
                "skipped_items": summary.skipped_items,
                "total_original_size_mb": summary.total_original_size_mb,
                "total_compressed_size_mb": summary.total_compressed_size_mb,
                "average_compression_ratio": summary.average_compression_ratio,
                "processing_time_seconds": summary.processing_time_seconds,
                "dry_run": dry_run
            }
            
            self.batch_stats["data_archived"] = summary.successful_backups
            self.batch_stats["archival_compression_ratio"] = summary.average_compression_ratio
            self.batch_stats["tasks_completed"] += 1
            
            logger.info(f"ì•„ì¹´ì´ë¹™ ì™„ë£Œ: {summary.successful_backups}ê°œ ë°±ì—…, "
                       f"ì••ì¶•ë¥  {summary.average_compression_ratio:.1f}%")
            
            # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬ (ì„ íƒì )
            if config.get("cleanup_old_backups", False) and not dry_run:
                cleanup_count = await self.backup_manager.cleanup_old_backups()
                archival_results["cleaned_backups"] = cleanup_count
                logger.info(f"ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬ ì™„ë£Œ: {cleanup_count}ê°œ")
            
        except Exception as e:
            self.batch_stats["tasks_failed"] += 1
            archival_results["error"] = str(e)
            logger.error(f"ì•„ì¹´ì´ë¹™ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        
        return archival_results
    
    async def _run_cleanup_process(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ì •ë¦¬ ë° ìµœì í™” ë‹¨ê³„"""
        logger.info("ğŸ§¹ ì •ë¦¬ ë° ìµœì í™” ë‹¨ê³„ ì‹œì‘")
        
        cleanup_results = {
            "expired_data_cleaned": 0,
            "cache_optimized": False,
            "storage_optimized": False
        }
        
        try:
            # ë§Œë£Œëœ ì›ë³¸ ë°ì´í„° ì •ë¦¬
            if config.get("cleanup_expired_data", True):
                async with self.api_client:
                    expired_count = await self.api_client.cleanup_expired_data()
                    cleanup_results["expired_data_cleaned"] = expired_count
                    logger.info(f"ë§Œë£Œëœ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {expired_count}ê°œ")
            
            # ìºì‹œ ìµœì í™”
            if config.get("optimize_cache", True):
                # ìºì‹œ ìµœì í™” ë¡œì§ (êµ¬í˜„ ì˜ˆì •)
                cleanup_results["cache_optimized"] = True
            
            # ì €ì¥ì†Œ ìµœì í™”
            if config.get("optimize_storage", True):
                # ì €ì¥ì†Œ ìµœì í™” ë¡œì§ (êµ¬í˜„ ì˜ˆì •)
                cleanup_results["storage_optimized"] = True
            
            self.batch_stats["tasks_completed"] += 1
            
        except Exception as e:
            self.batch_stats["tasks_failed"] += 1
            cleanup_results["error"] = str(e)
            logger.error(f"ì •ë¦¬ ë° ìµœì í™” ë‹¨ê³„ ì‹¤íŒ¨: {e}")
        
        return cleanup_results
    
    def _get_monitoring_summary(self) -> Optional[Dict[str, Any]]:
        """ëª¨ë‹ˆí„°ë§ ìš”ì•½ ì •ë³´"""
        if not self.monitoring_manager:
            return None
        
        try:
            return self.monitoring_manager.get_monitoring_status()
        except Exception as e:
            logger.error(f"ëª¨ë‹ˆí„°ë§ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def _send_batch_completion_alert(self, results: Dict[str, Any]):
        """ë°°ì¹˜ ì™„ë£Œ ì•Œë¦¼"""
        try:
            duration = self.batch_stats["duration_seconds"]
            data_collected = self.batch_stats["data_collected"]
            data_archived = self.batch_stats["data_archived"]
            compression_ratio = self.batch_stats["archival_compression_ratio"]
            
            message = f"""í†µí•© ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ:

ğŸ“Š ì‹¤í–‰ ìš”ì•½:
â€¢ ì²˜ë¦¬ ì‹œê°„: {duration:.1f}ì´ˆ
â€¢ ìˆ˜ì§‘ëœ ë°ì´í„°: {data_collected:,}ê°œ
â€¢ ì•„ì¹´ì´ë¹™ëœ ë°ì´í„°: {data_archived:,}ê°œ
â€¢ ì••ì¶•ë¥ : {compression_ratio:.1f}%
â€¢ ì™„ë£Œëœ ì‘ì—…: {self.batch_stats['tasks_completed']}ê°œ
â€¢ ì‹¤íŒ¨í•œ ì‘ì—…: {self.batch_stats['tasks_failed']}ê°œ

âœ… ëª¨ë“  ë°°ì¹˜ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."""
            
            from app.monitoring.alert_system import AlertSeverity
            
            result = self.alert_system.send_system_alert(
                title="ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ",
                message=message,
                severity=AlertSeverity.INFO,
                source="integrated_batch",
                metadata=results
            )
            
            logger.info("ë°°ì¹˜ ì™„ë£Œ ì•Œë¦¼ ì „ì†¡ë¨")
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì™„ë£Œ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    async def _send_batch_error_alert(self, error_result: Dict[str, Any]):
        """ë°°ì¹˜ ì˜¤ë¥˜ ì•Œë¦¼"""
        try:
            duration = self.batch_stats["duration_seconds"]
            error = error_result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
            
            message = f"""í†µí•© ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨:

âŒ ì˜¤ë¥˜ ì •ë³´:
â€¢ ì˜¤ë¥˜ ë‚´ìš©: {error}
â€¢ ì‹¤í–‰ ì‹œê°„: {duration:.1f}ì´ˆ
â€¢ ì™„ë£Œëœ ì‘ì—…: {self.batch_stats['tasks_completed']}ê°œ
â€¢ ì‹¤íŒ¨í•œ ì‘ì—…: {self.batch_stats['tasks_failed']}ê°œ

ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."""
            
            from app.monitoring.alert_system import AlertSeverity
            
            result = self.alert_system.send_system_alert(
                title="ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨",
                message=message,
                severity=AlertSeverity.CRITICAL,
                source="integrated_batch",
                metadata=error_result
            )
            
            logger.info("ë°°ì¹˜ ì˜¤ë¥˜ ì•Œë¦¼ ì „ì†¡ë¨")
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì˜¤ë¥˜ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í†µí•© ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ (ì•„ì¹´ì´ë¹™ í¬í•¨)")
    parser.add_argument("--config", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--no-monitoring", action="store_true", help="ëª¨ë‹ˆí„°ë§ ë¹„í™œì„±í™”")
    parser.add_argument("--no-archiving", action="store_true", help="ì•„ì¹´ì´ë¹™ ë¹„í™œì„±í™”")
    parser.add_argument("--kto-only", action="store_true", help="KTO ë°ì´í„°ë§Œ ìˆ˜ì§‘")
    parser.add_argument("--forecast-only", action="store_true", help="ì˜ˆë³´êµ¬ì—­ ë°ì´í„°ë§Œ ìˆ˜ì§‘")
    parser.add_argument("--archival-dry-run", action="store_true", help="ì•„ì¹´ì´ë¹™ ë“œë¼ì´ëŸ° ëª¨ë“œ")
    parser.add_argument("--archival-provider", help="íŠ¹ì • API ì œê³µìë§Œ ì•„ì¹´ì´ë¹™")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ë°°ì¹˜ ì„¤ì •
    batch_config = {
        "collect_kto_data": not args.forecast_only,
        "collect_forecast_regions": not args.kto_only,
        "enable_data_transformation": True,
        "enable_quality_checks": True,
        "cleanup_expired_data": True,
        "optimize_cache": True,
        "optimize_storage": True,
        "archival_dry_run": args.archival_dry_run,
        "archival_api_provider": args.archival_provider,
        "cleanup_old_backups": True
    }
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ (êµ¬í˜„ ì˜ˆì •)
    if args.config:
        logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë“œ: {args.config}")
        # TODO: YAML/JSON ì„¤ì • íŒŒì¼ ë¡œë“œ ë¡œì§
    
    try:
        # í†µí•© ë°°ì¹˜ ì²˜ë¦¬ê¸° ì‹¤í–‰
        processor = IntegratedBatchProcessor(
            enable_monitoring=not args.no_monitoring,
            enable_archiving=not args.no_archiving
        )
        
        results = await processor.run_full_batch(batch_config)
        
        # ê²°ê³¼ ì¶œë ¥
        if results["success"]:
            print("\nğŸ‰ í†µí•© ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ!")
            print(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {results['batch_stats']['duration_seconds']:.1f}ì´ˆ")
            print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°: {results['batch_stats']['data_collected']:,}ê°œ")
            if results.get("archival_results"):
                arch_results = results["archival_results"]
                print(f"ğŸ—„ï¸  ì•„ì¹´ì´ë¹™ëœ ë°ì´í„°: {arch_results.get('successful_backups', 0)}ê°œ")
                print(f"ğŸ“¦ í‰ê·  ì••ì¶•ë¥ : {arch_results.get('average_compression_ratio', 0):.1f}%")
        else:
            print(f"\nâŒ í†µí•© ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            sys.exit(1)
    
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())