#!/usr/bin/env python3
"""
ìºì‹œ ì„±ëŠ¥ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸

Redis ìºì‹œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ìµœì í™”í•˜ëŠ” í†µí•© ë„êµ¬ì…ë‹ˆë‹¤.
- ìºì‹œ ì„±ëŠ¥ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
- ìºì‹œ ì›Œë° ì‹¤í–‰
- TTL ìµœì í™” ì œì•ˆ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
- ìºì‹œ ë¬´íš¨í™” ì „ëµ í…ŒìŠ¤íŠ¸

ì‹¤í–‰ ë°©ë²•:
python scripts/cache_performance_optimizer.py [--action analyze|warm|optimize|test] [--config-level aggressive]

ì˜µì…˜:
--action: ì‹¤í–‰í•  ì‘ì—… (analyze, warm, optimize, test, monitor)
--config-level: ìµœì í™” ë ˆë²¨ (conservative, balanced, aggressive)
--duration: ëª¨ë‹ˆí„°ë§ ì§€ì† ì‹œê°„ (ë¶„, ê¸°ë³¸: 60)
--report: ë³´ê³ ì„œ ì €ì¥ (ê¸°ë³¸: True)
"""

import sys
import argparse
import asyncio
import time
import json
import traceback
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from app.core.advanced_cache_manager import (
    get_advanced_cache_manager, 
    CacheWarming
)
from app.core.cache_monitoring import get_cache_monitor
from app.core.logger import get_logger
from utils.redis_client import RedisClient


class CachePerformanceOptimizer:
    """ìºì‹œ ì„±ëŠ¥ ìµœì í™” ë„êµ¬"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.cache_manager = get_advanced_cache_manager()
        self.cache_monitor = get_cache_monitor()
        self.redis_client = RedisClient()
        
        # ìµœì í™” ì„¤ì •
        self.optimization_levels = {
            "conservative": {
                "batch_size": 50,
                "concurrent_workers": 2,
                "warming_enabled": True,
                "aggressive_ttl": False,
                "memory_threshold": 70
            },
            "balanced": {
                "batch_size": 100,
                "concurrent_workers": 5,
                "warming_enabled": True,
                "aggressive_ttl": True,
                "memory_threshold": 80
            },
            "aggressive": {
                "batch_size": 200,
                "concurrent_workers": 10,
                "warming_enabled": True,
                "aggressive_ttl": True,
                "memory_threshold": 90
            }
        }
    
    async def analyze_cache_performance(self, save_report: bool = True) -> Dict[str, Any]:
        """ìºì‹œ ì„±ëŠ¥ ì¢…í•© ë¶„ì„"""
        self.logger.info("ìºì‹œ ì„±ëŠ¥ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        analysis_result = {
            "timestamp": datetime.now().isoformat(),
            "redis_info": {},
            "performance_metrics": {},
            "optimization_suggestions": [],
            "health_status": {},
            "memory_analysis": {},
            "hit_rate_analysis": {},
            "bottleneck_analysis": {}
        }
        
        try:
            # 1. Redis ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            analysis_result["redis_info"] = await self._analyze_redis_info()
            
            # 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            analysis_result["performance_metrics"] = await self._collect_performance_metrics()
            
            # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
            analysis_result["memory_analysis"] = await self._analyze_memory_usage()
            
            # 4. íˆíŠ¸ìœ¨ ë¶„ì„
            analysis_result["hit_rate_analysis"] = await self._analyze_hit_rate()
            
            # 5. ë³‘ëª© í˜„ìƒ ë¶„ì„
            analysis_result["bottleneck_analysis"] = await self._analyze_bottlenecks()
            
            # 6. ê±´ê°• ìƒíƒœ í™•ì¸
            analysis_result["health_status"] = await self.cache_manager.get_cache_health()
            
            # 7. ìµœì í™” ì œì•ˆ ìƒì„±
            analysis_result["optimization_suggestions"] = await self.cache_monitor.generate_optimization_suggestions()
            
            # 8. ë³´ê³ ì„œ ì €ì¥
            if save_report:
                await self._save_analysis_report(analysis_result)
            
            self.logger.info("ìºì‹œ ì„±ëŠ¥ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            analysis_result["error"] = str(e)
            return analysis_result
    
    async def _analyze_redis_info(self) -> Dict[str, Any]:
        """Redis ê¸°ë³¸ ì •ë³´ ë¶„ì„"""
        try:
            info = await self.redis_client.client.info()
            
            return {
                "version": info.get("redis_version", "unknown"),
                "mode": info.get("redis_mode", "unknown"),
                "uptime_days": info.get("uptime_in_seconds", 0) / 86400,
                "connected_clients": info.get("connected_clients", 0),
                "blocked_clients": info.get("blocked_clients", 0),
                "used_memory_mb": info.get("used_memory", 0) / 1024 / 1024,
                "used_memory_peak_mb": info.get("used_memory_peak", 0) / 1024 / 1024,
                "maxmemory_mb": info.get("maxmemory", 0) / 1024 / 1024,
                "total_commands_processed": info.get("total_commands_processed", 0),
                "instantaneous_ops_per_sec": info.get("instantaneous_ops_per_sec", 0),
                "keyspace_hits": info.get("keyspace_hits", 0),
                "keyspace_misses": info.get("keyspace_misses", 0),
                "evicted_keys": info.get("evicted_keys", 0),
                "expired_keys": info.get("expired_keys", 0)
            }
            
        except Exception as e:
            self.logger.error(f"Redis ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def _collect_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # ìºì‹œ ë§¤ë‹ˆì € ë©”íŠ¸ë¦­
            cache_metrics = await self.cache_manager.get_cache_metrics()
            
            # ì„±ëŠ¥ ìš”ì•½ (ìµœê·¼ 1ì‹œê°„)
            performance_summary = await self.cache_monitor.get_performance_summary(hours=1)
            
            return {
                "cache_metrics": {
                    "hit_count": cache_metrics.hit_count,
                    "miss_count": cache_metrics.miss_count,
                    "hit_rate": cache_metrics.hit_rate,
                    "total_requests": cache_metrics.total_requests,
                    "avg_response_time_ms": cache_metrics.avg_response_time_ms,
                    "cache_size_mb": cache_metrics.cache_size_mb,
                    "memory_usage_percent": cache_metrics.memory_usage_percent
                },
                "performance_summary": performance_summary
            }
            
        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def _analyze_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        try:
            info = await self.redis_client.client.info("memory")
            
            used_memory = info.get("used_memory", 0)
            used_memory_peak = info.get("used_memory_peak", 0)
            maxmemory = info.get("maxmemory", 0)
            
            # ë°ì´í„°ë² ì´ìŠ¤ë³„ í‚¤ ê°œìˆ˜
            db_info = await self.redis_client.client.info("keyspace")
            keyspace_analysis = {}
            
            for key, value in db_info.items():
                if key.startswith("db"):
                    # ì˜ˆ: db0: keys=1000,expires=100,avg_ttl=3600
                    keyspace_analysis[key] = value
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³„ì‚°
            memory_efficiency = "good"
            usage_percent = (used_memory / maxmemory * 100) if maxmemory > 0 else 0
            
            if usage_percent > 90:
                memory_efficiency = "critical"
            elif usage_percent > 80:
                memory_efficiency = "warning"
            elif usage_percent > 70:
                memory_efficiency = "moderate"
            
            return {
                "used_memory_mb": used_memory / 1024 / 1024,
                "used_memory_peak_mb": used_memory_peak / 1024 / 1024,
                "maxmemory_mb": maxmemory / 1024 / 1024,
                "usage_percent": usage_percent,
                "memory_efficiency": memory_efficiency,
                "fragmentation_ratio": info.get("mem_fragmentation_ratio", 0),
                "keyspace_analysis": keyspace_analysis,
                "rss_overhead_bytes": info.get("used_memory_rss", 0) - used_memory,
                "dataset_overhead_bytes": info.get("used_memory_overhead", 0)
            }
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def _analyze_hit_rate(self) -> Dict[str, Any]:
        """íˆíŠ¸ìœ¨ ë¶„ì„"""
        try:
            info = await self.redis_client.client.info("stats")
            
            hits = info.get("keyspace_hits", 0)
            misses = info.get("keyspace_misses", 0)
            total_requests = hits + misses
            
            hit_rate = (hits / total_requests * 100) if total_requests > 0 else 0
            
            # íˆíŠ¸ìœ¨ í’ˆì§ˆ í‰ê°€
            hit_quality = "excellent"
            if hit_rate < 50:
                hit_quality = "poor"
            elif hit_rate < 70:
                hit_quality = "fair"
            elif hit_rate < 85:
                hit_quality = "good"
            
            # íŒ¨í„´ë³„ íˆíŠ¸ìœ¨ ë¶„ì„ (ì˜ˆìƒ)
            pattern_analysis = await self._analyze_key_patterns()
            
            return {
                "total_hits": hits,
                "total_misses": misses,
                "total_requests": total_requests,
                "hit_rate_percent": hit_rate,
                "hit_quality": hit_quality,
                "pattern_analysis": pattern_analysis,
                "improvement_potential": 100 - hit_rate if hit_rate < 95 else 0
            }
            
        except Exception as e:
            self.logger.error(f"íˆíŠ¸ìœ¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def _analyze_key_patterns(self) -> Dict[str, Any]:
        """í‚¤ íŒ¨í„´ ë¶„ì„"""
        try:
            # ì£¼ìš” íŒ¨í„´ë³„ í‚¤ ê°œìˆ˜ ì¡°ì‚¬
            patterns = {
                "api_cache:kma:*": "ê¸°ìƒì²­ API ìºì‹œ",
                "api_cache:kto:*": "ê´€ê´‘ê³µì‚¬ API ìºì‹œ", 
                "weather_scores:*": "ë‚ ì”¨ ì ìˆ˜ ìºì‹œ",
                "tourism_data:*": "ê´€ê´‘ ë°ì´í„° ìºì‹œ",
                "recommendations:*": "ì¶”ì²œ ë°ì´í„° ìºì‹œ"
            }
            
            pattern_stats = {}
            
            for pattern, description in patterns.items():
                keys = await self.redis_client.client.keys(pattern)
                
                # ìƒ˜í”Œ í‚¤ë“¤ì˜ TTL í™•ì¸
                ttl_stats = []
                if keys:
                    sample_size = min(10, len(keys))
                    sample_keys = keys[:sample_size]
                    
                    for key in sample_keys:
                        ttl = await self.redis_client.client.ttl(key)
                        if ttl > 0:
                            ttl_stats.append(ttl)
                
                pattern_stats[pattern] = {
                    "description": description,
                    "key_count": len(keys),
                    "avg_ttl": sum(ttl_stats) / len(ttl_stats) if ttl_stats else 0,
                    "has_expiry": len(ttl_stats) > 0
                }
            
            return pattern_stats
            
        except Exception as e:
            self.logger.error(f"í‚¤ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def _analyze_bottlenecks(self) -> Dict[str, Any]:
        """ë³‘ëª© í˜„ìƒ ë¶„ì„"""
        try:
            info = await self.redis_client.client.info()
            
            # CPU ì‚¬ìš©ë¥  (Redis ë‚´ë¶€)
            used_cpu_sys = info.get("used_cpu_sys", 0)
            used_cpu_user = info.get("used_cpu_user", 0)
            
            # ë„¤íŠ¸ì›Œí¬ ì²˜ë¦¬ëŸ‰
            total_net_input_bytes = info.get("total_net_input_bytes", 0)
            total_net_output_bytes = info.get("total_net_output_bytes", 0)
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬ í†µê³„
            total_commands_processed = info.get("total_commands_processed", 0)
            instantaneous_ops_per_sec = info.get("instantaneous_ops_per_sec", 0)
            
            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
            connected_clients = info.get("connected_clients", 0)
            blocked_clients = info.get("blocked_clients", 0)
            
            bottlenecks = []
            
            # ë³‘ëª© ìƒí™© ê°ì§€
            if connected_clients > 100:
                bottlenecks.append("ë†’ì€ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìˆ˜")
            
            if blocked_clients > 0:
                bottlenecks.append("ë¸”ë¡ëœ í´ë¼ì´ì–¸íŠ¸ ì¡´ì¬")
            
            if instantaneous_ops_per_sec > 10000:
                bottlenecks.append("ë†’ì€ ì´ˆë‹¹ ëª…ë ¹ì–´ ì²˜ë¦¬ëŸ‰")
            
            # ë©”ëª¨ë¦¬ ì••ë°•
            used_memory = info.get("used_memory", 0)
            maxmemory = info.get("maxmemory", 0)
            if maxmemory > 0 and (used_memory / maxmemory) > 0.9:
                bottlenecks.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 90% ì´ˆê³¼")
            
            return {
                "cpu_usage": {
                    "system": used_cpu_sys,
                    "user": used_cpu_user
                },
                "network_throughput": {
                    "input_bytes": total_net_input_bytes,
                    "output_bytes": total_net_output_bytes
                },
                "command_processing": {
                    "total_processed": total_commands_processed,
                    "ops_per_sec": instantaneous_ops_per_sec
                },
                "client_connections": {
                    "connected": connected_clients,
                    "blocked": blocked_clients
                },
                "identified_bottlenecks": bottlenecks,
                "performance_score": max(0, 100 - len(bottlenecks) * 20)
            }
            
        except Exception as e:
            self.logger.error(f"ë³‘ëª© í˜„ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def execute_cache_warming(self, config_level: str = "balanced") -> Dict[str, Any]:
        """ìºì‹œ ì›Œë° ì‹¤í–‰"""
        self.logger.info(f"ìºì‹œ ì›Œë°ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ë ˆë²¨: {config_level})")
        
        try:
            config = self.optimization_levels.get(config_level, self.optimization_levels["balanced"])
            
            # ì›Œë° í•¨ìˆ˜ë“¤ ì •ì˜
            warming_functions = {
                "weather_data": self._warm_weather_cache,
                "tourism_data": self._warm_tourism_cache,
                "api_metadata": self._warm_api_metadata_cache
            }
            
            # ìºì‹œ ì›Œë° ì„¤ì • ì—…ë°ì´íŠ¸
            warming_config = CacheWarming(
                enabled=config["warming_enabled"],
                concurrent_workers=config["concurrent_workers"]
            )
            
            self.cache_manager.warming_config = warming_config
            
            # ì›Œë° ì‹¤í–‰
            start_time = time.time()
            await self.cache_manager.warm_cache(warming_functions)
            execution_time = time.time() - start_time
            
            # ì›Œë° í›„ ì„±ëŠ¥ í™•ì¸
            post_warming_metrics = await self.cache_manager.get_cache_metrics()
            
            result = {
                "success": True,
                "config_level": config_level,
                "execution_time_seconds": execution_time,
                "functions_executed": list(warming_functions.keys()),
                "post_warming_metrics": {
                    "cache_size_mb": post_warming_metrics.cache_size_mb,
                    "memory_usage_percent": post_warming_metrics.memory_usage_percent
                }
            }
            
            self.logger.info(f"ìºì‹œ ì›Œë° ì™„ë£Œ: {execution_time:.2f}ì´ˆ ì†Œìš”")
            return result
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì›Œë° ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _warm_weather_cache(self) -> Dict[str, Any]:
        """ë‚ ì”¨ ë°ì´í„° ìºì‹œ ì›Œë°"""
        try:
            # ì£¼ìš” ì§€ì—­ì˜ ë‚ ì”¨ ë°ì´í„° ë¯¸ë¦¬ ìºì‹±
            regions = ["ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°"]
            warming_data = {}
            
            for region in regions:
                # í˜„ì¬ ë‚ ì”¨ ìºì‹œ í‚¤
                current_key = f"weather_cache:current:{region}"
                warming_data[current_key] = {
                    "region": region,
                    "temperature": 20.0,
                    "humidity": 60,
                    "condition": "ë§‘ìŒ",
                    "cached_at": datetime.now().isoformat()
                }
                
                # ì˜ˆë³´ ë°ì´í„° ìºì‹œ í‚¤
                forecast_key = f"weather_cache:forecast:{region}"
                warming_data[forecast_key] = {
                    "region": region,
                    "forecasts": [
                        {"date": "2024-01-01", "temp": 15, "condition": "íë¦¼"},
                        {"date": "2024-01-02", "temp": 18, "condition": "ë§‘ìŒ"}
                    ],
                    "cached_at": datetime.now().isoformat()
                }
            
            return warming_data
            
        except Exception as e:
            self.logger.error(f"ë‚ ì”¨ ìºì‹œ ì›Œë° ì‹¤íŒ¨: {e}")
            return {}
    
    async def _warm_tourism_cache(self) -> Dict[str, Any]:
        """ê´€ê´‘ ë°ì´í„° ìºì‹œ ì›Œë°"""
        try:
            # ì¸ê¸° ê´€ê´‘ì§€ ë°ì´í„° ë¯¸ë¦¬ ìºì‹±
            attractions = ["ê²½ë³µê¶", "ì œì£¼ë„", "ë¶€ì‚°í•´ë³€", "ì„¤ì•…ì‚°", "í•œë¼ì‚°"]
            warming_data = {}
            
            for attraction in attractions:
                cache_key = f"tourism_cache:attraction:{attraction}"
                warming_data[cache_key] = {
                    "name": attraction,
                    "category": "ê´€ê´‘ì§€",
                    "rating": 4.5,
                    "cached_at": datetime.now().isoformat()
                }
            
            return warming_data
            
        except Exception as e:
            self.logger.error(f"ê´€ê´‘ ìºì‹œ ì›Œë° ì‹¤íŒ¨: {e}")
            return {}
    
    async def _warm_api_metadata_cache(self) -> Dict[str, Any]:
        """API ë©”íƒ€ë°ì´í„° ìºì‹œ ì›Œë°"""
        try:
            # API ì„¤ì • ë° ë©”íƒ€ë°ì´í„° ìºì‹±
            warming_data = {
                "api_config:kma": {
                    "endpoint": "weather",
                    "rate_limit": 1000,
                    "cached_at": datetime.now().isoformat()
                },
                "api_config:kto": {
                    "endpoint": "tourism",
                    "rate_limit": 1000,
                    "cached_at": datetime.now().isoformat()
                }
            }
            
            return warming_data
            
        except Exception as e:
            self.logger.error(f"API ë©”íƒ€ë°ì´í„° ìºì‹œ ì›Œë° ì‹¤íŒ¨: {e}")
            return {}
    
    async def optimize_ttl_settings(self, config_level: str = "balanced") -> Dict[str, Any]:
        """TTL ì„¤ì • ìµœì í™”"""
        self.logger.info(f"TTL ì„¤ì • ìµœì í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (ë ˆë²¨: {config_level})")
        
        try:
            config = self.optimization_levels.get(config_level, self.optimization_levels["balanced"])
            
            # í˜„ì¬ í‚¤ íŒ¨í„´ë³„ TTL ë¶„ì„
            pattern_analysis = await self._analyze_key_patterns()
            
            # ìµœì í™”ëœ TTL ì œì•ˆ
            ttl_recommendations = {}
            
            if config["aggressive_ttl"]:
                # ê³µê²©ì  TTL (ë” ê¸´ ìºì‹œ ìœ ì§€)
                ttl_recommendations = {
                    "api_cache:kma:*": 7200,    # 2ì‹œê°„ (ê¸°ì¡´ 30ë¶„ì—ì„œ ì¦ê°€)
                    "api_cache:kto:*": 86400,   # 24ì‹œê°„ (ê¸°ì¡´ 6ì‹œê°„ì—ì„œ ì¦ê°€)
                    "weather_scores:*": 10800,  # 3ì‹œê°„ (ê¸°ì¡´ 1ì‹œê°„ì—ì„œ ì¦ê°€)
                    "tourism_data:*": 21600,    # 6ì‹œê°„ (ê¸°ì¡´ 2ì‹œê°„ì—ì„œ ì¦ê°€)
                    "recommendations:*": 7200   # 2ì‹œê°„ (ê¸°ì¡´ 30ë¶„ì—ì„œ ì¦ê°€)
                }
            else:
                # ë³´ìˆ˜ì  TTL (ì ì ˆí•œ ìºì‹œ ìœ ì§€)
                ttl_recommendations = {
                    "api_cache:kma:*": 3600,    # 1ì‹œê°„
                    "api_cache:kto:*": 14400,   # 4ì‹œê°„
                    "weather_scores:*": 1800,   # 30ë¶„
                    "tourism_data:*": 7200,     # 2ì‹œê°„
                    "recommendations:*": 3600   # 1ì‹œê°„
                }
            
            # TTL ìµœì í™” ì ìš© (ê¸°ì¡´ í‚¤ë“¤ì˜ TTL ì—…ë°ì´íŠ¸)
            optimization_results = []
            
            for pattern, recommended_ttl in ttl_recommendations.items():
                keys = await self.redis_client.client.keys(pattern)
                updated_count = 0
                
                for key in keys[:100]:  # ë°°ì¹˜ í¬ê¸° ì œí•œ
                    try:
                        await self.redis_client.client.expire(key, recommended_ttl)
                        updated_count += 1
                    except Exception as e:
                        self.logger.warning(f"TTL ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ [{key}]: {e}")
                
                optimization_results.append({
                    "pattern": pattern,
                    "recommended_ttl": recommended_ttl,
                    "keys_found": len(keys),
                    "keys_updated": updated_count
                })
            
            return {
                "success": True,
                "config_level": config_level,
                "ttl_recommendations": ttl_recommendations,
                "optimization_results": optimization_results,
                "pattern_analysis": pattern_analysis
            }
            
        except Exception as e:
            self.logger.error(f"TTL ìµœì í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def test_cache_invalidation(self) -> Dict[str, Any]:
        """ìºì‹œ ë¬´íš¨í™” ì „ëµ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ìºì‹œ ë¬´íš¨í™” ì „ëµ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ìºì‹œ ë°ì´í„° ìƒì„±
            test_keys = [
                "test_cache:data:1",
                "test_cache:data:2", 
                "test_cache:dependency:1",
                "test_cache:dependency:2"
            ]
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì •
            for key in test_keys:
                await self.cache_manager.set_cache(key, {"test": True, "created": datetime.now().isoformat()}, 300)
            
            # ë¬´íš¨í™” ì „ ìƒíƒœ í™•ì¸
            before_invalidation = {}
            for key in test_keys:
                before_invalidation[key] = await self.cache_manager.get_cache(key) is not None
            
            # ì˜ì¡´ì„± ê¸°ë°˜ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸
            await self.cache_manager.invalidate_by_dependency("test_cache:data")
            
            # ë¬´íš¨í™” í›„ ìƒíƒœ í™•ì¸
            after_invalidation = {}
            for key in test_keys:
                after_invalidation[key] = await self.cache_manager.get_cache(key) is not None
            
            # ë°°ì¹˜ ì‚­ì œ í…ŒìŠ¤íŠ¸
            await self.cache_manager.batch_delete(["test_cache:*"])
            
            # ìµœì¢… ìƒíƒœ í™•ì¸
            after_batch_delete = {}
            for key in test_keys:
                after_batch_delete[key] = await self.cache_manager.get_cache(key) is not None
            
            return {
                "success": True,
                "test_keys": test_keys,
                "before_invalidation": before_invalidation,
                "after_invalidation": after_invalidation,
                "after_batch_delete": after_batch_delete,
                "invalidation_effective": sum(before_invalidation.values()) > sum(after_invalidation.values()),
                "batch_delete_effective": sum(after_batch_delete.values()) == 0
            }
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def monitor_real_time(self, duration_minutes: int = 60) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
        self.logger.info(f"ì‹¤ì‹œê°„ ìºì‹œ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤ ({duration_minutes}ë¶„)")
        
        try:
            # ëª¨ë‹ˆí„°ë§ ì‹œì‘
            await self.cache_monitor.start_monitoring()
            
            # ì§€ì •ëœ ì‹œê°„ë§Œí¼ ëŒ€ê¸°
            await asyncio.sleep(duration_minutes * 60)
            
            # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            await self.cache_monitor.stop_monitoring()
            
            # ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ìˆ˜ì§‘
            performance_summary = await self.cache_monitor.get_performance_summary(hours=duration_minutes/60)
            optimization_suggestions = await self.cache_monitor.generate_optimization_suggestions()
            alert_history = await self.cache_monitor.get_alert_history(days=1)
            
            return {
                "success": True,
                "duration_minutes": duration_minutes,
                "performance_summary": performance_summary,
                "optimization_suggestions": optimization_suggestions,
                "alert_history": alert_history,
                "monitoring_completed": True
            }
            
        except Exception as e:
            self.logger.error(f"ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
            await self.cache_monitor.stop_monitoring()
            return {"success": False, "error": str(e)}
    
    async def _save_analysis_report(self, analysis_result: Dict[str, Any]):
        """ë¶„ì„ ë³´ê³ ì„œ ì €ì¥"""
        try:
            # ë³´ê³ ì„œ íŒŒì¼ëª…
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = project_root / "logs" / f"cache_analysis_report_{timestamp}.json"
            report_file.parent.mkdir(exist_ok=True)
            
            # JSON í˜•íƒœë¡œ ì €ì¥
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, indent=2, ensure_ascii=False, default=str)
            
            # í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œë„ ìƒì„±
            summary_file = project_root / "logs" / f"cache_analysis_summary_{timestamp}.txt"
            await self._generate_text_summary(analysis_result, summary_file)
            
            self.logger.info(f"ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {report_file}")
            self.logger.info(f"ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: {summary_file}")
            
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _generate_text_summary(self, analysis_result: Dict[str, Any], output_file: Path):
        """í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        try:
            lines = []
            lines.append("="*80)
            lines.append("Redis ìºì‹œ ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ")
            lines.append(f"ìƒì„±ì¼ì‹œ: {analysis_result.get('timestamp', 'N/A')}")
            lines.append("="*80)
            
            # Redis ê¸°ë³¸ ì •ë³´
            redis_info = analysis_result.get('redis_info', {})
            if redis_info and 'error' not in redis_info:
                lines.append("\nğŸ“Š Redis ê¸°ë³¸ ì •ë³´:")
                lines.append(f"  ë²„ì „: {redis_info.get('version', 'N/A')}")
                lines.append(f"  ê°€ë™ ì‹œê°„: {redis_info.get('uptime_days', 0):.1f}ì¼")
                lines.append(f"  ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸: {redis_info.get('connected_clients', 0)}ê°œ")
                lines.append(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {redis_info.get('used_memory_mb', 0):.1f}MB")
                lines.append(f"  ìµœëŒ€ ë©”ëª¨ë¦¬: {redis_info.get('maxmemory_mb', 0):.1f}MB")
                lines.append(f"  ì´ˆë‹¹ ëª…ë ¹ì–´ ì²˜ë¦¬: {redis_info.get('instantaneous_ops_per_sec', 0)}ê°œ")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            perf_metrics = analysis_result.get('performance_metrics', {})
            if perf_metrics and 'error' not in perf_metrics:
                cache_metrics = perf_metrics.get('cache_metrics', {})
                lines.append("\nğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
                lines.append(f"  ìºì‹œ íˆíŠ¸ìœ¨: {cache_metrics.get('hit_rate', 0):.1%}")
                lines.append(f"  ì´ ìš”ì²­ ìˆ˜: {cache_metrics.get('total_requests', 0)}ê±´")
                lines.append(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {cache_metrics.get('avg_response_time_ms', 0):.1f}ms")
                lines.append(f"  ìºì‹œ í¬ê¸°: {cache_metrics.get('cache_size_mb', 0):.1f}MB")
            
            # ë©”ëª¨ë¦¬ ë¶„ì„
            memory_analysis = analysis_result.get('memory_analysis', {})
            if memory_analysis and 'error' not in memory_analysis:
                lines.append("\nğŸ’¾ ë©”ëª¨ë¦¬ ë¶„ì„:")
                lines.append(f"  ì‚¬ìš©ë¥ : {memory_analysis.get('usage_percent', 0):.1f}%")
                lines.append(f"  íš¨ìœ¨ì„±: {memory_analysis.get('memory_efficiency', 'N/A')}")
                lines.append(f"  ë‹¨í¸í™” ë¹„ìœ¨: {memory_analysis.get('fragmentation_ratio', 0):.2f}")
            
            # íˆíŠ¸ìœ¨ ë¶„ì„
            hit_rate_analysis = analysis_result.get('hit_rate_analysis', {})
            if hit_rate_analysis and 'error' not in hit_rate_analysis:
                lines.append("\nğŸ¯ íˆíŠ¸ìœ¨ ë¶„ì„:")
                lines.append(f"  ì „ì²´ íˆíŠ¸ìœ¨: {hit_rate_analysis.get('hit_rate_percent', 0):.1f}%")
                lines.append(f"  í’ˆì§ˆ í‰ê°€: {hit_rate_analysis.get('hit_quality', 'N/A')}")
                lines.append(f"  ê°œì„  ì—¬ì§€: {hit_rate_analysis.get('improvement_potential', 0):.1f}%")
            
            # ìµœì í™” ì œì•ˆ
            suggestions = analysis_result.get('optimization_suggestions', [])
            if suggestions:
                lines.append("\nğŸ’¡ ìµœì í™” ì œì•ˆ:")
                for i, suggestion in enumerate(suggestions[:5], 1):
                    lines.append(f"  {i}. [{suggestion.get('priority', 'medium').upper()}] {suggestion.get('title', 'N/A')}")
                    lines.append(f"     {suggestion.get('description', 'N/A')}")
            
            # ê±´ê°• ìƒíƒœ
            health_status = analysis_result.get('health_status', {})
            if health_status and 'error' not in health_status:
                lines.append("\nğŸ¥ ê±´ê°• ìƒíƒœ:")
                lines.append(f"  ìƒíƒœ: {health_status.get('status', 'unknown')}")
                lines.append(f"  íˆíŠ¸ìœ¨: {health_status.get('hit_rate', 0):.1%}")
                lines.append(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {health_status.get('memory_usage_percent', 0):.1f}%")
            
            lines.append("\n" + "="*80)
            
            # íŒŒì¼ì— ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines))
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ìºì‹œ ì„±ëŠ¥ ìµœì í™” ë„êµ¬')
    parser.add_argument('--action', choices=['analyze', 'warm', 'optimize', 'test', 'monitor'], 
                       default='analyze', help='ì‹¤í–‰í•  ì‘ì—…')
    parser.add_argument('--config-level', choices=['conservative', 'balanced', 'aggressive'],
                       default='balanced', help='ìµœì í™” ë ˆë²¨')
    parser.add_argument('--duration', type=int, default=60,
                       help='ëª¨ë‹ˆí„°ë§ ì§€ì† ì‹œê°„ (ë¶„)')
    parser.add_argument('--report', type=bool, default=True,
                       help='ë³´ê³ ì„œ ì €ì¥ ì—¬ë¶€')
    
    args = parser.parse_args()
    
    optimizer = CachePerformanceOptimizer()
    
    try:
        if args.action == 'analyze':
            print("ğŸ“Š ìºì‹œ ì„±ëŠ¥ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = await optimizer.analyze_cache_performance(save_report=args.report)
            
            if 'error' not in result:
                print("âœ… ìºì‹œ ì„±ëŠ¥ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ì£¼ìš” ì§€í‘œ ì¶œë ¥
                health = result.get('health_status', {})
                if health:
                    print(f"   ìƒíƒœ: {health.get('status', 'unknown')}")
                    print(f"   íˆíŠ¸ìœ¨: {health.get('hit_rate', 0):.1%}")
                    print(f"   ë©”ëª¨ë¦¬: {health.get('memory_usage_percent', 0):.1f}%")
                
                suggestions = result.get('optimization_suggestions', [])
                if suggestions:
                    print(f"   ìµœì í™” ì œì•ˆ: {len(suggestions)}ê±´")
            else:
                print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result['error']}")
        
        elif args.action == 'warm':
            print(f"ğŸ”¥ ìºì‹œ ì›Œë°ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ë ˆë²¨: {args.config_level})...")
            result = await optimizer.execute_cache_warming(args.config_level)
            
            if result.get('success'):
                print(f"âœ… ìºì‹œ ì›Œë° ì™„ë£Œ: {result.get('execution_time_seconds', 0):.2f}ì´ˆ")
            else:
                print(f"âŒ ì›Œë° ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
        
        elif args.action == 'optimize':
            print(f"âš¡ TTL ìµœì í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (ë ˆë²¨: {args.config_level})...")
            result = await optimizer.optimize_ttl_settings(args.config_level)
            
            if result.get('success'):
                optimizations = result.get('optimization_results', [])
                total_updated = sum(opt.get('keys_updated', 0) for opt in optimizations)
                print(f"âœ… TTL ìµœì í™” ì™„ë£Œ: {total_updated}ê°œ í‚¤ ì—…ë°ì´íŠ¸")
            else:
                print(f"âŒ ìµœì í™” ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
        
        elif args.action == 'test':
            print("ğŸ§ª ìºì‹œ ë¬´íš¨í™” ì „ëµ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = await optimizer.test_cache_invalidation()
            
            if result.get('success'):
                print("âœ… ë¬´íš¨í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                print(f"   ì˜ì¡´ì„± ë¬´íš¨í™”: {'âœ…' if result.get('invalidation_effective') else 'âŒ'}")
                print(f"   ë°°ì¹˜ ì‚­ì œ: {'âœ…' if result.get('batch_delete_effective') else 'âŒ'}")
            else:
                print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
        
        elif args.action == 'monitor':
            print(f"ğŸ“¡ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤ ({args.duration}ë¶„)...")
            result = await optimizer.monitor_real_time(args.duration)
            
            if result.get('success'):
                print("âœ… ëª¨ë‹ˆí„°ë§ ì™„ë£Œ")
                perf = result.get('performance_summary', {})
                if perf:
                    hit_rate = perf.get('hit_rate', {})
                    print(f"   í‰ê·  íˆíŠ¸ìœ¨: {hit_rate.get('average', 0):.1%}")
                    print(f"   ì´ ìš”ì²­: {perf.get('total_requests', 0)}ê±´")
            else:
                print(f"âŒ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)