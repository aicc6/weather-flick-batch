#!/usr/bin/env python3
"""
API ì €ì¥ ì •ì±… ë° ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸

1. API ì›ë³¸ ë°ì´í„° ì €ì¥ í…ŒìŠ¤íŠ¸
2. ë°ì´í„° ë§Œë£Œ ì •ì±… í…ŒìŠ¤íŠ¸
3. ìë™ ì•„ì¹´ì´ë¹™ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
4. ì €ì¥ ê³µê°„ ìµœì í™” í…ŒìŠ¤íŠ¸
5. ë°±ì—… ë° ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
6. ë°ì´í„° ì••ì¶• ë° ë³´ê´€ ì •ì±… í…ŒìŠ¤íŠ¸
"""

import sys
import os
import asyncio
import logging
import tempfile
import shutil
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.api_storage_policy_engine import APIStoragePolicyEngine, get_policy_engine
from app.core.database_manager import SyncDatabaseManager
from app.archiving.backup_manager import BackupManager, BackupConfiguration
from app.archiving.archival_engine import ArchivalEngine
from app.archiving.archival_policies import get_archival_policy_manager
from config.api_storage_policy import ProviderConfig, EndpointConfig, StoragePolicy

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ComprehensiveAPIStorageTest:
    """API ì €ì¥ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™”"""
        self.start_time = datetime.now()
        self.test_results = {}
        self.temp_backup_dir = None
        self.db_manager = SyncDatabaseManager()
        self.policy_engine = get_policy_engine()
        
        # í…ŒìŠ¤íŠ¸ í†µê³„
        self.stats = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "errors": []
        }
    
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ API ì €ì¥ ì •ì±… ë° ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        try:
            # 1. API ì›ë³¸ ë°ì´í„° ì €ì¥ í…ŒìŠ¤íŠ¸
            self.test_api_storage_policy()
            
            # 2. ë°ì´í„° ë§Œë£Œ ì •ì±… í…ŒìŠ¤íŠ¸
            self.test_ttl_policy()
            
            # 3. ìë™ ì•„ì¹´ì´ë¹™ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            asyncio.run(self.test_archival_system())
            
            # 4. ì €ì¥ ê³µê°„ ìµœì í™” í…ŒìŠ¤íŠ¸
            self.test_storage_optimization()
            
            # 5. ë°±ì—… ë° ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
            asyncio.run(self.test_backup_recovery())
            
            # 6. ë°ì´í„° ì••ì¶• ë° ë³´ê´€ ì •ì±… í…ŒìŠ¤íŠ¸
            self.test_compression_policy()
            
            # ê²°ê³¼ ì¶œë ¥
            self.generate_final_report()
            
        except Exception as e:
            logger.error(f"ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            self.stats["errors"].append(str(e))
        
        finally:
            self.cleanup()
    
    def test_api_storage_policy(self):
        """API ì €ì¥ ì •ì±… í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“‹ 1. API ì›ë³¸ ë°ì´í„° ì €ì¥ ì •ì±… í…ŒìŠ¤íŠ¸")
        
        test_cases = [
            # KMA í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            {
                "name": "KMA ì˜ˆë³´êµ¬ì—­ API - ì •ìƒ ì‘ë‹µ",
                "provider": "KMA",
                "endpoint": "fct_shrt_reg",
                "response_size": 500_000,  # 500KB
                "status_code": 200,
                "expected": True
            },
            {
                "name": "KMA ì˜ˆë³´êµ¬ì—­ API - í¬ê¸° ì´ˆê³¼",
                "provider": "KMA",
                "endpoint": "fct_shrt_reg",
                "response_size": 2_000_000,  # 2MB (ì œí•œ: 1MB)
                "status_code": 200,
                "expected": False
            },
            {
                "name": "KMA í—¬ìŠ¤ì²´í¬ - ì €ì¥ ë¹„í™œì„±í™”",
                "provider": "KMA",
                "endpoint": "health",
                "response_size": 100,
                "status_code": 200,
                "expected": False
            },
            {
                "name": "KMA ì˜¤ë¥˜ ì‘ë‹µ - ì €ì¥ í™œì„±í™”",
                "provider": "KMA",
                "endpoint": "fct_shrt_reg",
                "response_size": 1000,
                "status_code": 500,
                "expected": True
            },
            
            # KTO í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            {
                "name": "KTO ì§€ì—­ì½”ë“œ API - ì •ìƒ ì‘ë‹µ",
                "provider": "KTO",
                "endpoint": "areaCode2",
                "response_size": 500_000,
                "status_code": 200,
                "expected": True
            },
            {
                "name": "KTO ì´ë¯¸ì§€ API - ëŒ€ìš©ëŸ‰",
                "provider": "KTO",
                "endpoint": "detailImage2",
                "response_size": 25_000_000,  # 25MB
                "status_code": 200,
                "expected": True  # KTOëŠ” 30MBê¹Œì§€ í—ˆìš©
            },
            {
                "name": "KTO ì´ë¯¸ì§€ API - í¬ê¸° ì´ˆê³¼",
                "provider": "KTO",
                "endpoint": "detailImage2",
                "response_size": 35_000_000,  # 35MB (ì œí•œ: 30MB)
                "status_code": 200,
                "expected": False
            },
            
            # ë¯¸ì§€ì› ì œê³µì
            {
                "name": "ì•Œ ìˆ˜ ì—†ëŠ” ì œê³µì",
                "provider": "UNKNOWN",
                "endpoint": "test",
                "response_size": 1000,
                "status_code": 200,
                "expected": False
            },
            
            # ëª¨ë‹ˆí„°ë§ ì œê³µì (ë¹„í™œì„±í™”)
            {
                "name": "ëª¨ë‹ˆí„°ë§ API - ë¹„í™œì„±í™”",
                "provider": "MONITORING",
                "endpoint": "health",
                "response_size": 100,
                "status_code": 200,
                "expected": False
            }
        ]
        
        passed_cases = 0
        total_cases = len(test_cases)
        
        for test_case in test_cases:
            self.stats["total_tests"] += 1
            
            try:
                should_store, reason = self.policy_engine.should_store(
                    provider=test_case["provider"],
                    endpoint=test_case["endpoint"],
                    response_size_bytes=test_case["response_size"],
                    status_code=test_case["status_code"]
                )
                
                if should_store == test_case["expected"]:
                    logger.info(f"âœ… {test_case['name']}: PASS - {reason}")
                    passed_cases += 1
                    self.stats["passed_tests"] += 1
                else:
                    logger.error(f"âŒ {test_case['name']}: FAIL - ì˜ˆìƒ: {test_case['expected']}, ì‹¤ì œ: {should_store}")
                    self.stats["failed_tests"] += 1
                    self.stats["errors"].append(f"ì €ì¥ ì •ì±… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_case['name']}")
                    
            except Exception as e:
                logger.error(f"âŒ {test_case['name']}: ERROR - {e}")
                self.stats["failed_tests"] += 1
                self.stats["errors"].append(f"ì €ì¥ ì •ì±… í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {test_case['name']} - {e}")
        
        # ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸
        self.stats["total_tests"] += 1
        try:
            metadata = self.policy_engine.get_storage_metadata("KMA", "fct_shrt_reg")
            if all(key in metadata for key in ["ttl_days", "priority", "expires_at", "compression_enabled"]):
                logger.info("âœ… ì €ì¥ ë©”íƒ€ë°ì´í„° ìƒì„±: PASS")
                passed_cases += 1
                self.stats["passed_tests"] += 1
            else:
                logger.error("âŒ ì €ì¥ ë©”íƒ€ë°ì´í„° ìƒì„±: FAIL - í•„ìˆ˜ í‚¤ ëˆ„ë½")
                self.stats["failed_tests"] += 1
        except Exception as e:
            logger.error(f"âŒ ì €ì¥ ë©”íƒ€ë°ì´í„° ìƒì„±: ERROR - {e}")
            self.stats["failed_tests"] += 1
        
        # í†µê³„ í™•ì¸
        stats = self.policy_engine.get_statistics()
        logger.info(f"ğŸ“Š ì •ì±… ì—”ì§„ í†µê³„: {stats}")
        
        self.test_results["api_storage_policy"] = {
            "passed": passed_cases,
            "total": total_cases + 1,
            "success_rate": round(passed_cases / (total_cases + 1) * 100, 2)
        }
        
        logger.info(f"ğŸ“‹ API ì €ì¥ ì •ì±… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {passed_cases}/{total_cases + 1} í†µê³¼")
    
    def test_ttl_policy(self):
        """TTL ì •ì±… í…ŒìŠ¤íŠ¸"""
        logger.info("â° 2. ë°ì´í„° ë§Œë£Œ ì •ì±… í…ŒìŠ¤íŠ¸")
        
        # ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë§Œë£Œ ëŒ€ìƒ ë°ì´í„° í™•ì¸
        try:
            # ë§Œë£Œëœ ë°ì´í„° ì¡°íšŒ (ë‹¨ìˆœ ì¿¼ë¦¬)
            expired_query = """
                SELECT COUNT(*) as count, 
                       COUNT(CASE WHEN expires_at < NOW() THEN 1 END) as expired_count,
                       COUNT(CASE WHEN created_at < NOW() - INTERVAL '90 days' THEN 1 END) as old_count
                FROM api_raw_data
            """
            
            result = self.db_manager.fetch_one(expired_query)
            
            logger.info(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ë§Œë£Œ ë¶„ì„:")
            logger.info(f"  - ì „ì²´ ë ˆì½”ë“œ: {result['count']:,}ê°œ")
            logger.info(f"  - ë§Œë£Œëœ ë ˆì½”ë“œ: {result['expired_count']:,}ê°œ")
            logger.info(f"  - 90ì¼ ì´ìƒ ëœ ë ˆì½”ë“œ: {result['old_count']:,}ê°œ")
            
            # ì œê³µìë³„ ë¶„ì„
            provider_query = """
                SELECT api_provider, 
                       COUNT(*) as total,
                       COUNT(CASE WHEN expires_at < NOW() THEN 1 END) as expired,
                       AVG(response_size) as avg_size_bytes
                FROM api_raw_data
                GROUP BY api_provider
                ORDER BY total DESC
            """
            
            providers = self.db_manager.fetch_all(provider_query)
            
            logger.info("ğŸ“Š ì œê³µìë³„ ë°ì´í„° ë¶„ì„:")
            for provider in providers:
                avg_size_mb = (provider['avg_size_bytes'] or 0) / (1024 * 1024)
                logger.info(f"  - {provider['api_provider']}: {provider['total']:,}ê°œ (ë§Œë£Œ: {provider['expired']:,}ê°œ, í‰ê· í¬ê¸°: {avg_size_mb:.2f}MB)")
            
            # í¬ê¸°ë³„ ë¶„ì„
            size_query = """
                SELECT 
                    COUNT(CASE WHEN response_size > 10*1024*1024 THEN 1 END) as large_files,
                    COUNT(CASE WHEN response_size > 1*1024*1024 THEN 1 END) as medium_files,
                    COUNT(CASE WHEN response_size <= 1*1024*1024 THEN 1 END) as small_files,
                    SUM(response_size)::bigint as total_size_bytes
                FROM api_raw_data
            """
            
            size_result = self.db_manager.fetch_one(size_query)
            total_size_mb = (size_result['total_size_bytes'] or 0) / (1024 * 1024)
            
            logger.info(f"ğŸ“Š í¬ê¸°ë³„ ë°ì´í„° ë¶„ì„:")
            logger.info(f"  - ëŒ€ìš©ëŸ‰ íŒŒì¼ (>10MB): {size_result['large_files']:,}ê°œ")
            logger.info(f"  - ì¤‘ê°„ íŒŒì¼ (1-10MB): {size_result['medium_files']:,}ê°œ")
            logger.info(f"  - ì†Œìš©ëŸ‰ íŒŒì¼ (<1MB): {size_result['small_files']:,}ê°œ")
            logger.info(f"  - ì „ì²´ ë°ì´í„° í¬ê¸°: {total_size_mb:.2f}MB")
            
            self.stats["total_tests"] += 1
            self.stats["passed_tests"] += 1
            
            self.test_results["ttl_policy"] = {
                "total_records": result['count'],
                "expired_records": result['expired_count'],
                "old_records": result['old_count'],
                "total_size_mb": round(total_size_mb, 2),
                "providers": len(providers)
            }
            
            logger.info("âœ… TTL ì •ì±… ë¶„ì„ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ TTL ì •ì±… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.stats["total_tests"] += 1
            self.stats["failed_tests"] += 1
            self.stats["errors"].append(f"TTL ì •ì±… í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
    
    async def test_archival_system(self):
        """ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ—„ï¸ 3. ìë™ ì•„ì¹´ì´ë¹™ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
        
        # ì„ì‹œ ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.temp_backup_dir = tempfile.mkdtemp(prefix="archival_test_")
        logger.info(f"ì„ì‹œ ë°±ì—… ë””ë ‰í† ë¦¬: {self.temp_backup_dir}")
        
        try:
            # ë°±ì—… ì„¤ì •
            config = BackupConfiguration(
                base_backup_path=self.temp_backup_dir,
                max_concurrent_backups=3,
                verify_integrity=True
            )
            
            backup_manager = BackupManager(config)
            archival_engine = ArchivalEngine(backup_manager)
            
            # ê°€ìƒ ì•„ì¹´ì´ë¹™ ë°ì´í„° ìƒì„± ë° í…ŒìŠ¤íŠ¸
            test_data = {
                "api_response": {
                    "resultCode": "0000",
                    "resultMsg": "OK",
                    "items": [
                        {"contentid": "123", "title": "í…ŒìŠ¤íŠ¸ ê´€ê´‘ì§€ 1"},
                        {"contentid": "124", "title": "í…ŒìŠ¤íŠ¸ ê´€ê´‘ì§€ 2"}
                    ]
                },
                "metadata": {
                    "query_time": datetime.now().isoformat(),
                    "total_count": 2
                }
            }
            
            # ì•„ì¹´ì´ë¹™ ì •ì±… ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸
            policy_manager = get_archival_policy_manager()
            policies = policy_manager.get_all_policies()
            
            logger.info(f"ğŸ“‹ ì•„ì¹´ì´ë¹™ ì •ì±…: {len(policies)}ê°œ ë¡œë“œë¨")
            
            # ë°±ì—… ìƒì„± í…ŒìŠ¤íŠ¸
            from app.archiving.archival_policies import ArchivalRule, ArchivalTrigger, CompressionType, StorageLocation
            
            test_rule = ArchivalRule(
                rule_id="test_rule",
                name="í…ŒìŠ¤íŠ¸ ë°±ì—… ê·œì¹™",
                description="í…ŒìŠ¤íŠ¸ìš© ë°±ì—… ê·œì¹™",
                trigger=ArchivalTrigger.MANUAL,
                condition={},
                target_location=StorageLocation.LOCAL_DISK,
                compression=CompressionType.GZIP,
                retention_days=30
            )
            
            backup_record = await backup_manager.backup_data(
                data_id="test_123",
                api_provider="TEST",
                endpoint="test_endpoint",
                data=test_data,
                rule=test_rule
            )
            
            if backup_record and backup_record.status.value == "completed":
                logger.info(f"âœ… ë°±ì—… ìƒì„± ì„±ê³µ: {backup_record.backup_id}")
                logger.info(f"   ì••ì¶•ë¥ : {backup_record.compression_ratio:.1f}%")
                
                # ë³µì› í…ŒìŠ¤íŠ¸
                restored_data = await backup_manager.restore_data(backup_record.backup_id)
                
                if restored_data == test_data:
                    logger.info("âœ… ë°±ì—… ë³µì› ì„±ê³µ: ë°ì´í„° ì¼ì¹˜ í™•ì¸")
                    self.stats["passed_tests"] += 2
                else:
                    logger.error("âŒ ë°±ì—… ë³µì› ì‹¤íŒ¨: ë°ì´í„° ë¶ˆì¼ì¹˜")
                    self.stats["failed_tests"] += 1
                    
                self.stats["total_tests"] += 2
                
            else:
                logger.error("âŒ ë°±ì—… ìƒì„± ì‹¤íŒ¨")
                self.stats["total_tests"] += 1
                self.stats["failed_tests"] += 1
            
            # ì•„ì¹´ì´ë¹™ ì—”ì§„ í†µê³„
            stats = archival_engine.get_archival_statistics()
            logger.info(f"ğŸ“Š ì•„ì¹´ì´ë¹™ ì—”ì§„ í†µê³„: {stats['engine_statistics']}")
            
            self.test_results["archival_system"] = {
                "backup_created": backup_record is not None,
                "compression_ratio": backup_record.compression_ratio if backup_record else 0,
                "policies_count": len(policies)
            }
            
            logger.info("âœ… ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.stats["total_tests"] += 1
            self.stats["failed_tests"] += 1
            self.stats["errors"].append(f"ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
    
    def test_storage_optimization(self):
        """ì €ì¥ ê³µê°„ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ’¾ 4. ì €ì¥ ê³µê°„ ìµœì í™” í…ŒìŠ¤íŠ¸")
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
            disk_usage_query = """
                SELECT 
                    schemaname,
                    tablename,
                    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                    pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
                FROM pg_tables 
                WHERE schemaname = 'public'
                ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
                LIMIT 10
            """
            
            tables = self.db_manager.fetch_all(disk_usage_query)
            
            logger.info("ğŸ“Š í…Œì´ë¸”ë³„ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 10ê°œ):")
            total_size_bytes = 0
            for table in tables:
                size_mb = table['size_bytes'] / (1024 * 1024)
                total_size_bytes += table['size_bytes']
                logger.info(f"  - {table['tablename']}: {table['size']} ({size_mb:.1f}MB)")
            
            total_size_gb = total_size_bytes / (1024 * 1024 * 1024)
            logger.info(f"ğŸ“Š ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°: {total_size_gb:.2f}GB")
            
            # API ì›ë³¸ ë°ì´í„° ìµœì í™” ê¸°íšŒ ë¶„ì„
            optimization_query = """
                SELECT 
                    api_provider,
                    endpoint,
                    COUNT(*) as record_count,
                    SUM(response_size) as total_size_bytes,
                    AVG(response_size) as avg_size_bytes,
                    COUNT(CASE WHEN expires_at < NOW() THEN 1 END) as expired_count,
                    COUNT(CASE WHEN response_size > 5*1024*1024 THEN 1 END) as large_files_count
                FROM api_raw_data
                GROUP BY api_provider, endpoint
                HAVING COUNT(*) > 10
                ORDER BY SUM(response_size) DESC
                LIMIT 20
            """
            
            optimizations = self.db_manager.fetch_all(optimization_query)
            
            logger.info("ğŸ“Š API ë°ì´í„° ìµœì í™” ë¶„ì„ (ìƒìœ„ 20ê°œ):")
            total_optimizable_mb = 0
            
            for opt in optimizations:
                size_mb = (opt['total_size_bytes'] or 0) / (1024 * 1024)
                avg_size_kb = (opt['avg_size_bytes'] or 0) / 1024
                total_optimizable_mb += size_mb
                
                # ìµœì í™” ê¸°íšŒ í‰ê°€
                optimization_potential = ""
                if opt['expired_count'] > 0:
                    optimization_potential += f"ë§Œë£Œ:{opt['expired_count']}ê°œ "
                if opt['large_files_count'] > 0:
                    optimization_potential += f"ëŒ€ìš©ëŸ‰:{opt['large_files_count']}ê°œ "
                
                logger.info(f"  - {opt['api_provider']}/{opt['endpoint']}: {size_mb:.1f}MB, {opt['record_count']}ê°œ (í‰ê· : {avg_size_kb:.1f}KB) {optimization_potential}")
            
            # ì••ì¶• íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸
            compression_test_data = json.dumps({
                "test_data": ["sample"] * 1000,
                "metadata": {"created": datetime.now().isoformat()}
            }, ensure_ascii=False)
            
            original_size = len(compression_test_data.encode('utf-8'))
            
            import gzip
            compressed_data = gzip.compress(compression_test_data.encode('utf-8'))
            compressed_size = len(compressed_data)
            compression_ratio = (1 - compressed_size / original_size) * 100
            
            logger.info(f"ğŸ“Š ì••ì¶• íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸:")
            logger.info(f"  - ì›ë³¸ í¬ê¸°: {original_size:,} bytes")
            logger.info(f"  - ì••ì¶• í¬ê¸°: {compressed_size:,} bytes")
            logger.info(f"  - ì••ì¶•ë¥ : {compression_ratio:.1f}%")
            
            self.test_results["storage_optimization"] = {
                "total_db_size_gb": round(total_size_gb, 2),
                "optimizable_data_mb": round(total_optimizable_mb, 2),
                "compression_ratio": round(compression_ratio, 1),
                "optimization_candidates": len(optimizations)
            }
            
            self.stats["total_tests"] += 1
            self.stats["passed_tests"] += 1
            
            logger.info("âœ… ì €ì¥ ê³µê°„ ìµœì í™” ë¶„ì„ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì €ì¥ ê³µê°„ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.stats["total_tests"] += 1
            self.stats["failed_tests"] += 1
            self.stats["errors"].append(f"ì €ì¥ ê³µê°„ ìµœì í™” ì˜¤ë¥˜: {e}")
    
    async def test_backup_recovery(self):
        """ë°±ì—… ë° ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ’¿ 5. ë°±ì—… ë° ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        
        if not self.temp_backup_dir:
            self.temp_backup_dir = tempfile.mkdtemp(prefix="backup_test_")
        
        try:
            config = BackupConfiguration(
                base_backup_path=self.temp_backup_dir,
                max_concurrent_backups=2,
                verify_integrity=True
            )
            
            backup_manager = BackupManager(config)
            
            # ë‹¤ì–‘í•œ í¬ê¸°ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            test_datasets = [
                {
                    "name": "small_dataset",
                    "data": {"items": [f"item_{i}" for i in range(10)]},
                    "expected_compression": 50  # ì˜ˆìƒ ì••ì¶•ë¥  (%)
                },
                {
                    "name": "medium_dataset", 
                    "data": {"items": [{"id": i, "description": "í…ŒìŠ¤íŠ¸ ë°ì´í„° " * 10} for i in range(100)]},
                    "expected_compression": 80
                },
                {
                    "name": "large_dataset",
                    "data": {"items": [{"id": i, "content": "ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° " * 100} for i in range(1000)]},
                    "expected_compression": 90
                }
            ]
            
            backup_results = []
            
            for dataset in test_datasets:
                # ë°±ì—… ê·œì¹™ ìƒì„±
                from app.archiving.archival_policies import ArchivalRule, ArchivalTrigger, CompressionType, StorageLocation
                
                rule = ArchivalRule(
                    rule_id=f"test_rule_{dataset['name']}",
                    name=f"í…ŒìŠ¤íŠ¸ ê·œì¹™ - {dataset['name']}",
                    description=f"{dataset['name']} ë°±ì—… í…ŒìŠ¤íŠ¸",
                    trigger=ArchivalTrigger.MANUAL,
                    condition={},
                    target_location=StorageLocation.LOCAL_DISK,
                    compression=CompressionType.GZIP,
                    retention_days=7
                )
                
                # ë°±ì—… ì‹¤í–‰
                start_time = time.time()
                backup_record = await backup_manager.backup_data(
                    data_id=f"test_{dataset['name']}",
                    api_provider="TEST",
                    endpoint="backup_test",
                    data=dataset["data"],
                    rule=rule
                )
                backup_time = time.time() - start_time
                
                if backup_record:
                    # ë³µì› í…ŒìŠ¤íŠ¸
                    start_time = time.time()
                    restored_data = await backup_manager.restore_data(backup_record.backup_id)
                    restore_time = time.time() - start_time
                    
                    # ê²°ê³¼ ê²€ì¦
                    data_match = restored_data == dataset["data"]
                    compression_achieved = backup_record.compression_ratio
                    
                    result = {
                        "dataset": dataset['name'],
                        "backup_success": True,
                        "restore_success": data_match,
                        "backup_time": round(backup_time, 3),
                        "restore_time": round(restore_time, 3),
                        "compression_ratio": round(compression_achieved, 1),
                        "original_size": backup_record.original_size_bytes,
                        "compressed_size": backup_record.compressed_size_bytes
                    }
                    
                    backup_results.append(result)
                    
                    if data_match:
                        logger.info(f"âœ… {dataset['name']}: ë°±ì—…/ë³µì› ì„±ê³µ (ì••ì¶•ë¥ : {compression_achieved:.1f}%, ë°±ì—…: {backup_time:.3f}s, ë³µì›: {restore_time:.3f}s)")
                        self.stats["passed_tests"] += 1
                    else:
                        logger.error(f"âŒ {dataset['name']}: ë³µì› ë°ì´í„° ë¶ˆì¼ì¹˜")
                        self.stats["failed_tests"] += 1
                        
                else:
                    logger.error(f"âŒ {dataset['name']}: ë°±ì—… ì‹¤íŒ¨")
                    self.stats["failed_tests"] += 1
                    
                self.stats["total_tests"] += 1
            
            # ë°±ì—… ë§¤ë‹ˆì € í†µê³„
            backup_stats = backup_manager.get_backup_statistics()
            logger.info(f"ğŸ“Š ë°±ì—… ë§¤ë‹ˆì € í†µê³„: {backup_stats}")
            
            self.test_results["backup_recovery"] = {
                "test_datasets": len(test_datasets),
                "successful_backups": len([r for r in backup_results if r["backup_success"]]),
                "successful_restores": len([r for r in backup_results if r["restore_success"]]),
                "average_compression": round(sum(r["compression_ratio"] for r in backup_results) / len(backup_results), 1) if backup_results else 0,
                "backup_results": backup_results
            }
            
            logger.info("âœ… ë°±ì—… ë° ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë°±ì—… ë° ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.stats["total_tests"] += 1
            self.stats["failed_tests"] += 1
            self.stats["errors"].append(f"ë°±ì—… ë³µêµ¬ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
    
    def test_compression_policy(self):
        """ë°ì´í„° ì••ì¶• ë° ë³´ê´€ ì •ì±… í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ—œï¸ 6. ë°ì´í„° ì••ì¶• ë° ë³´ê´€ ì •ì±… í…ŒìŠ¤íŠ¸")
        
        try:
            from app.archiving.backup_manager import CompressionHandler
            from app.archiving.archival_policies import CompressionType
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ë‹¤ì–‘í•œ íŒ¨í„´)
            test_data_sets = {
                "json_repetitive": {
                    "data": json.dumps({"items": [{"id": i, "name": f"item_{i}", "description": "ë°˜ë³µì ì¸ ì„¤ëª… ë°ì´í„°"} for i in range(500)]}),
                    "type": "ë°˜ë³µì  JSON"
                },
                "json_random": {
                    "data": json.dumps({"data": [f"random_string_{i}_{hash(str(i))}" for i in range(100)]}),
                    "type": "ëœë¤ JSON"
                },
                "text_repetitive": {
                    "data": "ì´ê²ƒì€ ë°˜ë³µì ì¸ í…ìŠ¤íŠ¸ ë°ì´í„°ì…ë‹ˆë‹¤. " * 1000,
                    "type": "ë°˜ë³µì  í…ìŠ¤íŠ¸"
                },
                "binary_like": {
                    "data": "".join([chr(i % 256) for i in range(10000)]),
                    "type": "ë°”ì´ë„ˆë¦¬ í˜•íƒœ"
                }
            }
            
            compression_handler = CompressionHandler()
            compression_results = []
            
            # ê° ì••ì¶• ë°©ì‹ë³„ í…ŒìŠ¤íŠ¸
            compression_types = [
                (CompressionType.GZIP, "GZIP"),
                (CompressionType.BZIP2, "BZIP2"), 
                (CompressionType.LZMA, "LZMA")
            ]
            
            logger.info("ğŸ“Š ì••ì¶• ë°©ì‹ë³„ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸:")
            
            for comp_type, comp_name in compression_types:
                logger.info(f"\nğŸ—œï¸ {comp_name} ì••ì¶• í…ŒìŠ¤íŠ¸:")
                
                type_results = []
                
                for data_name, data_info in test_data_sets.items():
                    original_bytes = data_info["data"].encode('utf-8')
                    original_size = len(original_bytes)
                    
                    try:
                        # ì••ì¶•
                        compressor = compression_handler.get_compressor(comp_type, 6)
                        start_time = time.time()
                        compressed_data = compressor(original_bytes)
                        compress_time = time.time() - start_time
                        
                        compressed_size = len(compressed_data)
                        compression_ratio = (1 - compressed_size / original_size) * 100
                        
                        # ì••ì¶• í•´ì œ
                        decompressor = compression_handler.get_decompressor(comp_type)
                        start_time = time.time()
                        decompressed_data = decompressor(compressed_data)
                        decompress_time = time.time() - start_time
                        
                        # ê²€ì¦
                        data_integrity = decompressed_data == original_bytes
                        
                        result = {
                            "data_type": data_info["type"],
                            "original_size": original_size,
                            "compressed_size": compressed_size,
                            "compression_ratio": round(compression_ratio, 1),
                            "compress_time": round(compress_time * 1000, 2),  # ms
                            "decompress_time": round(decompress_time * 1000, 2),  # ms
                            "integrity_check": data_integrity
                        }
                        
                        type_results.append(result)
                        
                        status = "âœ…" if data_integrity else "âŒ"
                        logger.info(f"  {status} {data_info['type']}: {original_size:,} â†’ {compressed_size:,} bytes ({compression_ratio:.1f}% ì••ì¶•, {compress_time*1000:.1f}ms)")
                        
                        if data_integrity:
                            self.stats["passed_tests"] += 1
                        else:
                            self.stats["failed_tests"] += 1
                            self.stats["errors"].append(f"{comp_name} ì••ì¶• ë¬´ê²°ì„± ì‹¤íŒ¨: {data_name}")
                            
                        self.stats["total_tests"] += 1
                        
                    except Exception as e:
                        logger.error(f"  âŒ {data_info['type']}: ì••ì¶• ì˜¤ë¥˜ - {e}")
                        self.stats["failed_tests"] += 1
                        self.stats["total_tests"] += 1
                        self.stats["errors"].append(f"{comp_name} ì••ì¶• ì˜¤ë¥˜: {data_name} - {e}")
                
                compression_results.append({
                    "compression_type": comp_name,
                    "results": type_results,
                    "average_ratio": round(sum(r["compression_ratio"] for r in type_results) / len(type_results), 1) if type_results else 0,
                    "average_compress_time": round(sum(r["compress_time"] for r in type_results) / len(type_results), 2) if type_results else 0
                })
            
            # ì••ì¶• ë°©ì‹ ë¹„êµ ë¶„ì„
            logger.info("\nğŸ“Š ì••ì¶• ë°©ì‹ ë¹„êµ ë¶„ì„:")
            for result in compression_results:
                logger.info(f"  - {result['compression_type']}: í‰ê·  ì••ì¶•ë¥  {result['average_ratio']}%, í‰ê·  ì‹œê°„ {result['average_compress_time']}ms")
            
            # ê¶Œì¥ ì••ì¶• ì •ì±… ìƒì„±
            best_compression = max(compression_results, key=lambda x: x['average_ratio'])
            fastest_compression = min(compression_results, key=lambda x: x['average_compress_time'])
            
            logger.info(f"\nğŸ’¡ ê¶Œì¥ ì••ì¶• ì •ì±…:")
            logger.info(f"  - ìµœê³  ì••ì¶•ë¥ : {best_compression['compression_type']} ({best_compression['average_ratio']}%)")
            logger.info(f"  - ìµœê³  ì†ë„: {fastest_compression['compression_type']} ({fastest_compression['average_compress_time']}ms)")
            
            self.test_results["compression_policy"] = {
                "compression_results": compression_results,
                "best_compression": best_compression['compression_type'],
                "fastest_compression": fastest_compression['compression_type'],
                "data_types_tested": len(test_data_sets)
            }
            
            logger.info("âœ… ë°ì´í„° ì••ì¶• ë° ë³´ê´€ ì •ì±… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì••ì¶• ì •ì±… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.stats["total_tests"] += 1
            self.stats["failed_tests"] += 1
            self.stats["errors"].append(f"ì••ì¶• ì •ì±… ì˜¤ë¥˜: {e}")
    
    def generate_final_report(self):
        """ìµœì¢… í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        success_rate = round(self.stats["passed_tests"] / self.stats["total_tests"] * 100, 2) if self.stats["total_tests"] > 0 else 0
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ“‹ API ì €ì¥ ì •ì±… ë° ì•„ì¹´ì´ë¹™ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ")
        logger.info("="*80)
        
        logger.info(f"ğŸ• í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„: {duration.total_seconds():.2f}ì´ˆ")
        logger.info(f"ğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸: {self.stats['total_tests']}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {self.stats['passed_tests']}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {self.stats['failed_tests']}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate}%")
        
        if self.stats["errors"]:
            logger.info(f"\nâŒ ì˜¤ë¥˜ ëª©ë¡ ({len(self.stats['errors'])}ê°œ):")
            for i, error in enumerate(self.stats["errors"][:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                logger.info(f"  {i}. {error}")
            if len(self.stats["errors"]) > 10:
                logger.info(f"  ... ë° {len(self.stats['errors']) - 10}ê°œ ì¶”ê°€ ì˜¤ë¥˜")
        
        logger.info("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
        
        for test_name, result in self.test_results.items():
            logger.info(f"\nğŸ”¸ {test_name}:")
            if isinstance(result, dict):
                for key, value in result.items():
                    if key != "backup_results":  # ë„ˆë¬´ ê¸´ ë°ì´í„°ëŠ” ì œì™¸
                        logger.info(f"   - {key}: {value}")
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        logger.info("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        
        if success_rate >= 90:
            logger.info("âœ… ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        elif success_rate >= 70:
            logger.info("âš ï¸ ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•˜ì§€ë§Œ ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        else:
            logger.info("ğŸš¨ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì €ì¥ ê³µê°„ ìµœì í™” ê¶Œì¥ì‚¬í•­
        if "storage_optimization" in self.test_results:
            storage_result = self.test_results["storage_optimization"]
            if storage_result.get("optimizable_data_mb", 0) > 1000:  # 1GB ì´ìƒ
                logger.info(f"ğŸ’¾ {storage_result['optimizable_data_mb']}MBì˜ ë°ì´í„° ìµœì í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # ì••ì¶• ì •ì±… ê¶Œì¥ì‚¬í•­
        if "compression_policy" in self.test_results:
            comp_result = self.test_results["compression_policy"]
            logger.info(f"ğŸ—œï¸ ìµœì  ì••ì¶• ë°©ì‹: {comp_result.get('best_compression', 'GZIP')}")
        
        logger.info("\nğŸ‰ ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info("="*80)
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        report_file = f"test_results/api_storage_comprehensive_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs("test_results", exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                "test_info": {
                    "start_time": self.start_time.isoformat(),
                    "end_time": end_time.isoformat(),
                    "duration_seconds": duration.total_seconds(),
                    "success_rate": success_rate
                },
                "statistics": self.stats,
                "test_results": self.test_results
            }, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {report_file}")
    
    def cleanup(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬"""
        if self.temp_backup_dir and os.path.exists(self.temp_backup_dir):
            shutil.rmtree(self.temp_backup_dir, ignore_errors=True)
            logger.info(f"ğŸ§¹ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {self.temp_backup_dir}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    test_runner = ComprehensiveAPIStorageTest()
    test_runner.run_all_tests()


if __name__ == "__main__":
    main()