#!/usr/bin/env python3
"""
api_raw_data ÌÖåÏù¥Î∏î ÏÑ±Îä• Î∂ÑÏÑù Ïä§ÌÅ¨Î¶ΩÌä∏

ÌòÑÏû¨ ÌÖåÏù¥Î∏î Íµ¨Ï°∞, Ïù∏Îç±Ïä§, ÏøºÎ¶¨ ÏÑ±Îä•ÏùÑ Ï¢ÖÌï© Î∂ÑÏÑùÌïòÍ≥† ÏµúÏ†ÅÌôî Î∞©ÏïàÏùÑ Ï†úÏãúÌï©ÎãàÎã§.
"""

import sys
import os
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Í≤ΩÎ°ú Ï∂îÍ∞Ä
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.database_manager import DatabaseManager
from app.core.database_manager_extension import extend_database_manager


class APIRawDataAnalyzer:
    """api_raw_data ÌÖåÏù¥Î∏î ÏÑ±Îä• Î∂ÑÏÑùÍ∏∞"""
    
    def __init__(self):
        self.db_manager = extend_database_manager(DatabaseManager().sync_manager)
        self.analysis_results = {}
    
    def run_full_analysis(self) -> Dict[str, Any]:
        """Ï†ÑÏ≤¥ ÏÑ±Îä• Î∂ÑÏÑù Ïã§Ìñâ"""
        
        print("üîç api_raw_data ÌÖåÏù¥Î∏î ÏÑ±Îä• Î∂ÑÏÑù ÏãúÏûë...\n")
        
        # 1. Í∏∞Î≥∏ ÌÖåÏù¥Î∏î Ï†ïÎ≥¥
        self.analysis_results['table_info'] = self._analyze_table_structure()
        
        # 2. Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Î∂ÑÏÑù
        self.analysis_results['data_distribution'] = self._analyze_data_distribution()
        
        # 3. Ïù∏Îç±Ïä§ Î∂ÑÏÑù
        self.analysis_results['index_analysis'] = self._analyze_indexes()
        
        # 4. ÏøºÎ¶¨ ÏÑ±Îä• Ï∏°Ï†ï
        self.analysis_results['query_performance'] = self._measure_query_performance()
        
        # 5. Ïä§ÌÜ†Î¶¨ÏßÄ Î∂ÑÏÑù
        self.analysis_results['storage_analysis'] = self._analyze_storage_usage()
        
        # 6. ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠
        self.analysis_results['recommendations'] = self._generate_recommendations()
        
        return self.analysis_results
    
    def _analyze_table_structure(self) -> Dict[str, Any]:
        """ÌÖåÏù¥Î∏î Íµ¨Ï°∞ Î∂ÑÏÑù"""
        print("üìã ÌÖåÏù¥Î∏î Íµ¨Ï°∞ Î∂ÑÏÑù...")
        
        # ÌÖåÏù¥Î∏î Ïä§ÌÇ§Îßà Ï†ïÎ≥¥
        schema_query = """
        SELECT 
            column_name,
            data_type,
            is_nullable,
            column_default,
            character_maximum_length
        FROM information_schema.columns 
        WHERE table_name = 'api_raw_data'
        ORDER BY ordinal_position
        """
        
        columns = self.db_manager.fetch_all(schema_query)
        
        # ÌÖåÏù¥Î∏î ÌÅ¨Í∏∞ Ï†ïÎ≥¥
        size_query = """
        SELECT 
            pg_size_pretty(pg_total_relation_size('api_raw_data')) as total_size,
            pg_size_pretty(pg_relation_size('api_raw_data')) as table_size,
            pg_size_pretty(pg_total_relation_size('api_raw_data') - pg_relation_size('api_raw_data')) as index_size,
            (SELECT COUNT(*) FROM api_raw_data) as row_count
        """
        
        size_info = self.db_manager.fetch_one(size_query)
        
        # Ï†úÏïΩÏ°∞Í±¥ Ï†ïÎ≥¥
        constraints_query = """
        SELECT 
            constraint_name,
            constraint_type
        FROM information_schema.table_constraints
        WHERE table_name = 'api_raw_data'
        """
        
        constraints = self.db_manager.fetch_all(constraints_query)
        
        return {
            'columns': [dict(col) for col in columns],
            'size_info': dict(size_info) if size_info else {},
            'constraints': [dict(c) for c in constraints],
            'total_columns': len(columns)
        }
    
    def _analyze_data_distribution(self) -> Dict[str, Any]:
        """Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Î∂ÑÏÑù"""
        print("üìä Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Î∂ÑÏÑù...")
        
        # API Ï†úÍ≥µÏûêÎ≥Ñ Î∂ÑÌè¨
        provider_dist_query = """
        SELECT 
            api_provider,
            COUNT(*) as count,
            ROUND(AVG(response_size), 2) as avg_response_size,
            MIN(created_at) as earliest,
            MAX(created_at) as latest,
            COUNT(CASE WHEN response_status >= 400 THEN 1 END) as error_count
        FROM api_raw_data
        GROUP BY api_provider
        ORDER BY count DESC
        """
        
        provider_distribution = self.db_manager.fetch_all(provider_dist_query)
        
        # ÏóîÎìúÌè¨Ïù∏Ìä∏Î≥Ñ Î∂ÑÌè¨ (ÏÉÅÏúÑ 20Í∞ú)
        endpoint_dist_query = """
        SELECT 
            endpoint,
            COUNT(*) as count,
            ROUND(AVG(response_size), 2) as avg_response_size,
            MIN(created_at) as earliest,
            MAX(created_at) as latest
        FROM api_raw_data
        GROUP BY endpoint
        ORDER BY count DESC
        LIMIT 20
        """
        
        endpoint_distribution = self.db_manager.fetch_all(endpoint_dist_query)
        
        # ÏãúÍ∞ÑÎ≥Ñ Î∂ÑÌè¨ (ÏõîÎ≥Ñ)
        temporal_dist_query = """
        SELECT 
            DATE_TRUNC('month', created_at) as month,
            COUNT(*) as count,
            ROUND(AVG(response_size), 2) as avg_response_size,
            SUM(response_size) as total_size
        FROM api_raw_data
        GROUP BY DATE_TRUNC('month', created_at)
        ORDER BY month
        """
        
        temporal_distribution = self.db_manager.fetch_all(temporal_dist_query)
        
        # ÏùëÎãµ ÌÅ¨Í∏∞ Î∂ÑÌè¨
        size_dist_query = """
        SELECT 
            CASE 
                WHEN response_size < 1024 THEN '< 1KB'
                WHEN response_size < 10240 THEN '1KB - 10KB'
                WHEN response_size < 102400 THEN '10KB - 100KB'
                WHEN response_size < 1048576 THEN '100KB - 1MB'
                WHEN response_size < 10485760 THEN '1MB - 10MB'
                ELSE '> 10MB'
            END as size_range,
            COUNT(*) as count
        FROM api_raw_data
        WHERE response_size IS NOT NULL
        GROUP BY 
            CASE 
                WHEN response_size < 1024 THEN '< 1KB'
                WHEN response_size < 10240 THEN '1KB - 10KB'
                WHEN response_size < 102400 THEN '10KB - 100KB'
                WHEN response_size < 1048576 THEN '100KB - 1MB'
                WHEN response_size < 10485760 THEN '1MB - 10MB'
                ELSE '> 10MB'
            END
        ORDER BY 
            MIN(CASE 
                WHEN response_size < 1024 THEN 1
                WHEN response_size < 10240 THEN 2
                WHEN response_size < 102400 THEN 3
                WHEN response_size < 1048576 THEN 4
                WHEN response_size < 10485760 THEN 5
                ELSE 6
            END)
        """
        
        size_distribution = self.db_manager.fetch_all(size_dist_query)
        
        return {
            'provider_distribution': [dict(row) for row in provider_distribution],
            'endpoint_distribution': [dict(row) for row in endpoint_distribution],
            'temporal_distribution': [dict(row) for row in temporal_distribution],
            'size_distribution': [dict(row) for row in size_distribution]
        }
    
    def _analyze_indexes(self) -> Dict[str, Any]:
        """Ïù∏Îç±Ïä§ Î∂ÑÏÑù"""
        print("üóÇÔ∏è Ïù∏Îç±Ïä§ Î∂ÑÏÑù...")
        
        # ÌòÑÏû¨ Ïù∏Îç±Ïä§ Ï°∞Ìöå
        indexes_query = """
        SELECT 
            schemaname,
            tablename,
            indexname,
            indexdef
        FROM pg_indexes 
        WHERE tablename = 'api_raw_data'
        ORDER BY indexname
        """
        
        current_indexes = self.db_manager.fetch_all(indexes_query)
        
        # Ïù∏Îç±Ïä§ ÏÇ¨Ïö© ÌÜµÍ≥Ñ
        index_usage_query = """
        SELECT 
            indexrelname as index_name,
            idx_tup_read,
            idx_tup_fetch,
            idx_scan
        FROM pg_stat_user_indexes 
        WHERE relname = 'api_raw_data'
        ORDER BY idx_scan DESC
        """
        
        index_usage = self.db_manager.fetch_all(index_usage_query)
        
        # Ï§ëÎ≥µ Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÎäî Ïù∏Îç±Ïä§ Î∂ÑÏÑù
        duplicate_analysis = self._analyze_duplicate_indexes()
        
        return {
            'current_indexes': [dict(idx) for idx in current_indexes],
            'index_usage_stats': [dict(usage) for usage in index_usage],
            'duplicate_analysis': duplicate_analysis,
            'total_indexes': len(current_indexes)
        }
    
    def _analyze_duplicate_indexes(self) -> List[Dict[str, Any]]:
        """Ï§ëÎ≥µ Ïù∏Îç±Ïä§ Î∂ÑÏÑù"""
        # Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎäî Îçî Î≥µÏû°Ìïú Ï§ëÎ≥µ Î∂ÑÏÑùÏù¥ ÌïÑÏöî
        # Ïó¨Í∏∞ÏÑúÎäî Í∏∞Î≥∏Ï†ÅÏù∏ Î∂ÑÏÑùÎßå ÏàòÌñâ
        
        return [
            {
                'analysis': 'ÌòÑÏû¨ Ïù∏Îç±Ïä§ Ï§ëÎ≥µÏÑ± Í≤ÄÏÇ¨',
                'note': 'ÏÉÅÏÑ∏Ìïú Ï§ëÎ≥µ Î∂ÑÏÑùÏùÄ Ïã§Ï†ú ÏøºÎ¶¨ Ìå®ÌÑ¥ ÌôïÏù∏ ÌõÑ ÏàòÌñâ ÌïÑÏöî'
            }
        ]
    
    def _measure_query_performance(self) -> Dict[str, Any]:
        """ÏøºÎ¶¨ ÏÑ±Îä• Ï∏°Ï†ï"""
        print("‚ö° ÏøºÎ¶¨ ÏÑ±Îä• Ï∏°Ï†ï...")
        
        # ÏÑ±Îä• Ï∏°Ï†ïÌï† Ï£ºÏöî ÏøºÎ¶¨Îì§
        test_queries = [
            {
                'name': 'provider_lookup',
                'description': 'API Ï†úÍ≥µÏûêÎ≥Ñ Ï°∞Ìöå',
                'query': "SELECT COUNT(*) FROM api_raw_data WHERE api_provider = 'KMA'"
            },
            {
                'name': 'endpoint_lookup',
                'description': 'ÏóîÎìúÌè¨Ïù∏Ìä∏Î≥Ñ Ï°∞Ìöå',
                'query': "SELECT COUNT(*) FROM api_raw_data WHERE endpoint = 'getUltraSrtFcst'"
            },
            {
                'name': 'date_range_lookup',
                'description': 'ÎÇ†Ïßú Î≤îÏúÑ Ï°∞Ìöå',
                'query': "SELECT COUNT(*) FROM api_raw_data WHERE created_at >= NOW() - INTERVAL '7 days'"
            },
            {
                'name': 'combined_lookup',
                'description': 'Î≥µÌï© Ï°∞Í±¥ Ï°∞Ìöå',
                'query': "SELECT COUNT(*) FROM api_raw_data WHERE api_provider = 'KMA' AND created_at >= NOW() - INTERVAL '1 day'"
            },
            {
                'name': 'size_filter',
                'description': 'ÌÅ¨Í∏∞ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ',
                'query': "SELECT COUNT(*) FROM api_raw_data WHERE response_size > 1048576"
            },
            {
                'name': 'error_lookup',
                'description': 'Ïò§Î•ò ÏùëÎãµ Ï°∞Ìöå',
                'query': "SELECT COUNT(*) FROM api_raw_data WHERE response_status >= 400"
            }
        ]
        
        performance_results = []
        
        for test in test_queries:
            try:
                # ÏøºÎ¶¨ Ïã§Ìñâ ÏãúÍ∞Ñ Ï∏°Ï†ï
                start_time = time.time()
                result = self.db_manager.fetch_one(test['query'])
                end_time = time.time()
                
                execution_time = round((end_time - start_time) * 1000, 2)  # ms
                
                performance_results.append({
                    'query_name': test['name'],
                    'description': test['description'],
                    'execution_time_ms': execution_time,
                    'result_count': result.get('count', 0) if result else 0
                })
                
            except Exception as e:
                performance_results.append({
                    'query_name': test['name'],
                    'description': test['description'],
                    'execution_time_ms': -1,
                    'error': str(e)
                })
        
        return {
            'query_tests': performance_results,
            'baseline_established': datetime.now()
        }
    
    def _analyze_storage_usage(self) -> Dict[str, Any]:
        """Ïä§ÌÜ†Î¶¨ÏßÄ ÏÇ¨Ïö©Îüâ Î∂ÑÏÑù"""
        print("üíæ Ïä§ÌÜ†Î¶¨ÏßÄ ÏÇ¨Ïö©Îüâ Î∂ÑÏÑù...")
        
        # ÏÉÅÏÑ∏ Ïä§ÌÜ†Î¶¨ÏßÄ Î∂ÑÏÑù
        storage_query = """
        SELECT 
            schemaname,
            tablename,
            attname as column_name,
            n_distinct,
            correlation,
            avg_width
        FROM pg_stats 
        WHERE tablename = 'api_raw_data'
        ORDER BY avg_width DESC
        """
        
        storage_stats = self.db_manager.fetch_all(storage_query)
        
        # JSONB Ïª¨ÎüºÎ≥Ñ ÌÅ¨Í∏∞ Î∂ÑÏÑù
        jsonb_analysis_query = """
        SELECT 
            'raw_response' as column_name,
            AVG(pg_column_size(raw_response)) as avg_size_bytes,
            MIN(pg_column_size(raw_response)) as min_size_bytes,
            MAX(pg_column_size(raw_response)) as max_size_bytes
        FROM api_raw_data
        WHERE raw_response IS NOT NULL
        
        UNION ALL
        
        SELECT 
            'request_params' as column_name,
            AVG(pg_column_size(request_params)) as avg_size_bytes,
            MIN(pg_column_size(request_params)) as min_size_bytes,
            MAX(pg_column_size(request_params)) as max_size_bytes
        FROM api_raw_data
        WHERE request_params IS NOT NULL
        """
        
        jsonb_analysis = self.db_manager.fetch_all(jsonb_analysis_query)
        
        # ÏïïÏ∂ï Ìö®Í≥º Ï∂îÏ†ï
        compression_estimate = self._estimate_compression_benefits()
        
        return {
            'storage_stats': [dict(stat) for stat in storage_stats],
            'jsonb_analysis': [dict(analysis) for analysis in jsonb_analysis],
            'compression_estimate': compression_estimate
        }
    
    def _estimate_compression_benefits(self) -> Dict[str, Any]:
        """ÏïïÏ∂ï Ìö®Í≥º Ï∂îÏ†ï"""
        # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞Î°ú ÏïïÏ∂ï Ìö®Í≥º Ï∂îÏ†ï
        sample_query = """
        SELECT 
            AVG(LENGTH(raw_response::text)) as avg_raw_response_length,
            COUNT(*) as sample_size
        FROM api_raw_data
        WHERE raw_response IS NOT NULL
        LIMIT 1000
        """
        
        sample_result = self.db_manager.fetch_one(sample_query)
        
        if sample_result and sample_result.get('avg_raw_response_length'):
            # ÏùºÎ∞òÏ†ÅÏù∏ JSON ÏïïÏ∂ïÎ•† 70-80% Ï†ÅÏö©
            original_size = float(sample_result['avg_raw_response_length'])
            estimated_compressed_size = original_size * 0.25  # 75% ÏïïÏ∂ï Í∞ÄÏ†ï
            
            return {
                'original_avg_size_bytes': round(original_size, 2),
                'estimated_compressed_size_bytes': round(estimated_compressed_size, 2),
                'compression_ratio': round((1 - estimated_compressed_size / original_size) * 100, 1),
                'sample_size': sample_result.get('sample_size', 0)
            }
        
        return {'note': 'ÏïïÏ∂ï Ìö®Í≥º Ï∂îÏ†ïÏùÑ ÏúÑÌïú Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå'}
    
    def _generate_recommendations(self) -> List[Dict[str, Any]]:
        """ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        print("üí° ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±...")
        
        recommendations = []
        
        # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
        provider_dist = self.analysis_results.get('data_distribution', {}).get('provider_distribution', [])
        
        if provider_dist:
            # KMA Îç∞Ïù¥ÌÑ∞Í∞Ä ÎßéÏùÄ Í≤ΩÏö∞
            kma_data = next((p for p in provider_dist if p['api_provider'] == 'KMA'), None)
            if kma_data and kma_data.get('count', 0) > 30000:
                recommendations.append({
                    'category': 'indexing',
                    'priority': 'high',
                    'title': 'KMA Îç∞Ïù¥ÌÑ∞ Ï†ÑÏö© Ïù∏Îç±Ïä§ Ï∂îÍ∞Ä',
                    'description': 'KMA Îç∞Ïù¥ÌÑ∞Í∞Ä Ï†ÑÏ≤¥Ïùò 88%Î•º Ï∞®ÏßÄÌïòÎØÄÎ°ú api_provider + created_at Î≥µÌï© Ïù∏Îç±Ïä§ ÏÉùÏÑ±',
                    'sql': "CREATE INDEX CONCURRENTLY idx_api_raw_data_kma_created ON api_raw_data(api_provider, created_at) WHERE api_provider = 'KMA';"
                })
        
        # ÏÑ±Îä• Ï∏°Ï†ï Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
        query_performance = self.analysis_results.get('query_performance', {}).get('query_tests', [])
        
        slow_queries = [q for q in query_performance if q.get('execution_time_ms', 0) > 100]
        if slow_queries:
            recommendations.append({
                'category': 'performance',
                'priority': 'medium',
                'title': 'ÎäêÎ¶∞ ÏøºÎ¶¨ ÏµúÏ†ÅÌôî',
                'description': f'{len(slow_queries)}Í∞úÏùò ÏøºÎ¶¨Í∞Ä 100ms Ïù¥ÏÉÅ ÏÜåÏöîÎê®',
                'details': [q['query_name'] for q in slow_queries]
            })
        
        # Ïä§ÌÜ†Î¶¨ÏßÄ Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠
        storage_info = self.analysis_results.get('table_info', {}).get('size_info', {})
        row_count = storage_info.get('row_count', 0)
        
        if row_count > 30000:
            recommendations.extend([
                {
                    'category': 'partitioning',
                    'priority': 'medium',
                    'title': 'ÌååÌã∞ÏÖîÎãù Í≤ÄÌÜ†',
                    'description': f'ÌòÑÏû¨ {row_count:,}Í∞ú Î†àÏΩîÎìúÎ°ú ÏõîÎ≥Ñ ÌååÌã∞ÏÖîÎãù Í≥†Î†§',
                    'benefit': 'ÏøºÎ¶¨ ÏÑ±Îä• Ìñ•ÏÉÅ, Ïú†ÏßÄÎ≥¥Ïàò Ìö®Ïú®ÏÑ± Ï¶ùÎåÄ'
                },
                {
                    'category': 'archiving',
                    'priority': 'low',
                    'title': 'Ïò§ÎûòÎêú Îç∞Ïù¥ÌÑ∞ ÏïÑÏπ¥Ïù¥Îπô',
                    'description': '90Ïùº Ïù¥ÏÉÅ Îêú Îç∞Ïù¥ÌÑ∞Ïùò ÏïïÏ∂ï ÏïÑÏπ¥Ïù¥Îπô Í≤ÄÌÜ†',
                    'benefit': 'Ïä§ÌÜ†Î¶¨ÏßÄ ÎπÑÏö© Ï†àÍ∞ê'
                }
            ])
        
        # ÏïïÏ∂ï Í¥ÄÎ†® Í∂åÏû•ÏÇ¨Ìï≠
        compression_estimate = self.analysis_results.get('storage_analysis', {}).get('compression_estimate', {})
        if compression_estimate.get('compression_ratio', 0) > 50:
            recommendations.append({
                'category': 'compression',
                'priority': 'low',
                'title': 'JSONB Îç∞Ïù¥ÌÑ∞ ÏïïÏ∂ï',
                'description': f'ÏòàÏÉÅ ÏïïÏ∂ïÎ•† {compression_estimate.get("compression_ratio", 0)}%Î°ú ÏÉÅÎãπÌïú Í≥µÍ∞Ñ Ï†àÏïΩ Í∞ÄÎä•',
                'benefit': 'Ïä§ÌÜ†Î¶¨ÏßÄ ÎπÑÏö© Ï†àÍ∞ê'
            })
        
        # Ïù∏Îç±Ïä§ ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠
        recommendations.extend([
            {
                'category': 'indexing',
                'priority': 'high',
                'title': 'Î≥µÌï© Ïù∏Îç±Ïä§ ÏÉùÏÑ±',
                'description': 'ÏûêÏ£º ÏÇ¨Ïö©ÎêòÎäî ÏøºÎ¶¨ Ìå®ÌÑ¥Ïö© Î≥µÌï© Ïù∏Îç±Ïä§',
                'sql': "CREATE INDEX CONCURRENTLY idx_api_raw_data_provider_endpoint_created ON api_raw_data(api_provider, endpoint, created_at);"
            },
            {
                'category': 'indexing',
                'priority': 'medium',
                'title': 'Î∂ÄÎ∂Ñ Ïù∏Îç±Ïä§ ÏÉùÏÑ±',
                'description': 'Ïò§Î•ò ÏùëÎãµ Ï†ÑÏö© Î∂ÄÎ∂Ñ Ïù∏Îç±Ïä§',
                'sql': "CREATE INDEX CONCURRENTLY idx_api_raw_data_errors ON api_raw_data(api_provider, created_at) WHERE response_status >= 400;"
            }
        ])
        
        return recommendations
    
    def generate_optimization_sql(self) -> List[str]:
        """ÏµúÏ†ÅÌôî SQL Ïä§ÌÅ¨Î¶ΩÌä∏ ÏÉùÏÑ±"""
        
        sql_scripts = []
        
        # Í∂åÏû• Ïù∏Îç±Ïä§Îì§
        indexes = [
            # 1. API Ï†úÍ≥µÏûê + ÏÉùÏÑ±Ïùº Î≥µÌï© Ïù∏Îç±Ïä§
            """
            -- API Ï†úÍ≥µÏûêÎ≥Ñ ÏãúÍ∞Ñ Î≤îÏúÑ Ï°∞ÌöåÏö© Ïù∏Îç±Ïä§
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_api_raw_data_provider_created 
            ON api_raw_data(api_provider, created_at DESC);
            """,
            
            # 2. ÏóîÎìúÌè¨Ïù∏Ìä∏ + ÏÉùÏÑ±Ïùº Î≥µÌï© Ïù∏Îç±Ïä§  
            """
            -- ÏóîÎìúÌè¨Ïù∏Ìä∏Î≥Ñ ÏãúÍ∞Ñ Î≤îÏúÑ Ï°∞ÌöåÏö© Ïù∏Îç±Ïä§
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_api_raw_data_endpoint_created 
            ON api_raw_data(endpoint, created_at DESC);
            """,
            
            # 3. ÏùëÎãµ ÏÉÅÌÉú Î∂ÄÎ∂Ñ Ïù∏Îç±Ïä§
            """
            -- Ïò§Î•ò ÏùëÎãµ Ï†ÑÏö© Î∂ÄÎ∂Ñ Ïù∏Îç±Ïä§
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_api_raw_data_errors 
            ON api_raw_data(api_provider, created_at) 
            WHERE response_status >= 400;
            """,
            
            # 4. ÏùëÎãµ ÌÅ¨Í∏∞ Î∂ÄÎ∂Ñ Ïù∏Îç±Ïä§
            """
            -- ÎåÄÏö©Îüâ ÏùëÎãµ Ï†ÑÏö© Î∂ÄÎ∂Ñ Ïù∏Îç±Ïä§
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_api_raw_data_large_responses 
            ON api_raw_data(api_provider, response_size, created_at) 
            WHERE response_size > 1048576; -- 1MB Ïù¥ÏÉÅ
            """,
            
            # 5. TTL Í¥ÄÎ¶¨Ïö© Ïù∏Îç±Ïä§
            """
            -- TTL Í∏∞Î∞ò Ï†ïÎ¶¨ ÏûëÏóÖÏö© Ïù∏Îç±Ïä§
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_api_raw_data_ttl 
            ON api_raw_data(expires_at) 
            WHERE expires_at IS NOT NULL;
            """,
            
            # 6. API ÌÇ§ Ìï¥Ïãú Ïù∏Îç±Ïä§
            """
            -- API ÌÇ§Î≥Ñ ÏÇ¨Ïö©Îüâ Ï∂îÏ†ÅÏö© Ïù∏Îç±Ïä§
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_api_raw_data_api_key_hash 
            ON api_raw_data(api_key_hash, created_at) 
            WHERE api_key_hash IS NOT NULL;
            """
        ]
        
        sql_scripts.extend(indexes)
        
        # ÌÖåÏù¥Î∏î ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
        sql_scripts.append("""
        -- ÌÖåÏù¥Î∏î ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
        ANALYZE api_raw_data;
        """)
        
        return sql_scripts


def print_analysis_report(analysis: Dict[str, Any]):
    """Î∂ÑÏÑù Í≤∞Í≥º Î≥¥Í≥†ÏÑú Ï∂úÎ†•"""
    
    print("\n" + "="*80)
    print("üìä API_RAW_DATA ÌÖåÏù¥Î∏î ÏÑ±Îä• Î∂ÑÏÑù Î≥¥Í≥†ÏÑú")
    print("="*80)
    print(f"üìÖ Î∂ÑÏÑù ÏùºÏãú: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # 1. ÌÖåÏù¥Î∏î Í∏∞Î≥∏ Ï†ïÎ≥¥
    table_info = analysis.get('table_info', {})
    size_info = table_info.get('size_info', {})
    
    print("üìã **ÌÖåÏù¥Î∏î Í∏∞Î≥∏ Ï†ïÎ≥¥**")
    print("-" * 50)
    print(f"‚Ä¢ Ï¥ù Î†àÏΩîÎìú Ïàò: {size_info.get('row_count', 0):,}Í∞ú")
    print(f"‚Ä¢ ÌÖåÏù¥Î∏î ÌÅ¨Í∏∞: {size_info.get('table_size', 'N/A')}")
    print(f"‚Ä¢ Ïù∏Îç±Ïä§ ÌÅ¨Í∏∞: {size_info.get('index_size', 'N/A')}")
    print(f"‚Ä¢ Ï†ÑÏ≤¥ ÌÅ¨Í∏∞: {size_info.get('total_size', 'N/A')}")
    print(f"‚Ä¢ Ïª¨Îüº Ïàò: {table_info.get('total_columns', 0)}Í∞ú")
    print()
    
    # 2. Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨
    data_dist = analysis.get('data_distribution', {})
    provider_dist = data_dist.get('provider_distribution', [])
    
    if provider_dist:
        print("üìä **API Ï†úÍ≥µÏûêÎ≥Ñ Î∂ÑÌè¨**")
        print("-" * 50)
        for provider in provider_dist:
            error_rate = 0
            if provider.get('count', 0) > 0:
                error_rate = (provider.get('error_count', 0) / provider['count']) * 100
            
            avg_size = provider.get('avg_response_size') or 0
            print(f"‚Ä¢ {provider['api_provider']}: {provider['count']:,}Í∞ú "
                  f"(ÌèâÍ∑† ÌÅ¨Í∏∞: {avg_size:.0f}Î∞îÏù¥Ìä∏, "
                  f"Ïò§Î•òÏú®: {error_rate:.1f}%)")
        print()
    
    # 3. ÏøºÎ¶¨ ÏÑ±Îä•
    query_perf = analysis.get('query_performance', {})
    query_tests = query_perf.get('query_tests', [])
    
    if query_tests:
        print("‚ö° **ÏøºÎ¶¨ ÏÑ±Îä• Ï∏°Ï†ï Í≤∞Í≥º**")
        print("-" * 50)
        for test in query_tests:
            status = "‚úÖ" if test.get('execution_time_ms', 0) < 50 else "‚ö†Ô∏è" if test.get('execution_time_ms', 0) < 200 else "‚ùå"
            print(f"{status} {test['description']}: {test.get('execution_time_ms', 'N/A')}ms")
        print()
    
    # 4. ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠
    recommendations = analysis.get('recommendations', [])
    
    if recommendations:
        print("üí° **ÏµúÏ†ÅÌôî Í∂åÏû•ÏÇ¨Ìï≠**")
        print("-" * 50)
        
        # Ïö∞ÏÑ†ÏàúÏúÑÎ≥ÑÎ°ú Ï†ïÎ†¨
        high_priority = [r for r in recommendations if r.get('priority') == 'high']
        medium_priority = [r for r in recommendations if r.get('priority') == 'medium']
        low_priority = [r for r in recommendations if r.get('priority') == 'low']
        
        for priority_group, priority_name in [(high_priority, "üî¥ ÎÜíÏùå"), (medium_priority, "üü° Ï§ëÍ∞Ñ"), (low_priority, "üü¢ ÎÇÆÏùå")]:
            if priority_group:
                print(f"\n{priority_name} Ïö∞ÏÑ†ÏàúÏúÑ:")
                for i, rec in enumerate(priority_group, 1):
                    print(f"  {i}. {rec['title']}")
                    print(f"     ‚îî‚îÄ {rec['description']}")
                    if rec.get('benefit'):
                        print(f"     ‚îî‚îÄ Ìö®Í≥º: {rec['benefit']}")
        print()
    
    # 5. Îã§Ïùå Îã®Í≥Ñ
    print("üéØ **Îã§Ïùå Îã®Í≥Ñ**")
    print("-" * 50)
    print("1. Í∂åÏû• Ïù∏Îç±Ïä§ ÏÉùÏÑ± (CONCURRENTLY ÏòµÏÖò ÏÇ¨Ïö©)")
    print("2. ÏøºÎ¶¨ ÏÑ±Îä• Ïû¨Ï∏°Ï†ï Î∞è ÎπÑÍµê")
    print("3. ÌååÌã∞ÏÖîÎãù Ï†ÑÎûµ ÏÉÅÏÑ∏ ÏÑ§Í≥Ñ")
    print("4. ÏûêÎèô Ï†ïÎ¶¨ ÏãúÏä§ÌÖú Íµ¨ÌòÑ")
    print("5. Î™®ÎãàÌÑ∞ÎßÅ ÎåÄÏãúÎ≥¥Îìú Íµ¨Ï∂ï")
    print()
    
    print("="*80)


def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    
    try:
        print("üöÄ API Raw Data ÏÑ±Îä• Î∂ÑÏÑù ÎèÑÍµ¨ ÏãúÏûë\n")
        
        analyzer = APIRawDataAnalyzer()
        analysis_results = analyzer.run_full_analysis()
        
        # Î∂ÑÏÑù Í≤∞Í≥º Ï∂úÎ†•
        print_analysis_report(analysis_results)
        
        # ÏµúÏ†ÅÌôî SQL ÏÉùÏÑ±
        optimization_sql = analyzer.generate_optimization_sql()
        
        # SQL Ïä§ÌÅ¨Î¶ΩÌä∏ ÌååÏùºÎ°ú Ï†ÄÏû•
        sql_filename = f"api_raw_data_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sql"
        
        with open(sql_filename, 'w', encoding='utf-8') as f:
            f.write("-- API Raw Data ÌÖåÏù¥Î∏î ÏµúÏ†ÅÌôî SQL Ïä§ÌÅ¨Î¶ΩÌä∏\n")
            f.write(f"-- ÏÉùÏÑ±ÏùºÏãú: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            for i, sql in enumerate(optimization_sql, 1):
                f.write(f"-- {i}. ÏµúÏ†ÅÌôî Ïä§ÌÅ¨Î¶ΩÌä∏\n")
                f.write(sql.strip() + "\n\n")
        
        print(f"üìÑ ÏµúÏ†ÅÌôî SQL Ïä§ÌÅ¨Î¶ΩÌä∏Í∞Ä '{sql_filename}' ÌååÏùºÎ°ú Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")
        
        # JSON Í≤∞Í≥º ÌååÏùº Ï†ÄÏû•
        json_filename = f"api_raw_data_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # datetime Î∞è Decimal Í∞ùÏ≤¥Î•º JSON ÏßÅÎ†¨Ìôî Í∞ÄÎä•ÌïòÍ≤å Î≥ÄÌôòÌïòÎäî Ìï®Ïàò
        def json_serial(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif hasattr(obj, '__float__'):  # Decimal Îì±Ïùò Ïà´Ïûê ÌÉÄÏûÖ
                return float(obj)
            raise TypeError(f"Type {type(obj)} not serializable")
        
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=json_serial)
        
        print(f"üìä ÏÉÅÏÑ∏ Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä '{json_filename}' ÌååÏùºÎ°ú Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")
        
    except Exception as e:
        print(f"‚ùå Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        raise


if __name__ == "__main__":
    main()